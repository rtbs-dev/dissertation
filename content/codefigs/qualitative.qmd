---
title: Source for Case Studies
lightbox: auto
execute:
  cache: true
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: Python (Pixi)
    language: python
    name: pixi-kernel-python3
---

```{python}
import awkward as ak
from ruamel.yaml import YAML
from pathlib import Path 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl
import affinis.associations as asc
from affinis.utils import _sq, n_nodes_from_edges
from affinis.filter import min_connected_filter,edge_mask_to_laplacian
from affinis.proximity import sinkhorn, forest_correlation
from affinis.distance import generalized_graph_dists, adjusted_forest_dists
import scipy.sparse as sprs
from scipy.stats import powerlaw

from adjustText import adjust_text

import numpy as np
import networkx as nx
```

```{python}
mpl.rcParams['font.family']=['serif']
mpl.rcParams['font.serif']=['Bitstream Charter']
sns.set_theme(font='serif',palette='Set2', context='paper', style='ticks')
figsize = (10,6)
mpl.rcParams['figure.figsize'] = figsize

%config InlineBackend.figure_formats = ['svg']

nxlabel_kws={
    # "font_color": "white",
    "font_color": "0.3",
    "font_family":"Bitstream Charter",
    "font_weight":'bold',
    "font_size": 8,
}

nxdraw_kws = {
    "node_size": 50, 
    # "node_color": "xkcd:slate",
    "edgecolors": "grey",
    "edge_color": "grey",
    "linewidths": 1,
    "width": 2,
} | nxlabel_kws
```

## Network Scientist Network

```{python}
yaml = YAML(typ='safe')
# data = yaml.load(path)
datadir = Path('../../data/qual')

# s = ak.from_iter(yaml.load(datadir/'complex-networks.yaml').items())
# s = ak.Array(dict(zip(['id','meta'], ak.unzip(s))))
# ak.from_json()

df1 = (
    pd.DataFrame
    .from_dict(
        yaml.load(datadir/'netsci'/'complex-networks.yaml'), 
        orient='index'
    )
    .rename_axis('id')
)
df2 = (
    pd.DataFrame
    .from_dict(
        yaml.load(datadir/'netsci'/'complex-networks-struc-dyn.yaml'), 
        orient='index'
    )
    .rename_axis('id')
)
df3 = (
    pd.DataFrame
    .from_dict(
        yaml.load(datadir/'netsci'/'large-systems.yaml'), 
        orient='index'
    )
    .rename_axis('id')
)
df = pd.concat([df1, df2, df3])
df = df[~df.index.duplicated(keep='last')]
```

```{python}
# (~df2.index.isin(df1.index)).sum()

# df[df.index.duplicated(keep=False)]
```

```{python}
# ak.to_dataframe(s)
tidy=(df
 .explode('author')
 .reset_index()
 .assign(
     coauth=1,
     author=lambda df: (
         df.author
         .str.capitalize()
         # .astype('category')
     )
 )
 # .set_index(['id','author'])
 # ['coauth'].unstack().fillna(0)
)
# tidy.iloc[tidy.set_index(['id','author']).index.duplicated()]
# df.loc['WOS:000226704200004']
```

```{python}
Xdf = (
    tidy
    .set_index(['id','author'])
    ['coauth'].unstack().fillna(0)
)

def iter_filter(Xdf, rowmin=2, colmin=2,maxiter=50):
    colsum = Xdf.sum(axis=0)
    # colsum = Xdf.sum()
    for it in range(maxiter):
        Xdf = Xdf.loc[:,colsum>=colmin]
        rowsum = Xdf.sum(axis=1)
        if rowsum.min()>=rowmin: 
            break
        Xdf = Xdf.loc[rowsum>=rowmin]
        colsum = Xdf.sum(axis=0) 
        if  colsum.min()>=colmin:
            break
    return Xdf
# Xdf = Xdf[Xdf.sum(axis=1)>1]
# Xdf = Xdf.loc[:,Xdf.sum()>1]
# Xdf = Xdf[Xdf.sum(axis=1)>1]
# Xdf = Xdf.loc[:,Xdf.sum()>1]
print('papers,authors')
print('before filtering:',Xdf.shape)
Xdf = iter_filter(Xdf, rowmin=2,colmin=2)
print('after filtering:',Xdf.shape)
# Xdf = Xdf[Xdf.sum(axis=1)>1]

X = Xdf.values#.astype(bool)
Xdf.columns = Xdf.columns.astype('category')
authortype=Xdf.columns.dtype#.categories
authortype.categories
```

```{python}
f = mpl.figure.Figure(figsize=(6,2))
s1,s2 = f.subplots(1, 2, sharey=True)
sns.histplot(Xdf.sum(), bins=Xdf.sum().max(), discrete=True, ax=s1)
s1.set(yscale='log', xlabel='#papers/author')

sns.histplot(Xdf.sum(axis=1), bins=Xdf.sum(axis=1).max(), discrete=True, ax=s2)
s2.set(xlabel='#authors/paper')
# s1.set_yscale('log')
f
```

```{python}
# Xhyp = (Xdf.T/(Xdf.sum(axis=1)-1)).T

# Xhyp.T@Xhyp - np.diag(np.diag(Xhyp.T@Xhyp))#-np.diag(Xhyp.sum())
```

```{python}
# sns.heatmap(Xdf.T@Xdf - np.diag(Xdf.sum()))
```

```{python}
# figsize=(10,8)
def draw_graph_communities(G, pos, colors, ax=None):
    if ax is None: 
        ax = plt.gca()
    nx.draw_networkx(
        G, pos=pos, 
        with_labels=False, 
        node_color=[colors[i] for i in G.nodes], 
        ax=ax,
        **nxdraw_kws
    )
# nx.connected.is_connected(G)
    labels=nx.draw_networkx_labels(
        G, pos=pos,
        bbox=dict(edgecolor='grey',boxstyle='round,pad=0.2', alpha=0.7), 
        ax=ax,
        **nxlabel_kws
    )
    #iterate over the labels (key = label, value=matplotlib Text object)
    labels=adjust_text(list(labels.values()), ax=ax)
    for t in labels[0]:
        #manipulate indiviual text objects
        # print(t)
        t.set_backgroundcolor(colors[t.get_text()])
    sns.despine(left=True, bottom=True)
    return ax
```

```{python}
#| label: fig-netsci-cooc
#| fig-cap: 134 Network scientists connected by co-authorship
#| editable: true
#| lightbox: {desc-position: right, group: netsci}
#| slideshow: {slide_type: ''}

A_cooc = Xdf.T@Xdf - np.diag(Xdf.sum())
G = nx.from_pandas_adjacency(A_cooc>0)

subset = sorted(nx.connected_components(G), key=len, reverse=True)[0]
s_subs=pd.Series(list(subset), dtype=authortype)
# G = nx.from_pandas_adjacency((Xhyp.T@Xhyp - np.diag(np.diag(Xhyp.T@Xhyp)))>0.1)
# list(nx.neighbors(G, 'newman, m.'))

pos_cos = nx.kamada_kawai_layout(G)
G = G.subgraph(subset)

commun = list(list(i) for i in nx.community.greedy_modularity_communities(G))
pal = sns.color_palette(n_colors=len(commun))
colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G,pos_cos,colors)

        
# print(len(subset))
plt.title(
    'Network Scientist Co-Occurrence Graph '
    f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
);
insax = ax.inset_axes([0,0,0.3,0.3])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos_cos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
#| label: fig-netsci-tree
#| fig-cap: Chow-Liu tree of NetSci collaborator dependency relationships
#| lightbox: {desc-position: right, group: netsci}

G = nx.from_pandas_adjacency(pd.DataFrame(asc.chow_liu(X, pseudocts=0.5), index=Xdf.columns, columns=Xdf.columns))
pos_tree = nx.kamada_kawai_layout(G, pos=pos_cos)
G = G.subgraph(subset)

# nx.draw_networkx(G, pos=pos_tree, node_color='w')
# nx.draw_networkx_labels(G, pos=pos_tree, font_color=colors);
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G, pos_tree, colors)
nx.connected.is_connected(G)
# plt.annotate(f'r = {nx.degree_assortativity_coefficient(G):.3f}', (0.25, 0.5), xycoords='axes fraction', size=20)
plt.title(
    'Network Scientist Collaborator Chow-Liu Tree '
    f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
    # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
);
insax = ax.inset_axes([0,0,0.3,0.3])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos_tree[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
#| label: fig-netsci-fp
#| fig-cap: Forest Pursuit estimate of NetSci collaborator dependency relationships
#| lightbox: {desc-position: right, group: netsci}

evd_L_pursuit = _sq(asc.forest_pursuit_edge(X)[s_subs.cat.codes].T[s_subs.cat.codes])
A_fp = pd.DataFrame(_sq(evd_L_pursuit), index=s_subs, columns=s_subs)
G = nx.from_pandas_adjacency(A_fp>0.01)
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=s_subs, columns=s_subs))
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=Xdf.columns, columns=Xdf.columns))
# G = G.subgraph(subset)
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G,pos,colors)
plt.title(
    'Network Scientist Collaboration Network Estimate '
    f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
    # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
);

insax = ax.inset_axes([0,0,0.3,0.3])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
nx.degree_assortativity_coefficient(G)
# deg_cts = nx.degree_histogram(G)
# plt.bar(np.arange(0,len(deg_cts)),nx.degree_histogram(G))
```

```{python}
#| label: fig-netsci-degree

degs = pd.DataFrame({
    'Orig.':(A_cooc>0).loc[list(subset),list(subset)].sum(), 
    'FP':(A_fp>0.01).sum()
}).melt(var_name='graph', value_name="degree")
# pd.DataFrame.melt()
plt.figure(figsize=(3,2))
sns.boxplot(
    degs, x='degree',y='graph',orient='h',
    # estimator='median',
    # errorbar=None,
    # linestyle='',
    color='grey',
    gap=.5,
    # marker='|',
    # markersize=15,
)
degs.groupby('graph').median();
```

```{python}
# L = edge_mask_to_laplacian(np.ma.masked_less_equal(evd_L_pursuit, 0.01))
L = nx.laplacian_matrix(G, nodelist=list(subset))
# sns.clustermap(pd.DataFrame(forest_correlation(L),  index=list(subset), columns=list(subset)))
from sklearn.cluster import AgglomerativeClustering
# sns.histplot(_sq(generalized_graph_dists(L)))
```

```{python}
from scipy.cluster.hierarchy import dendrogram

clust=AgglomerativeClustering(
    metric='precomputed', linkage='complete',#, distance_threshold=0.2,
    distance_threshold=4.5, n_clusters=None
).fit(adjusted_forest_dists(L.todense(), beta=100))

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)
plt.figure(figsize=(12,5))
plot_dendrogram(clust)
```

```{python}
# np.array(pal)[clust.labels_]
# clust.children_
clust.labels_
```

```{python}
# commun = list(list(i) for i in nx.community.greedy_modularity_communities(G))

# pal = sns.color_palette(n_colors=len(commun))
# colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
```

## Les Miserables Network

```{python}
chars = pd.read_csv(
    datadir/'lesmis'/'jean-nodes.csv',dtype={'id':'category'}
).set_index('id').assign(
    name=lambda df: df['name'].str.replace('\\','')
)

chartype = chars.index.dtype
JeanTidy = (
    pd.read_json(
        # datadir/'qual'/'jean-cooc.json', # orig
        datadir/'lesmis'/'jean-cooc-condense.json', # condense

        typ='series',
        orient='index', 
        # dtype=set, 
        convert_axes=False
    )
    .rename_axis('chapter')
    .rename('character')
    .apply(set).apply(list)      # condensed
    .explode().astype(chartype)  # condensed
    # .explode().reset_index().explode('character')  # orig
    # .rename_axis('scene')['character'].astype(chartype)  # orig
    .cat.rename_categories(chars['name'].to_dict())
    .dropna()
)
JeanDF=(
    JeanTidy
    .to_frame().reset_index()
    .assign(cooc=1).set_index(['chapter','character'])['cooc']  # condense
    # .assign(cooc=1).set_index(['scene','character'])['cooc']

    .unstack().fillna(0)
)

# JeanDF = JeanDF[JeanDF.sum(axis=1)>1]
# JeanDF = JeanDF.loc[:,JeanDF.sum()>1]
JeanDF = iter_filter(JeanDF, 2,2)
JeanTidy
```

```{python}
#| label: fig-lesmis-cooc
#| fig-cap: Les Miserables character co-occurrence network
#| lightbox: {desc-position: right, group: lesmis}
# plt.figure(figsize=(20,20))

G = nx.from_pandas_adjacency((JeanDF.T@JeanDF - np.diag(JeanDF.sum())))
pos_cos = nx.spring_layout(G, weight='weight', seed=2, iterations=1000)
# pos_cos = nx.kamada_kawai_layout(G)
commun = list(list(i) for i in nx.community.greedy_modularity_communities(G, weight='weight'))
# import itertools
# limited = itertools.takewhile(lambda c: len(c) <= 11, nx.community.girvan_newman(G))
# commun = list(list(i) for i in list(limited)[-1])

pal = sns.color_palette(n_colors=len(commun))
colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
plt.figure(figsize=(12,10))
ax = draw_graph_communities(G,pos_cos,colors)
cent=pd.Series(nx.centrality.eigenvector_centrality(G)).sort_values(ascending=False)#.head(10)#.rank(ascending=False)
# plt.figure(figsize=(3,4))
# sns.barplot(cent, orient='h')
insax = ax.inset_axes([0.7,0.8,0.3,0.2])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos_cos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=.1*np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
# plt.title(
#     'Les Misérables Character Dependency Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
```

```{python}
# from scipy.stats import percentileofscore
# plt.plot(np.linspace(0,1),np.gradient(percentileofscore(evd_L_pursuit, np.linspace(0,1))))
```

```{python}
# percentileofscore(evd_L_pursuit, 0.1), np.percentile(evd_L_pursuit, 95)
```

```{python}
#| label: fig-lesmis-fp
#| fig-cap: Les Miserables character dependency network (Forest Pursuit)
#| editable: true
#| lightbox: {desc-position: right, group: lesmis}
#| slideshow: {slide_type: ''}

evd_L_pursuit = _sq(asc.expected_forest_maximization(JeanDF.values))
# ~min_connected_filter(evd_L_pursuit).mask
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(evd_L_pursuit>=np.percentile(evd_L_pursuit, 95)), 
                                          index=JeanDF.columns, columns=JeanDF.columns))
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=s_subs, columns=s_subs))

# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=Xdf.columns, columns=Xdf.columns))

# G = G.subgraph(subset)
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
plt.figure(figsize=(12,10))
ax = draw_graph_communities(G, pos, colors)
centFP=pd.Series(nx.centrality.eigenvector_centrality(G)).sort_values(ascending=False)#.head(15)

insax = ax.inset_axes([0.7,0.8,0.3,0.2])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=.5*np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
# plt.title(
#     'Les Misérables Character Dependency Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
```

_TODO_: rank changes for different kinds of centrality? 

```{python}
plt.figure(figsize=(3,4))
sns.barplot(centFP.head(10), orient='h')
```

```{python}
from itertools import chain
commun_map=dict(chain.from_iterable(d.items() for d in [{char:n for char in i} for n,i in enumerate(commun)]))
cent_df = pd.concat([
    cent.rename('centrality').to_frame().rename_axis('character').reset_index().assign(graph='Orig.'),
    centFP.rename('centrality').to_frame().rename_axis('character').reset_index().assign(graph='FP')
]).assign(
    group=lambda df: df['character'].map(commun_map).astype('category'),
    rank = lambda df: df.groupby('graph')['centrality'].rank(ascending=False).astype(int)
)#.set_index(['character','graph'])['centrality'].unstack()
cent_df = cent_df.loc[
    cent_df.groupby('character')['rank'].transform('min')<=20
].set_index(['character','graph'])['centrality'].unstack()

#.reset_index()
# pd.DataFrame(dict(zip(range(len(commun)),commun)))
#.groups
# df.rank(
# cent_df.head()
# colors
cent_df
```

```{python}
from collections import defaultdict
from scipy import interpolate


def bumpsplot(dataframe, color_dict=defaultdict(lambda: "k"), 
                         linewidth_dict=defaultdict(lambda: 1),
                         labels=[], topn=15):
    """ adapted from 
    https://www.kaggle.com/code/markalec/olympics-streams-and-bumps-charts-in-python
    """
    og_ranks = dataframe.rank(method="first", ascending=False).astype(int)
    is_topn = (og_ranks.min(axis=1)<=topn)
    og_topn = og_ranks[og_ranks.min(axis=1)<=topn]
    r = dataframe.loc[og_ranks.min(axis=1)<=topn].rank(method="first")
    r = (r - r.max() + r.max().max()).fillna(0) # Sets NAs to 0 in rank
    def add_widths(x, y, width=0.1):
        """ Adds flat parts to widths """
        new_x = []
        new_y = []
        for i,j in zip(x,y):
            new_x += [i-width, i, i+width]
            new_y += [j, j, j]
        return new_x, new_y
    for n,i in enumerate(r.index):
        x = np.arange(r.shape[1])
        y = r.loc[i].values
        color = color_dict[i]
        lw = linewidth_dict[i]
        x, y = add_widths(x, y, width=0.1)
        xs = np.linspace(0, x[-1], num=1024)
        plt.plot(xs, interpolate.PchipInterpolator(x, y)(xs), color=color, linewidth=lw, alpha=0.5)
        if i in labels:
            plt.text(x[0] , y[0], s=f'{i} ({og_topn.loc[i].iloc[0]})', 
                     horizontalalignment="right", verticalalignment="center", 
                     color=color,weight='bold')
            plt.text(x[-1] + 0.1, y[-1], s=f'({og_topn.loc[i].iloc[-1]}) {i}', 
                     horizontalalignment="left", verticalalignment="center", 
                     color=color,weight='bold')
    plt.xticks(np.arange(r.shape[1]), dataframe.columns)
```

```{python}
#| label: fig-lesmis-centrality
#| editable: true
#| slideshow: {slide_type: ''}


plt.figure(figsize=(2,6))

lw = defaultdict(lambda: 1)
for c in [
    'Eponine',
    'Cosette',
    'Fantine',
    'Madamoiselle Gillenormand',
    "Madame Th'enardier",
    'Marguerite',
    'Anzelma',
]:
    lw[c] = 4
bumpsplot(
    cent_df[['Orig.','FP']],
    color_dict=colors,
    labels=cent_df.index,
    linewidth_dict=lw,
)#['rank'].unstack())
sns.despine(left=True)
plt.gca().get_yaxis().set_visible(False)
```

```{python}
lw
```

## Semantic Fluency (SNAFU)

```{python}
df = pd.read_csv(datadir/'snafu_sample.csv', dtype={'category':'category'})
idlist=df.id.rename('idlist').str.cat(df.listnum.astype(str))
df = df.assign(
    item=df['item']
     .str.replace('aligator', 'alligator')
     .str.replace('^a+rdva+rk', 'aardvark')
     .str.replace('baboob', 'baboon')
     .str.replace('antaloupe', 'antelope'),
    idlist=idlist
)
df=df.set_index([idlist, 'item'], drop=False)
df
```

```{python}
animals = (df

 .query('category=="animals"')#[['idlist','item']]
 .assign(animals=1.)['animals']
 .pipe(lambda df: df[~df.index.duplicated(keep='first')])
 .unstack().fillna(0.)#.drop_levels(0)
 .pipe(lambda df: df.loc[:,df.sum()>30])
)

all_X = animals.values
animals
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
from numpy.lib.stride_tricks import as_strided, sliding_window_view
def arr_cooc(x, n=2):
    return sliding_window_view(x,min(n, x.shape[0]))

animal_occ = (df
 .query('category=="animals"')
 .item.astype('category')
)
dummies = np.eye(animal_occ.dtype.categories.shape[0])
roll_X=np.vstack([
    dummies[arr_cooc(g[1].values, n=10),:].max(axis=1) 
    for g in animal_occ.cat.codes.groupby(level=0)
])#[:,]
roll_X = roll_X[
    :,animal_occ[
        animal_occ.isin(animals.columns.tolist())
    ].cat.codes.unique()
]
# roll_X.shape
# roll_X = roll_df.values
roll_X = roll_X[roll_X.sum(axis=1)>0]
roll_df = pd.DataFrame(data=roll_X, columns=animals.columns)
roll_df = iter_filter(roll_df, rowmin=2, colmin=2)
roll_X = roll_df.values

roll_df.shape#, roll_X.shape
# roll_df.sum()

# X = all_X
X = roll_df.values
animals.columns.sort_values() == roll_df.columns.sort_values()
# roll_X
```

```{python}
#| label: fig-fluency-workingmemory
#| fig-cap: Effects of rolling-window activations on observation data

# roll_df.query('spider==1 & whale==1').sum().sort_values(ascending=False).head(10)
# sns.histplot(all_X.sum(axis=1))

f = mpl.figure.Figure(figsize=(9,3))
s1,s2,s3 = f.subplots(1, 3, sharey=False)
sns.histplot(all_X.mean(axis=0), ax=s1, stat='density', label='complete list')
sns.histplot(roll_X.mean(axis=0), ax=s1, stat='density', label='working memory (10)')
s1.legend()
s1.set(xlabel='animal frequency')

sns.histplot(all_X.sum(axis=1), ax=s2, stat='probability', discrete=True)
sns.histplot(roll_X.sum(axis=1), ax=s2, stat='probability', discrete=True)

s2.set(xlabel='#animals/observation')
# s1.set_yscale('log')
sns.histplot(_sq(asc.ochiai(all_X, pseudocts=0.5)), stat='density', label='all co-occurrences', bins=20,ax=s3)
sns.histplot(_sq(asc.ochiai(roll_X, pseudocts=0.5)), stat='density', label='10 in-memory', bins=20,ax=s3)
s3.set_xlabel('cosine sim. (Ochiai coeff.)');
f.tight_layout(pad=2.)
f
```

```{python}
# min_connect_filter(_sq(sinkhorn(asc.coocur_prob(X)))).sum(),np.ma.masked_less_equal(_sq(sinkhorn(asc.coocur_prob(X))), 0.02376).sum()
# _sq(~np.ma.masked_less_equal(_sq(sinkhorn(asc.coocur_prob(X))), 0.02376).mask)

# A_cooc.sum()
# X
# A_cooc.sum()
# min_connected_filter(_sq(sinkhorn(asc.coocur_prob(X)))).min()
# np.unique(_sq(sinkhorn(asc.coocur_prob(X))))
# np.sort(_sq(sinkhorn(asc.coocur_prob(X))))
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
def connective_efficiency(e):
    n = n_nodes_from_edges(e)
    emin = n-1
    emax = n*(n-1)/2
    eff = emin/e.sum()
    return (eff-emin/emax)/(1-emin/emax)
    # return 1-(e.sum()-emin)/(emax-emin)

def hyp_proj(X): 
    w_hyp = 1/(X.sum(axis=1)-1)
    return _sq(((X.T*w_hyp)@X)) 


connective_efficiency(evd_L_pursuit>0.9)
# hyp_proj(roll_X)
roll_X.sum(axis=1)-1
```

```{python}
#| label: fig-fluency-dsmin
#| fig-cap: Verbal Fluency (animals) Network Backbone (Doubly-Stochastic)
#| lightbox:
#|   group: fluency
#| editable: true
#| slideshow: {slide_type: ''}

# A_cooc = roll_df.T@roll_df - np.diag(roll_df.sum())
evd_cooc = _sq(sinkhorn(asc.coocur_prob(X)))
# evd_cooc = hyp_proj(roll_X)

A_cooc = pd.DataFrame(
    _sq(e_cooc:= ~min_connected_filter(evd_cooc).mask), 
    index=roll_df.columns, columns=roll_df.columns
)
# A_cooc = pd.DataFrame(
#     _sq(~np.ma.masked_less_equal(_sq(sinkhorn(asc.coocur_prob(X))), 0.02376).mask), 
#     index=roll_df.columns, columns=roll_df.columns
# )

G = nx.from_pandas_adjacency(A_cooc)

# G = nx.from_pandas_adjacency(A_cooc>30)

subset = sorted(nx.connected_components(G), key=len, reverse=True)[0]
s_subs=pd.Series(list(subset), dtype=authortype)
# G = nx.from_pandas_adjacency((Xhyp.T@Xhyp - np.diag(np.diag(Xhyp.T@Xhyp)))>0.1)
# list(nx.neighbors(G, 'newman, m.'))
# print(len(subset))

# pos_cos = nx.kamada_kawai_layout(G)
pos_cos = nx.spring_layout(G, seed=2, iterations=1000, k=.5)
# G = G.subgraph(subset)

commun = list(list(i) for i in nx.community.greedy_modularity_communities(G))
pal = sns.color_palette(n_colors=len(commun))
colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G,pos_cos,colors)

plt.title(
    'Verbal Fluency Animals (DS-filtered) Co-Occurrence Graph '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f}) '
    fr'($\psi$ = {connective_efficiency(e_cooc):.2f})'
    # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
);
insax = ax.inset_axes([0,0.6,0.3,0.3])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos_cos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
centCS=pd.Series(nx.centrality.eigenvector_centrality_numpy(G)).sort_values(ascending=False)#.head(15)
centCS.head(15)
```

```{python}
#| label: fig-fluency-tree
#| fig-cap: Verbal Fluency (animals) Dependency Network (Chow-Liu Tree)
#| lightbox:
#|   group: fluency
#| editable: true
#| slideshow: {slide_type: ''}

e_tree = _sq(asc.chow_liu(X, pseudocts=0.5)>0)
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(e_tree), index=roll_df.columns, columns=roll_df.columns))
pos_tree = nx.kamada_kawai_layout(G, pos=pos_cos)
# G = G.subgraph(subset)

# nx.draw_networkx(G, pos=pos_tree, node_color='w')
# nx.draw_networkx_labels(G, pos=pos_tree, font_color=colors);
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G, pos_tree, colors)
nx.connected.is_connected(G)
# plt.annotate(f'r = {nx.degree_assortativity_coefficient(G):.3f}', (0.25, 0.5), xycoords='axes fraction', size=20)
plt.title(
    'Verbal Fluency Animal Dependencies (Chow-Liu) Network '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
    fr'($\psi$ = {connective_efficiency(e_tree):.2f})'
    # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
);
insax = ax.inset_axes([0,0.0,0.3,0.3])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos_tree[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
#| label: fig-fluency-glassomin
#| fig-cap: Verbal Fluency (animals) Dependency Network (GLASSO)
#| lightbox:
#|   group: fluency
#| editable: true
#| slideshow: {slide_type: ''}

from sklearn.covariance import GraphicalLasso, GraphicalLassoCV
from affinis.associations import coocur_prob
# graphical_lasso()
# plt.figure(figsize=(15,15))
glasso = (-(
    _sq(
        GraphicalLassoCV( 
            # alpha=0.0001,
            # covariance='precomputed',
            assume_centered=True,
        )
        .fit(X)
        # .fit(coocur_prob(X))
        .get_precision()
)))


G = nx.from_pandas_adjacency(pd.DataFrame(_sq(e_gl:=~min_connected_filter(glasso).mask), index=animals.columns, columns=animals.columns))
# pos_cos = nx.kamada_kawai_layout(G, dist = pd.DataFrame(-np.log(ochiai(X)), columns=animals.columns, index=animals.columns).to_dict())
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
# nx.draw_networkx(G, pos=pos, node_color='w')
# nx.connected.is_connected(G)
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G,pos,colors)
plt.title(
    'Verbal Fluency Animal Dependencies (GLASSO) Network '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
    fr'($\psi$ = {connective_efficiency(e_gl):.2f})'
);
insax = ax.inset_axes([0,0,0.2,0.2])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
centGL=pd.Series(nx.centrality.eigenvector_centrality_numpy(G)).sort_values(ascending=False)#.head(15)
centGL.head(15)
```

```{python}
#| label: fig-fluency-fpmin
#| fig-cap: Verbal Fluency (animals) Dependency Network (Forest Pursuit)
#| lightbox:
#|   group: fluency
#| editable: true
#| slideshow: {slide_type: ''}

evd_L_pursuit = _sq(asc.forest_pursuit_edge(X))#[s_subs.cat.codes].T[s_subs.cat.codes])
# A_fp = pd.DataFrame(_sq(evd_L_pursuit), index=s_subs, columns=s_subs)
# G = nx.from_pandas_adjacency(A_fp>0.01)
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=s_subs, columns=s_subs))
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=roll_df.columns, columns=roll_df.columns))
G = nx.from_pandas_adjacency(pd.DataFrame(
    _sq(e_fp := ~min_connected_filter(evd_L_pursuit).mask),
    # _sq(~np.ma.masked_less_equal(evd_L_pursuit, np.percentile(evd_L_pursuit, 96)).mask),

    index=roll_df.columns, columns=roll_df.columns
))

# G = G.subgraph(subset)
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
# pos = nx.spring_layout(G)
plt.figure(figsize=(12,10))

ax = draw_graph_communities(G,pos,colors)
plt.title(
    'Verbal Fluency Animal Dependencies (FP) Network Estimate '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
    fr'($\psi$ = {connective_efficiency(e_fp):.2f})'

    # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
);
# nx.is_connected(G)
insax = ax.inset_axes([0,0.6,0.3,0.3])

QG = nx.quotient_graph(G,commun, relabel=True)
qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
nx.draw_networkx(
    QG, pos=qpos, node_color=pal, 
    node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
    width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
    edge_color='grey', with_labels=False,
    ax=insax,
)
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
nx.eigenvector_centrality_numpy(G)
centFP=pd.Series(nx.centrality.eigenvector_centrality_numpy(G)).sort_values(ascending=False)#.head(15)
centFP.head(15)
```

TODO: edge type annotation? 
- co-located
- taxonomic
- predator/prey
- pop-culture
- similar ecological niche/role
- conservation/public awareness
- further investigation required!

```{python}
#| label: fig-fluency-preservation
#| fig-cap: Differences in structural preservation with over-thresholding.
#| fig-subcap:
#|   - co-occurrence retains local communities at the cost of global structure
#|   - clique-bias correction preserves central structure by disconnecting rare nodes
#| editable: true
#| slideshow: {slide_type: ''}


plt.figure(figsize=(10,6))
A_cooc = pd.DataFrame(
    _sq(e_cooc_98:=~np.ma.masked_less_equal(evd_cooc,np.percentile(evd_cooc, 98)).mask), 
    index=roll_df.columns, columns=roll_df.columns
)

G = nx.from_pandas_adjacency(A_cooc)
pos_cos = nx.spring_layout(G, seed=2)

draw_graph_communities(G,pos_cos,colors)

plt.title(
    'Verbal Fluency Animals Co-Occurrence (DS 98%) Network '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f}) '
    fr'($\psi$ = {connective_efficiency(e_cooc_98):.2f})'
    # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
);

plt.figure(figsize=(10,6))

evd_L_pursuit = _sq(asc.forest_pursuit_edge(X))#[s_subs.cat.codes].T[s_subs.cat.codes])
G = nx.from_pandas_adjacency(pd.DataFrame(
    _sq(e_fp_98 := ~np.ma.masked_less_equal(evd_L_pursuit, np.percentile(evd_L_pursuit, 98)).mask),
    index=roll_df.columns, columns=roll_df.columns
))
# pos = nx.kamada_kawai_layout(G)
pos = nx.spring_layout(G, seed=2)

draw_graph_communities(G,pos,colors)
plt.title(
    'Verbal Fluency Animal Dependencies (FP 98%) Network '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
    fr'($\psi$ = {connective_efficiency(e_fp_98):.2f})'
)
# plt.tight_layout()
plt.show()
```

### Using FP as Preprocessing

```{python}
from affinis.utils import sparse_adj_to_incidence, _norm_diag
from affinis.associations import _spanning_forests_obs_bootstrap
E_obs = _spanning_forests_obs_bootstrap(X)
n1, n2 = np.triu(_sq(evd_L_pursuit)).nonzero()
# print(n1.shape)
e = np.ma.nonzero(evd_L_pursuit)[0]
print(e.shape, n1.shape, n2.shape)
B = sprs.coo_array((np.concatenate([evd_L_pursuit, -evd_L_pursuit]), (np.concatenate([e,e]),np.concatenate([n1,n2]))), shape=(e.shape[0], X.shape[1]))

# np.diag((B.T@B).toarray())==np.diag(nx.laplacian_matrix(G).toarray()).round(1)
Xest=(E_obs@(np.abs(B))).toarray()
```

```{python}
#| label: fig-fluency-preprocess
#| fig-cap: Forest Pursuit preprocessing for Doubly-Stochastic and GLASSO recovered networks
#| fig-subcap:
#|   - Doubly Stochastic filter with FP (node degree) preprocessing
#|   - GLASSO Precision estimate with FP (node-degree) preprocessing
#| editable: true
#| slideshow: {slide_type: ''}



Gest = ~min_connected_filter(_sq(sinkhorn(coocur_prob(Xest)))).mask
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(Gest), index=animals.columns, columns=animals.columns))
#
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
plt.figure(figsize=(10,6))
draw_graph_communities(G,pos,colors)
plt.title(
    r' Animal Dependencies (FP$\rightarrow$DS) Network '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
    fr'($\psi$ = {connective_efficiency(Gest):.2f})'
)

plt.figure(figsize=(10,6))
glasso = (np.abs(
    _sq(
        GraphicalLasso(assume_centered=True)
        # .fit(ochiai(X))
        .fit(Xest)
        .get_precision()
)))

# Gest = min_connected_filter(_sq(glasso))
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(glasso).mask), index=animals.columns, columns=animals.columns))
#
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
draw_graph_communities(G,pos, colors)
plt.title(
    r' Animal Dependencies (FP$\rightarrow$GLASSO) Network '
    f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
    fr'($\psi$ = {connective_efficiency(Gest):.2f})'
);
```

```{python}
# plt.plot(np.linspace(0,1),np.gradient(percentileofscore(evd_L_pursuit, np.linspace(0,1))))
# np.percentile(evd_L_pursuit, 96)
```
