---
title: Source for Case Studies
lightbox: auto
format:
  html:
    fig-width: 10
    fig-height: 6
  umd-thesis-pdf:
    fig-width: 10
    fig-height: 6
execute:
  cache: true
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.17.2
  kernelspec:
    display_name: Python (Pixi)
    language: python
    name: pixi-kernel-python3
---


```{python}
import awkward as ak
from ruamel.yaml import YAML
from pathlib import Path 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.patheffects as path_effects
import matplotlib as mpl
import affinis.associations as asc
from affinis.utils import _sq, n_nodes_from_edges
from affinis.filter import min_connected_filter,edge_mask_to_laplacian
from affinis.proximity import sinkhorn, forest_correlation
from affinis.distance import generalized_graph_dists, adjusted_forest_dists
import scipy.sparse as sprs
from scipy.stats import powerlaw

from adjustText import adjust_text

import numpy as np
import networkx as nx
```

```{python}
mpl.rcParams['font.family']=['serif']
mpl.rcParams['font.serif']=['Bitstream Charter']
sns.set_theme(font='serif',palette='Set2', context='paper', style='ticks')
figsize = (9,6)
mpl.rcParams['figure.figsize'] = figsize
# figsize = mpl.rcParams['figure.figsize']
mpl.rcParams['savefig.transparent'] = True
mpl.rcParams["figure.facecolor"] = (1.0, 1.0, 1.0, 0.0) 
mpl.rcParams["axes.facecolor"] = (1.0, 1.0, 1.0, 0.0) 

%config InlineBackend.figure_formats = ['svg']

nxlabel_kws={
    # "font_color": "white",
    "font_color": "0.5",
    # "font_family":"Bitstream Charter",
    "font_family":"Cooper Hewitt",
    "font_weight":'heavy',
    "font_size": 6,
}

nxdraw_kws = {
    "node_size": 100, 
    # "node_color": "xkcd:slate",
    # "edgecolors": "grey",
    "edge_color": "grey",
    "linewidths": 1,
    "width": 2,
} | nxlabel_kws
```

## Network Scientist Network

```{python}
yaml = YAML(typ='safe')
# data = yaml.load(path)
datadir = Path('../../data/qual')

# s = ak.from_iter(yaml.load(datadir/'complex-networks.yaml').items())
# s = ak.Array(dict(zip(['id','meta'], ak.unzip(s))))
# ak.from_json()

df1 = (
    pd.DataFrame
    .from_dict(
        yaml.load(datadir/'netsci'/'complex-networks.yaml'), 
        orient='index'
    )
    .rename_axis('id')
)
df2 = (
    pd.DataFrame
    .from_dict(
        yaml.load(datadir/'netsci'/'complex-networks-struc-dyn.yaml'), 
        orient='index'
    )
    .rename_axis('id')
)
df3 = (
    pd.DataFrame
    .from_dict(
        yaml.load(datadir/'netsci'/'large-systems.yaml'), 
        orient='index'
    )
    .rename_axis('id')
)
df = pd.concat([df1, df2, df3])
df = df[~df.index.duplicated(keep='last')]
```

```{python}
# (~df2.index.isin(df1.index)).sum()

# df[df.index.duplicated(keep=False)]
# df
```

```{python}
# ak.to_dataframe(s)
tidy=(df
 .explode('author')
 .reset_index()
 .assign(
     coauth=1,
     author=lambda df: (
         df.author
         .str.capitalize()
         # .str.split(', ')
         # .str[0]
         # .str[:-4]
         # .astype('category')
     )
 )
 # .set_index(['id','author'])
 # ['coauth'].unstack().fillna(0)
)
# tidy = tidy[~tidy.set_index(['id','author'])['coauth'].index.duplicated()]
# tidy.iloc[tidy.set_index(['id','author']).index.duplicated()]
# df.loc['WOS:000226704200004']
# pd.Series.duplicated(
```

```{python}
# filt = tidy.set_index(['id','author'])['coauth'].index.duplicated(keep=False)

# pd.Series.duplicated()
```

```{python}
Xdf = (
    tidy
    .set_index(['id','author'])
    ['coauth'].unstack().fillna(0)
)

def iter_filter(Xdf, rowmin=2, colmin=2,maxiter=50):
    colsum = Xdf.sum(axis=0)
    # colsum = Xdf.sum()
    for it in range(maxiter):
        Xdf = Xdf.loc[:,colsum>=colmin]
        rowsum = Xdf.sum(axis=1)
        if rowsum.min()>=rowmin: 
            break
        Xdf = Xdf.loc[rowsum>=rowmin]
        colsum = Xdf.sum(axis=0) 
        if  colsum.min()>=colmin:
            break
    return Xdf
# Xdf = Xdf[Xdf.sum(axis=1)>1]
# Xdf = Xdf.loc[:,Xdf.sum()>1]
# Xdf = Xdf[Xdf.sum(axis=1)>1]
# Xdf = Xdf.loc[:,Xdf.sum()>1]
print('papers,authors')
print('before filtering:',Xdf.shape)
Xdf = iter_filter(Xdf, rowmin=2,colmin=2)
print('after filtering:',Xdf.shape)
# Xdf = Xdf[Xdf.sum(axis=1)>1]

X = Xdf.values#.astype(bool)
Xdf.columns = Xdf.columns.astype('category')
authortype=Xdf.columns.dtype#.categories
authortype.categories
```

```{python}
f = mpl.figure.Figure(figsize=(6,2))
s1,s2 = f.subplots(1, 2, sharey=True)
sns.histplot(Xdf.sum(), bins=Xdf.sum().max(), discrete=True, ax=s1)
s1.set(yscale='log', xlabel='#papers/author')

sns.histplot(Xdf.sum(axis=1), bins=Xdf.sum(axis=1).max(), discrete=True, ax=s2)
s2.set(xlabel='#authors/paper')
# s1.set_yscale('log')
f
```

```{python}
# Xhyp = (Xdf.T/(Xdf.sum(axis=1)-1)).T

# Xhyp.T@Xhyp - np.diag(np.diag(Xhyp.T@Xhyp))#-np.diag(Xhyp.sum())
```

```{python}
# sns.heatmap(Xdf.T@Xdf - np.diag(Xdf.sum()))
```

```{python}
# figsize=(10,8)

def log_lin_norm(a, vmin=2, vmax=10): 
    ln_a = np.log(1+a)
    med = (vmax-vmin)/2
    spread = med-vmin
    # a_std =  (ln_a - ln_a.min())/(ln_a.max()-ln_a.min())
    a_std =  (ln_a - ln_a.mean())/ln_a.std()
    return a_std*spread+med
    # return a_std*(vmax - vmin) + vmin


def draw_graph_communities(
    G, pos, colors, 
    ax=None, 
    commun=None, 
    annot='',
    insax_bound = [0,0,0.2,0.3],
    adjust_labels=True,
):
    if ax is None: 
        ax = plt.gca()
    grph = nx.draw_networkx_nodes(
        G, pos=pos, 
        # with_labels=False, 
        node_color=[colors[i] for i in G.nodes], 
        # node_color=None,
        ax=ax,
        node_size=nxdraw_kws['node_size'],
        # **nxdraw_kws
    )
    nx.draw_networkx_edges(
        G, pos=pos,
        # "edgecolors": "grey",
        edge_color= "grey",
        # width=1,
        width=2,
        ax=ax,
    )
# nx.connected.is_connected(G)
    labels=nx.draw_networkx_labels(
        G, pos=pos,
        # bbox=dict(edgecolor='grey',boxstyle='round,pad=0.2', alpha=0.5), 
        # bbox=dict(boxstyle='round,pad=0.2', alpha=0.5, facecolor='white'), 
        horizontalalignment='center',
        ax=ax,
        **nxlabel_kws
    )
    #iterate over the labels (key = label, value=matplotlib Text object)
    ax.margins(0)
    labels = list(labels.values())
    
    for t in labels:
        #manipulate indiviual text objects
        # print(t)
        # t.set_backgroundcolor(colors[t.get_text()])
        t.set_path_effects([
            path_effects.Stroke(linewidth=1.5, foreground='white'),
            path_effects.Normal()
        ])
        # t.set_color(colors[t.get_text()])
        # t.set_text(t.get_text()[:-3])
    

    sns.despine(ax=ax, left=True, bottom=True)
    obj_avoid=None
    if commun: 
        insax = ax.inset_axes(insax_bound)
        QG = nx.quotient_graph(G,commun, relabel=True)
        qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
        sns.color_palette(n_colors=len(commun))
        edge_weights = np.array(list(nx.get_edge_attributes(QG,'weight').values()))
        node_weights = np.array(list(nx.get_node_attributes(QG,'nnodes').values()))
        nx.draw_networkx(
            QG, pos=qpos, node_color=pal, 
            node_size=log_lin_norm(node_weights, vmin=50,vmax=300),
            width=log_lin_norm(edge_weights),
            edge_color='grey', with_labels=False,
            ax=insax,
        )
        insax.set_title('Community Block\nQuotient Graph')
        insax_annot = insax.annotate(
            # f'r = {nx.degree_assortativity_coefficient(G):.3f}',
            annot,
            (0.05,-0.1),xycoords='axes fraction', fontsize=9,
        )
        obj_avoid = [insax, insax_annot]
    if adjust_labels:
        labels, label_links = adjust_text(
            labels, ax=ax,
            arrowprops=dict(arrowstyle="-", color='0.5', lw=0.6), 
            objects=obj_avoid,
            # avoid_self=False,
            # pull_threshold=50,
            # max_move=50,
            time_lim=4,
            # force_pull = (.2,0.2),
            # force_text=(.5,1),
            # force_explode=(0.7,.7),
            # force_static=(0.1,.2),
            # expand=(2,2),
            expand_axes=True,
        )

        
    return ax
```

```{python}
#| label: fig-netsci-cooc
#| fig-cap: 134 Network scientists connected by co-authorship
#| editable: true
#| lightbox: {desc-position: right, group: netsci}
#| slideshow: {slide_type: ''}

A_cooc = Xdf.T@Xdf - np.diag(Xdf.sum())
G = nx.from_pandas_adjacency(A_cooc>0)

subset = sorted(nx.connected_components(G), key=len, reverse=True)[0]
s_subs=pd.Series(list(subset), dtype=authortype)
# G = nx.from_pandas_adjacency((Xhyp.T@Xhyp - np.diag(np.diag(Xhyp.T@Xhyp)))>0.1)
# list(nx.neighbors(G, 'newman, m.'))

pos_cos = nx.kamada_kawai_layout(G)
# pos_cos = nx.fruchterman_reingold_layout(G, seed=2)
# pos_cos = nx.forceatlas2_layout(G, seed=1, dissuade_hubs=True,distributed_action=True,)
G = G.subgraph(subset)

commun = list(list(i) for i in nx.community.greedy_modularity_communities(G))
pal = sns.color_palette(n_colors=len(commun))
colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
plt.figure()

ax = draw_graph_communities(
    G,pos_cos,colors, 
    commun=commun, 
    annot= f'r = {nx.degree_assortativity_coefficient(G):.3f}',
    # adjust_labels=False
    # insax_bound=[0,0.7,0.2,0.3],
)

        
# print(len(subset))

# insax = ax.inset_axes([0,0,0.2,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos_cos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=10*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# insax.set_title('Community Block\nQuotient Graph')
# insax.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G):.3f}',
#     (0.05,0.9),xycoords='axes fraction', fontsize=9,
# );
```

```{python}
#| label: fig-netsci-tree
#| fig-cap: Chow-Liu tree of NetSci collaborator dependency relationships
#| lightbox: {desc-position: right, group: netsci}

G = nx.from_pandas_adjacency(pd.DataFrame(asc.chow_liu(X, pseudocts=0.5), index=Xdf.columns, columns=Xdf.columns))
pos_tree = nx.kamada_kawai_layout(G, pos=pos_cos)
G = G.subgraph(subset)

# nx.draw_networkx(G, pos=pos_tree, node_color='w')
# nx.draw_networkx_labels(G, pos=pos_tree, font_color=colors);
plt.figure()
ax = draw_graph_communities(
    G,pos_tree,colors, 
    commun=commun, 
    # insax_bound=[0,0.7,0.2,0.3],
    annot= f'r = {nx.degree_assortativity_coefficient(G):.3f}'
)
# ax = draw_graph_communities(G, pos_tree, colors)
# nx.connected.is_connected(G)
# # plt.annotate(f'r = {nx.degree_assortativity_coefficient(G):.3f}', (0.25, 0.5), xycoords='axes fraction', size=20)
# # plt.title(
# #     'Network Scientist Collaborator Chow-Liu Tree '
# #     f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
# #     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# # );
# insax = ax.inset_axes([0,0,0.2,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos_tree[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=10*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# insax.set_title('Community Block\nQuotient Graph')
# insax.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G):.3f}',
#     (0.05,0.9),xycoords='axes fraction', fontsize=9,
# );
```

```{python}
#| label: fig-netsci-fp
#| fig-cap: Forest Pursuit estimate of NetSci collaborator dependency relationships
#| lightbox: {desc-position: right, group: netsci}

evd_L_pursuit = _sq(asc.forest_pursuit_edge(X)[s_subs.cat.codes].T[s_subs.cat.codes])
A_fp = pd.DataFrame(_sq(evd_L_pursuit), index=s_subs, columns=s_subs)
G = nx.from_pandas_adjacency(A_fp>0.01)
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=s_subs, columns=s_subs))
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=Xdf.columns, columns=Xdf.columns))
# G = G.subgraph(subset)
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
plt.figure()
ax = draw_graph_communities(
    G,pos,colors, 
    commun=commun, 
    annot= f'r = {nx.degree_assortativity_coefficient(G):.3f}'
)
# ax = draw_graph_communities(G,pos,colors)
# # plt.title(
# #     'Network Scientist Collaboration Network Estimate '
# #     f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
# #     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# # );

# insax = ax.inset_axes([0,0,0.2,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=10*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# insax.set_title('Community Block\nQuotient Graph')
# insax.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G):.3f}',
#     (0.05,0.9),xycoords='axes fraction', fontsize=9,
# );
```

```{python}
nx.degree_assortativity_coefficient(G)
# deg_cts = nx.degree_histogram(G)
# plt.bar(np.arange(0,len(deg_cts)),nx.degree_histogram(G))
```

```{python}
#| label: fig-netsci-degree
#| fig-cap: Degree distributions of FP vs co-occurrence social networks
degs = pd.DataFrame({
    'Orig.':(A_cooc>0).loc[list(subset),list(subset)].sum(), 
    'FP':(A_fp>0.01).sum()
}).melt(var_name='graph', value_name="degree")
# pd.DataFrame.melt()
plt.figure(figsize=(3,2))
sns.boxplot(
    degs, x='degree',y='graph',orient='h',
    # estimator='median',
    # errorbar=None,
    # linestyle='',
    color='grey',
    gap=.5,
    # marker='|',
    # markersize=15,
)
degs.groupby('graph').median();
```

```{python}
# L = edge_mask_to_laplacian(np.ma.masked_less_equal(evd_L_pursuit, 0.01))
L = nx.laplacian_matrix(G, nodelist=list(subset))
# sns.clustermap(pd.DataFrame(forest_correlation(L),  index=list(subset), columns=list(subset)))
from sklearn.cluster import AgglomerativeClustering
# sns.histplot(_sq(generalized_graph_dists(L)))
```

```{python}
from scipy.cluster.hierarchy import dendrogram

clust=AgglomerativeClustering(
    metric='precomputed', linkage='complete',#, distance_threshold=0.2,
    distance_threshold=4.5, n_clusters=None
).fit(adjusted_forest_dists(L.todense(), beta=100))

def plot_dendrogram(model, **kwargs):
    # Create linkage matrix and then plot the dendrogram

    # create the counts of samples under each node
    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)
    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack(
        [model.children_, model.distances_, counts]
    ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)
plt.figure(figsize=(12,5))
plot_dendrogram(clust)
```

```{python}
# np.array(pal)[clust.labels_]
# clust.children_
clust.labels_
```

```{python}
# commun = list(list(i) for i in nx.community.greedy_modularity_communities(G))

# pal = sns.color_palette(n_colors=len(commun))
# colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
```

## Les Miserables Network

```{python}
chars = pd.read_csv(
    datadir/'lesmis'/'jean-nodes.csv',dtype={'id':'category'}
).set_index('id').assign(
    name=lambda df: df['name'].str.replace('\\','')
)

chartype = chars.index.dtype
JeanTidy = (
    pd.read_json(
        # datadir/'qual'/'jean-cooc.json', # orig
        datadir/'lesmis'/'jean-cooc-condense.json', # condense

        typ='series',
        orient='index', 
        # dtype=set, 
        convert_axes=False
    )
    .rename_axis('chapter')
    .rename('character')
    .apply(set).apply(list)      # condensed
    .explode().astype(chartype)  # condensed
    # .explode().reset_index().explode('character')  # orig
    # .rename_axis('scene')['character'].astype(chartype)  # orig
    .cat.rename_categories(chars['name'].to_dict())
    .dropna()
)
JeanDF=(
    JeanTidy
    .to_frame().reset_index()
    .assign(cooc=1).set_index(['chapter','character'])['cooc']  # condense
    # .assign(cooc=1).set_index(['scene','character'])['cooc']

    .unstack().fillna(0)
)

# JeanDF = JeanDF[JeanDF.sum(axis=1)>1]
# JeanDF = JeanDF.loc[:,JeanDF.sum()>1]
JeanDF = iter_filter(JeanDF, 2,2)
JeanTidy
```

```{python}
#| label: fig-lesmis-cooc
#| fig-cap: Les Miserables character co-occurrence network
#| lightbox: {desc-position: right, group: lesmis}
# plt.figure(figsize=(20,20))

G = nx.from_pandas_adjacency((JeanDF.T@JeanDF - np.diag(JeanDF.sum())))
pos_cos = nx.spring_layout(G, weight='weight', seed=6, iterations=1000)
# pos_cos = nx.kamada_kawai_layout(G)
commun = list(list(i) for i in nx.community.greedy_modularity_communities(G, weight='weight'))
# import itertools
# limited = itertools.takewhile(lambda c: len(c) <= 11, nx.community.girvan_newman(G))
# commun = list(list(i) for i in list(limited)[-1])

pal = sns.color_palette(n_colors=len(commun))
colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
plt.figure()


ax = draw_graph_communities(
    G,pos_cos,colors, 
    commun=commun,
    insax_bound=[0,0.7,0.2,0.3],

)

cent=pd.Series(nx.centrality.eigenvector_centrality(G)).sort_values(ascending=False)#.head(10)#.rank(ascending=False)
# plt.figure(figsize=(3,4))
# sns.barplot(cent, orient='h')
# insax = ax.inset_axes([-.1,0.45,0.3,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos_cos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=10*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=.1*np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# insax.set_title('Community Block Quotient Graph')
# plt.tight_layout()
# plt.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G):.3f}',
#     (0.5,0.95),xycoords='axes fraction', fontsize=12,
# );
# plt.title(
#     'Les Misérables Character Dependency Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
```

```{python}
# from scipy.stats import percentileofscore
# plt.plot(np.linspace(0,1),np.gradient(percentileofscore(evd_L_pursuit, np.linspace(0,1))))
```

```{python}
# percentileofscore(evd_L_pursuit, 0.1), np.percentile(evd_L_pursuit, 95)
```

```{python}
#| label: fig-lesmis-fp
#| fig-cap: Les Miserables character dependency network (Forest Pursuit)
#| editable: true
#| lightbox: {desc-position: right, group: lesmis}
#| slideshow: {slide_type: ''}

evd_L_pursuit = _sq(asc.expected_forest_maximization(JeanDF.values))
# ~min_connected_filter(evd_L_pursuit).mask
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(evd_L_pursuit>=np.percentile(evd_L_pursuit, 95)), 
                                          index=JeanDF.columns, columns=JeanDF.columns))
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=s_subs, columns=s_subs))

# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=Xdf.columns, columns=Xdf.columns))

# G = G.subgraph(subset)
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
plt.figure()
ax = draw_graph_communities(G, pos, colors, commun=commun)
# centFP=pd.Series(nx.centrality.eigenvector_centrality(G)).sort_values(ascending=False)#.head(15)

# insax = ax.inset_axes([-.15,0.35,0.3,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=10*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=.5*np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# insax.set_title('Community Block Quotient Graph')
# plt.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G):.3f}',
#     (0.5,0.95),xycoords='axes fraction', fontsize=12,
# );
# plt.title(
#     'Les Misérables Character Dependency Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.3f})'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
```

_TODO_: rank changes for different kinds of centrality? 

```{python}
plt.figure(figsize=(3,4))
sns.barplot(centFP.head(10), orient='h')
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
from itertools import chain
commun_map=dict(chain.from_iterable(d.items() for d in [{char:n for char in i} for n,i in enumerate(commun)]))
cent_df = pd.concat([
    cent.rename('centrality').to_frame().rename_axis('character').reset_index().assign(graph='Orig.'),
    centFP.rename('centrality').to_frame().rename_axis('character').reset_index().assign(graph='FP')
]).assign(
    group=lambda df: df['character'].map(commun_map).astype('category'),
    rank = lambda df: df.groupby('graph')['centrality'].rank(ascending=False).astype(int)
)#.set_index(['character','graph'])['centrality'].unstack()
cent_df = cent_df.loc[
    cent_df.groupby('character')['rank'].transform('min')<=20
].set_index(['character','graph'])['centrality'].unstack()

#.reset_index()
# pd.DataFrame(dict(zip(range(len(commun)),commun)))
#.groups
# df.rank(
# cent_df.head()
# colors
cent_df
```

```{python}
from collections import defaultdict
from scipy import interpolate


def bumpsplot(dataframe, color_dict=defaultdict(lambda: "k"), 
                         linewidth_dict=defaultdict(lambda: 1),
                         labels=[], topn=15):
    """ adapted from 
    https://www.kaggle.com/code/markalec/olympics-streams-and-bumps-charts-in-python
    """
    og_ranks = dataframe.rank(method="first", ascending=False).astype(int)
    is_topn = (og_ranks.min(axis=1)<=topn)
    og_topn = og_ranks[og_ranks.min(axis=1)<=topn]
    r = dataframe.loc[og_ranks.min(axis=1)<=topn].rank(method="first")
    r = (r - r.max() + r.max().max()).fillna(0) # Sets NAs to 0 in rank
    def add_widths(x, y, width=0.1):
        """ Adds flat parts to widths """
        new_x = []
        new_y = []
        for i,j in zip(x,y):
            new_x += [i-width, i, i+width]
            new_y += [j, j, j]
        return new_x, new_y
    for n,i in enumerate(r.index):
        x = np.arange(r.shape[1])
        y = r.loc[i].values
        color = color_dict[i]
        lw = linewidth_dict[i]
        x, y = add_widths(x, y, width=0.1)
        xs = np.linspace(0, x[-1], num=1024)
        plt.plot(xs, interpolate.PchipInterpolator(x, y)(xs), color=color, linewidth=lw, alpha=0.5)
        if i in labels:
            plt.text(x[0] , y[0], s=f'{i} ({og_topn.loc[i].iloc[0]})', 
                     horizontalalignment="right", verticalalignment="center", 
                     color=color,weight='bold')
            plt.text(x[-1] + 0.1, y[-1], s=f'({og_topn.loc[i].iloc[-1]}) {i}', 
                     horizontalalignment="left", verticalalignment="center", 
                     color=color,weight='bold')
    plt.xticks(np.arange(r.shape[1]), dataframe.columns)
```

```{python}
#| label: fig-lesmis-centrality
#| fig-cap: Changes in character centrality ranking for FP vs co-occurrence
#| editable: true
#| slideshow: {slide_type: ''}

plt.figure(figsize=(2,6))

lw = defaultdict(lambda: 1)
for c in [
    'Eponine',
    'Cosette',
    'Fantine',
    'Madamoiselle Gillenormand',
    "Madame Th'enardier",
    'Marguerite',
    'Anzelma',
]:
    lw[c] = 4
bumpsplot(
    cent_df[['Orig.','FP']],
    color_dict=colors,
    labels=cent_df.index,
    linewidth_dict=lw,
)#['rank'].unstack())
sns.despine(left=True)
plt.gca().get_yaxis().set_visible(False)
```

```{python}
plt.figure(figsize=(2,6))

important_chars = [
    'Jean Valjean',
    'Javert',
    'Eponine',
    'Cosette',
    'Fantine',
    'Marius',
    "Th'enardier",
    "Mme. Th'enardier",
    "Gavroche",
    "Bishop Myriel",
]

lw = defaultdict(lambda: 1)
for c in important_chars:
    lw[c] = 4
bumpsplot(
    cent_df[['Orig.','FP']],
    color_dict=colors,
    labels=cent_df.index,
    linewidth_dict=lw,
)#['rank'].unstack())
sns.despine(left=True)
plt.gca().get_yaxis().set_visible(False)
plt.savefig('../images/lesmis-cent-topchars.pdf', bbox_inches='tight')
```

## Semantic Fluency (SNAFU)

```{python}
df = pd.read_csv(datadir/'snafu_sample.csv', dtype={'category':'category'})
idlist=df.id.rename('idlist').str.cat(df.listnum.astype(str))
df = df.assign(
    item=df['item']
     .str.replace('aligator', 'alligator')
     .str.replace('^a+rdva+rk', 'aardvark')
     .str.replace('baboob', 'baboon')
     .str.replace('antaloupe', 'antelope'),
    idlist=idlist
)
df=df.set_index([idlist, 'item'], drop=False)
df
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
animals = (df

 .query('category=="animals"')#[['idlist','item']]
 .assign(animals=1.)['animals']
 .pipe(lambda df: df[~df.index.duplicated(keep='first')])
 .unstack().fillna(0.)#.drop_levels(0)
 # .pipe(lambda df: df.loc[:,df.sum()>0])
)

animals
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
animal_occ = (df
 .query('category=="animals"')
 .item.astype('category')
)
animals_list = iter_filter(animals, rowmin=2, colmin=30).sum().sort_values(ascending=False)
animals_list = animals_list.reset_index()['item'].astype(animal_occ.dtype)
all_X = animals[animals_list].values

# animal_occ
all_X.shape,animals_list.head(10)#.cat.codes
# animal_occ
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
from numpy.lib.stride_tricks import as_strided, sliding_window_view
def arr_cooc(x, n=2):
    return sliding_window_view(x,min(n, x.shape[0]))


dummies = np.eye(animal_occ.dtype.categories.shape[0])
roll_X=np.vstack([
    dummies[arr_cooc(g[1].values, n=10),:].max(axis=1) 
    for g in animal_occ.cat.codes.groupby(level=0)
])#[:,]
roll_X = roll_X[
    # :,animal_occ[
    #     animal_occ.isin(animals.columns.tolist())
    # ].cat.codes.unique()
    :,animals_list.cat.codes.tolist()
]
# roll_X.shape
# roll_X = roll_df.values
roll_X = roll_X[roll_X.sum(axis=1)>0]
roll_df = pd.DataFrame(data=roll_X, columns=animals_list.values)
roll_df = iter_filter(roll_df, rowmin=2, colmin=2)
roll_X = roll_df.values

roll_df.shape#, roll_X.shape
# roll_df.sum()

# X = all_X
X = roll_df.values
# animals.columns.sort_values() == roll_df.columns.sort_values()
X.shape,roll_df.sum().sort_values()
# roll_X
```

```{python}
#| label: fig-fluency-workingmemory
#| fig-cap: Effects of rolling-window activations on observation data
#| editable: true
#| slideshow: {slide_type: ''}

# roll_df.query('spider==1 & whale==1').sum().sort_values(ascending=False).head(10)
# sns.histplot(all_X.sum(axis=1))

f = mpl.figure.Figure(figsize=(6,1.618))
s1,s2,s3 = f.subplots(1, 3, sharey=True)
sns.histplot(all_X.mean(axis=0), ax=s1, stat='density', label='complete list', log_scale=True)
sns.histplot(roll_X.mean(axis=0), ax=s1, stat='density', label='working memory (10)', log_scale=True)
s1.set(xlabel='animal frequency', yscale='log')

sns.histplot(all_X.sum(axis=1), ax=s2, stat='density', log_scale=True)
sns.histplot(roll_X.sum(axis=1), ax=s2, stat='density', log_scale=True, bins=10)

s2.set(xlabel='#animals/observation', yscale='log')
# s1.set_yscale('log')
sns.histplot(_sq(asc.ochiai(all_X, pseudocts=0.5)), stat='density', label='all co-occurrences', bins=20,ax=s3)
sns.histplot(_sq(asc.ochiai(roll_X, pseudocts=0.5)), stat='density', label='10 in-memory', bins=20,ax=s3)
s3.set(xlabel='cosine sim. (Ochiai coeff.)',yscale='log');
s3.legend(ncols=2, bbox_to_anchor=(0.3,1.3))
# f.tight_layout(pad=2.)
f
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# min_connect_filter(_sq(sinkhorn(asc.coocur_prob(X)))).sum(),np.ma.masked_less_equal(_sq(sinkhorn(asc.coocur_prob(X))), 0.02376).sum()
# _sq(~np.ma.masked_less_equal(_sq(sinkhorn(asc.coocur_prob(X))), 0.02376).mask)

# A_cooc.sum()
# X
# A_cooc.sum()
# min_connected_filter(_sq(sinkhorn(asc.coocur_prob(X)))).min()
# np.unique(_sq(sinkhorn(asc.coocur_prob(X))))
# np.sort(_sq(sinkhorn(asc.coocur_prob(X))))
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
def connective_efficiency(e):
    n = n_nodes_from_edges(e)
    emin = n-1
    emax = n*(n-1)/2
    eff = emin/e.sum()
    return (eff-emin/emax)/(1-emin/emax)
    # return 1-(e.sum()-emin)/(emax-emin)

def hyp_proj(X): 
    w_hyp = 1/(X.sum(axis=1)-1)
    return _sq(((X.T*w_hyp)@X)) 


plt.plot(np.linspace(0.01,0.99),[connective_efficiency(evd_L_pursuit>i) for i in np.linspace(0.01,0.99)])
# hyp_proj(roll_X)
# roll_X.sum(axis=1)-1
```

```{python}
#| label: fig-fluency-dsmin
#| fig-cap: Verbal Fluency (animals) Network Backbone (Doubly-Stochastic)
#| editable: true
#| lightbox: {group: fluency}
#| slideshow: {slide_type: ''}

# A_cooc = roll_df.T@roll_df - np.diag(roll_df.sum())
evd_cooc = _sq(sinkhorn(asc.coocur_prob(X)))
# evd_cooc = hyp_proj(roll_X)

A_cooc = pd.DataFrame(
    _sq(e_cooc:= ~min_connected_filter(evd_cooc).mask), 
    index=roll_df.columns, columns=roll_df.columns
)
# A_cooc = pd.DataFrame(
#     _sq(~np.ma.masked_less_equal(_sq(sinkhorn(asc.coocur_prob(X))), 0.02376).mask), 
#     index=roll_df.columns, columns=roll_df.columns
# )

G_ds = nx.from_pandas_adjacency(A_cooc)

# G = nx.from_pandas_adjacency(A_cooc>30)
centCS=pd.Series(nx.centrality.eigenvector_centrality_numpy(G_ds)).sort_values(ascending=False)#.head(15)

subset = sorted(nx.connected_components(G_ds), key=len, reverse=True)[0]
s_subs=pd.Series(list(subset), dtype=authortype)
# G = nx.from_pandas_adjacency((Xhyp.T@Xhyp - np.diag(np.diag(Xhyp.T@Xhyp)))>0.1)
# list(nx.neighbors(G, 'newman, m.'))
# print(len(subset))

# pos_cos = nx.kamada_kawai_layout(G_ds)
pos_cos = nx.spring_layout(G_ds, seed=2, iterations=1000, k=0.8)
# G = G.subgraph(subset)

commun = list(list(i) for i in nx.community.greedy_modularity_communities(G_ds))
pal = sns.color_palette(n_colors=len(commun))
colors = pd.DataFrame({'color':pal,'nodes':commun}).explode('nodes').set_index('nodes').to_dict()['color']
animalfig_ds = plt.figure()
annot = (
    f'r = {nx.degree_assortativity_coefficient(G_ds):.2f}\t'
    fr'$\psi$ = {connective_efficiency(e_cooc):.2f}'
)
ax = draw_graph_communities(
    G_ds,pos_cos,colors,
    commun=commun,
    annot= annot,
)

# plt.title(
#     'Verbal Fluency Animals (DS-filtered) Co-Occurrence Graph '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f}) '
#     fr'($\psi$ = {connective_efficiency(e_cooc):.2f}'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
# insax = ax.inset_axes([0.6,0.6,0.3,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos_cos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# insax.set_title('Community Block Quotient Graph')
# plt.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G_ds):.2f} \t'
#     fr'$\psi$ = {connective_efficiency(e_cooc):.2f}',
#     (0.5,1.),xycoords='axes fraction', fontsize=12,
#     horizontalalignment='center'
# );
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
centCS.head(15)
```

```{python}
#| label: fig-fluency-tree
#| fig-cap: Verbal Fluency (animals) Dependency Network (Chow-Liu Tree)
#| editable: true
#| lightbox: {group: fluency}
#| slideshow: {slide_type: ''}

e_tree = _sq(asc.chow_liu(X, pseudocts=0.5)>0)
G_tr = nx.from_pandas_adjacency(pd.DataFrame(_sq(e_tree), index=roll_df.columns, columns=roll_df.columns))
pos_tree = nx.kamada_kawai_layout(G_tr, pos=pos_cos)
# G = G.subgraph(subset)

# nx.draw_networkx(G, pos=pos_tree, node_color='w')
# nx.draw_networkx_labels(G, pos=pos_tree, font_color=colors);
animalfig_tr = plt.figure()
annot = (
    f'r = {nx.degree_assortativity_coefficient(G_tr):.2f}\t'
    fr'$\psi$ = {connective_efficiency(e_tree):.2f}'
)
ax = draw_graph_communities(
    G_tr, pos_tree, colors,
    annot = annot,
    commun=commun,
)
nx.connected.is_connected(G_tr)
# plt.annotate(f'r = {nx.degree_assortativity_coefficient(G):.3f}', (0.25, 0.5), xycoords='axes fraction', size=20)
# plt.title(
#     'Verbal Fluency Animal Dependencies (Chow-Liu) Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
#     fr'($\psi$ = {connective_efficiency(e_tree):.2f})'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
# insax = ax.inset_axes([0.65,0.,0.3,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos_tree[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# plt.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G_tr):.2f} \t'
#     fr'$\psi$ = {connective_efficiency(e_tree):.2f}',
#     (0.5,0.95),xycoords='axes fraction', fontsize=12,
#     horizontalalignment='center'
# );
```

```{python}
#| label: fig-fluency-glassomin
#| fig-cap: Verbal Fluency (animals) Dependency Network (GLASSO)
#| editable: true
#| lightbox: {group: fluency}
#| slideshow: {slide_type: ''}

from sklearn.covariance import GraphicalLasso, GraphicalLassoCV
from affinis.associations import coocur_prob
# graphical_lasso()
# plt.figure(figsize=(15,15))
glasso = (-(
    _sq(
        GraphicalLassoCV( 
            # alpha=0.0001,
            # covariance='precomputed',
            assume_centered=True,
        )
        .fit(X)
        # .fit(coocur_prob(X))
        .get_precision()
)))


G_gl = nx.from_pandas_adjacency(pd.DataFrame(_sq(e_gl:=~min_connected_filter(glasso).mask), index=animals_list.values, columns=animals_list.values))
# pos_cos = nx.kamada_kawai_layout(G, dist = pd.DataFrame(-np.log(ochiai(X)), columns=animals.columns, index=animals.columns).to_dict())
centGL=pd.Series(nx.centrality.eigenvector_centrality_numpy(G_gl)).sort_values(ascending=False)#.head(15)

pos_gl = nx.kamada_kawai_layout(G_gl, pos=pos_cos)
# nx.draw_networkx(G, pos=pos, node_color='w')
# nx.connected.is_connected(G)
animalfig_gl = plt.figure()
annot = (
    f'r = {nx.degree_assortativity_coefficient(G_gl):.2f}\t'
    fr'$\psi$ = {connective_efficiency(e_gl):.2f}'
)
ax = draw_graph_communities(
    G_gl,pos_gl,colors,
    commun=commun,
    annot=annot,
)
# plt.title(
#     'Verbal Fluency Animal Dependencies (GLASSO) Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
#     fr'($\psi$ = {connective_efficiency(e_gl):.2f})'
# );
# insax = ax.inset_axes([0,0,0.3,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )
# plt.Text()
# plt.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G_gl):.2f} \t'
#     fr'$\psi$ = {connective_efficiency(e_gl):.2f}',
#     (0.5,0.95),xycoords='axes fraction', fontsize=12,
#     horizontalalignment='center'
# );
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
centGL.head(15)
```

```{python}
#| label: fig-fluency-fpmin
#| fig-cap: Verbal Fluency (animals) Dependency Network (Forest Pursuit)
#| editable: true
#| lightbox: {group: fluency}
#| slideshow: {slide_type: ''}

evd_L_pursuit = _sq(asc.forest_pursuit_edge(X))#[s_subs.cat.codes].T[s_subs.cat.codes])
# A_fp = pd.DataFrame(_sq(evd_L_pursuit), index=s_subs, columns=s_subs)
# G = nx.from_pandas_adjacency(A_fp>0.01)
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=s_subs, columns=s_subs))
# G = nx.from_pandas_adjacency(pd.DataFrame(_sq(~min_connected_filter(evd_L_pursuit).mask), index=roll_df.columns, columns=roll_df.columns))
G_fp = nx.from_pandas_adjacency(pd.DataFrame(
    _sq(e_fp := ~min_connected_filter(evd_L_pursuit).mask),
    # _sq(~np.ma.masked_less_equal(evd_L_pursuit, np.percentile(evd_L_pursuit, 96)).mask),

    index=roll_df.columns, columns=roll_df.columns
))
centFP=pd.Series(nx.centrality.eigenvector_centrality_numpy(G_fp)).sort_values(ascending=False)#.head(15)

# G = G.subgraph(subset)
pos_fp = nx.kamada_kawai_layout(G_fp, pos=pos_cos)
# pos = nx.spring_layout(G, pos=pos_cos, seed=2, iterations=1000, k=.5)
animalfig_fp = plt.figure()


annot = (
    f'r = {nx.degree_assortativity_coefficient(G_fp):.2f}\t'
    fr'$\psi$ = {connective_efficiency(e_fp):.2f}'
)
ax = draw_graph_communities(
    G_fp,pos_fp,colors,
    commun=commun,
    annot=annot
)
# plt.title(
#     'Verbal Fluency Animal Dependencies (FP) Network Estimate '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
#     fr'($\psi$ = {connective_efficiency(e_fp):.2f})'

#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );
# # nx.is_connected(G)
# insax = ax.inset_axes([0.,0.65,0.3,0.3])

# QG = nx.quotient_graph(G,commun, relabel=True)
# qpos={n:np.mean([pos[i] for i in g.nodes], axis=0) for n,g in nx.get_node_attributes(QG, 'graph').items()}
# nx.draw_networkx(
#     QG, pos=qpos, node_color=pal, 
#     node_size=30*np.array(list(nx.get_node_attributes(QG,'nnodes').values())),
#     width=np.array(list(nx.get_edge_attributes(QG,'weight').values())),
#     edge_color='grey', with_labels=False,
#     ax=insax,
# )

# plt.annotate(
#     f'r = {nx.degree_assortativity_coefficient(G_fp):.2f} \t'
#     fr'$\psi$ = {connective_efficiency(e_fp):.2f}',
#     (0.5,0.95),xycoords='axes fraction', fontsize=12,
#     horizontalalignment='center'
# );
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
nx.eigenvector_centrality_numpy(G_fp)
centFP.head(15)
```

```{python}
#| label: fig-fluency-combined

f = mpl.figure.Figure(figsize=(15,8))
axs = f.subplots(2, 2)#, height_ratios=(1,1,1,1)

draw_graph_communities(G_ds, pos_cos, colors, ax=axs[0,0])
axs[0,0].set_title('Doubly-Stochastic Filter')

draw_graph_communities(G_tr, pos_tree, colors, ax=axs[1,1])
axs[1,1].set_title('Chow-Liu Tree')

draw_graph_communities(G_gl, pos_gl, colors, ax=axs[1,0])
axs[1,0].set_title('GLASSO (min-connect filter)')

draw_graph_communities(G_fp, pos_fp, colors, ax=axs[0,1])
axs[0,1].set_title('Forest Pursuit (min-connect filter)')

# sf[0] = animalfig_ds
# f
# f.add_subfigure() animalfig_ds
# animalfig_ds.axes[0]
plt.tight_layout(pad=-1)
f
```

TODO: edge type annotation? 
- co-located
- taxonomic
- predator/prey
- pop-culture
- similar ecological niche/role
- conservation/public awareness
- further investigation required!

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
commun_map=dict(chain.from_iterable(d.items() for d in [{char:n for char in i} for n,i in enumerate(commun)]))
cent_df = pd.concat([
    centCS.rename('centrality').to_frame().rename_axis('animal').reset_index().assign(graph='DS'),
    centFP.rename('centrality').to_frame().rename_axis('animal').reset_index().assign(graph='FP'),
    centGL.rename('centrality').to_frame().rename_axis('animal').reset_index().assign(graph='GL')    
]).assign(
    group=lambda df: df['animal'].map(commun_map).astype('category'),
    rank = lambda df: df.groupby('graph')['centrality'].rank(ascending=False).astype(int)
)#.set_index(['character','graph'])['centrality'].unstack()
cent_df = cent_df.loc[
    cent_df.groupby('animal')['rank'].transform('min')<=15
].set_index(['animal','graph'])['centrality'].unstack()

#.reset_index()
# pd.DataFrame(dict(zip(range(len(commun)),commun)))
#.groups
# df.rank(
# cent_df.head()
# colors
cent_df.shape
```

```{python}
#| label: fig-animal-centrality
#| fig-cap: Changes in animal centrality ranking for FP vs co-occurrence,GLASSO
#| editable: true
#| slideshow: {slide_type: ''}


plt.figure(figsize=(4,6))

lw = defaultdict(lambda: 1)
for c in [
    # 'elephant',
    'cat',
    'lion',
    'chicken',
    'cow',
    'goat',
    'sheep',
    'deer',
    # 'hyena',
    # 'ox',
]:
    lw[c] = 4
bumpsplot(
    cent_df[['DS','GL','FP']],
    color_dict=colors,
    labels=cent_df.index,
    topn=25,
    linewidth_dict=lw,
)
sns.despine(left=True)
plt.gca().get_yaxis().set_visible(False)
```

```{python}
#| label: fig-fluency-preservation
#| fig-cap: Differences in structural preservation with over-thresholding.
#| fig-subcap:
#|   - co-occurrence retains local communities at the cost of global structure
#|   - clique-bias correction preserves central structure by disconnecting rare nodes
#| editable: true
#| slideshow: {slide_type: ''}


plt.figure(figsize=(7,5))
A_cooc = pd.DataFrame(
    _sq(e_cooc_98:=~np.ma.masked_less_equal(evd_cooc,np.percentile(evd_cooc, 98)).mask), 
    index=roll_df.columns, columns=roll_df.columns
)

G = nx.from_pandas_adjacency(A_cooc)
pos_cos = nx.spring_layout(G, seed=2)

draw_graph_communities(G,pos_cos,colors)

# plt.title(
#     'Verbal Fluency Animals Co-Occurrence (DS 98%) Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f}) '
#     fr'($\psi$ = {connective_efficiency(e_cooc_98):.2f})'
#     # f'(p = {powerlaw.fit([d for n,d in nx.degree(G)])[0]:.3f})'
# );

plt.figure(figsize=(7,5))

evd_L_pursuit = _sq(asc.forest_pursuit_edge(X))#[s_subs.cat.codes].T[s_subs.cat.codes])
G = nx.from_pandas_adjacency(pd.DataFrame(
    _sq(e_fp_98 := ~np.ma.masked_less_equal(evd_L_pursuit, np.percentile(evd_L_pursuit, 98)).mask),
    index=roll_df.columns, columns=roll_df.columns
))
# pos = nx.kamada_kawai_layout(G)
pos = nx.spring_layout(G, seed=2)

draw_graph_communities(G,pos,colors)
# plt.title(
#     'Verbal Fluency Animal Dependencies (FP 98%) Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
#     fr'($\psi$ = {connective_efficiency(e_fp_98):.2f})'
# )
# plt.tight_layout()
plt.show()
```

### Using FP as Preprocessing

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
from affinis.utils import sparse_adj_to_incidence, _norm_diag
from affinis.associations import _spanning_forests_obs_bootstrap
E_obs = _spanning_forests_obs_bootstrap(X)
n1, n2 = np.triu(_sq(evd_L_pursuit)).nonzero()
# print(n1.shape)
e = np.ma.nonzero(evd_L_pursuit)[0]
print(e.shape, n1.shape, n2.shape)
B = sprs.coo_array((np.concatenate([evd_L_pursuit, -evd_L_pursuit]), (np.concatenate([e,e]),np.concatenate([n1,n2]))), shape=(e.shape[0], X.shape[1]))

# np.diag((B.T@B).toarray())==np.diag(nx.laplacian_matrix(G).toarray()).round(1)
Xest=(E_obs@(np.abs(B))).toarray()
```

```{python}
#| label: fig-fluency-preprocess
#| fig-cap: Forest Pursuit preprocessing for Doubly-Stochastic and GLASSO recovered networks
#| fig-subcap:
#|   - Doubly Stochastic filter with FP (node degree) preprocessing
#|   - GLASSO Precision estimate with FP (node-degree) preprocessing
#| editable: true
#| slideshow: {slide_type: ''}



Gest = ~min_connected_filter(_sq(sinkhorn(coocur_prob(Xest)))).mask
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(Gest), index=animals_list.values, columns=animals_list.values))
#
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
plt.figure(figsize=(7,5))
draw_graph_communities(G,pos,colors)
plt.annotate(
    f'r = {nx.degree_assortativity_coefficient(G):.2f} \t'
    fr'$\psi$ = {connective_efficiency(Gest):.2f}',
    (0.5,0.95),xycoords='axes fraction', fontsize=12,
    horizontalalignment='center'
);
# plt.title(
#     r' Animal Dependencies (FP$\rightarrow$DS) Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
#     fr'($\psi$ = {connective_efficiency(Gest):.2f})'
# )

plt.figure(figsize=(7,5))
glasso = (np.abs(
    _sq(
        GraphicalLasso(assume_centered=True)
        # .fit(ochiai(X))
        .fit(Xest)
        .get_precision()
)))

# Gest = min_connected_filter(_sq(glasso))
G = nx.from_pandas_adjacency(pd.DataFrame(_sq(e_fp_gp:=~min_connected_filter(glasso).mask), index=animals_list.values, columns=animals_list.values))
#
pos = nx.kamada_kawai_layout(G, pos=pos_cos)
draw_graph_communities(G,pos, colors)
# plt.title(
#     r' Animal Dependencies (FP$\rightarrow$GLASSO) Network '
#     f'(r = {nx.degree_assortativity_coefficient(G):.2f})'
#     fr'($\psi$ = {connective_efficiency(Gest):.2f})'
# );
plt.annotate(
    f'r = {nx.degree_assortativity_coefficient(G):.2f} \t'
    fr'$\psi$ = {connective_efficiency(e_fp_gp):.2f}',
    (0.5,0.95),xycoords='axes fraction', fontsize=12,
    horizontalalignment='center'
);
```

```{python}
#| editable: true
#| slideshow: {slide_type: ''}
# plt.plot(np.linspace(0,1),np.gradient(percentileofscore(evd_L_pursuit, np.linspace(0,1))))
# np.percentile(evd_L_pursuit, 96)
```
