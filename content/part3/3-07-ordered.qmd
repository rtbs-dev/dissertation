# Recovery from Working Memory & Partial Orders {#sec-ordered}

This whole time we have assumed that the ordering of nodes was unknown, or at the very least unreliable.
However, there are frequently cases, especially in text processing applications, where we have some sense of an ordering on activations.
By _partial order_ on a set (a "poset"), we mean that all elements are either comparable as greater (before) or less (after), or incomparable.
The set of posets is therefore precisely isomorphic to the set of _directed acyclic graphs_, based on reachability. 

Our original example with authors might be thought of as a poset: (i) precedes/asks (f) and (j), (j) precedes/asks (b), but (b) and (f) are incomparable.
We don't know if (i) asked (f) before or after (j) asked (b).^[
  Interestingly, in the case where the node activations _are_ given a total order (in the form of _timestamps_), @Inferringnetworksdiffusion_GomezRodriguez2012 derive an algorithm called _NetInf_.
  It utilizes sums over minimum spanning trees that satisfy time constraints, similar to a special case of Forest Pursuit where all activations have a total ordering.
]

Sadly our co-authorship example does not often include the (partial) order of author additions,^[though some domains do imbue special meaning to the listed author order, a fact that might be interesting to investigate in future network recovery efforts!
] but other common network recovery problems do have an inherent order to them.
A very common need is to recover concept association networks, whether from lists of tags or directly from a corpus of written language.
What's needed is an assumption on how the observed partial order of concepts is generated.
@ForagingSemanticFields_Hills2015 proposes a "foraging" mechanism, so that concepts get sequentially recalled from "semantic patches" of nearby concepts in memory.
The partial order comes from our ability to maintain more than one concept in working memory, so that the next concept can be "foraged" from any of the other recently recalled ones[@magicalnumberseven_Miller1956; @Dynamicsearchworking_Hills2012]. 

In this section, we briefly cover a method for network inference by @Humanmemorysearch_Jun2015 that utilizes partial order information from ordered lists of concepts, called INVITE.
We use it to demonstrate improvement over bag-of-words and markov assumptions for downstream technical language processing [@Technicallanguageprocessing_Brundage2021] tasks, as originally demonstrated in [@UsingSemanticFluency_Sexton2019; and @OrganizingTaggedKnowledge_Sexton2020]

Finally, we show that using _Forest Pursuit_ for partially ordered data can still be quite useful for network backboning, and for a fraction of the computational cost.
We investigate a network recovery task from verbal/semantic fluency data [@Estimatingsemanticnetworks_Zemla2018], which involves recovery of a network of animal relationships from memory and recall experiments. 
Even without directly using partial order information, proper data preparation along with previously-discussed (un-ordered) recovery methods can lead to significantly improved network backboning and analysis capability


## Technical Language Processing with INVITE

Maintenance work orders are often represented as categorized data, though increasingly quantitative use of a technician's descriptions is being pursued by maintenance management operations [@BenchmarkingKeywordExtraction_Sexton2018;@CategorizationErrorsData_Sexton2019].
Tags are another way to flexibly structure this otherwise "unstructured" data, which @tble-mwo shows in comparison to more traditional categorization.

{{< embed ../codefigs/graphs.qmd#tbl-mwo >}}

Whether entered directly or generated from text by keyword extraction, the tags will tend to have ordering information readily available. 
A traditional way to model this kind of text is through either bag-of-words (the co-occurrence node activation data already discussed) or as a sequence of order-n markov model emissions.
Unlike the clique bias from before, assuming markov jumps for each observation leads to a different kind of bias, with higher precision but reduced recall as shown in @fig-stack-markov. 

{{< embed ../codefigs/graphs.qmd#fig-stack-markov >}}

Without knowing the underlying dependency relationships, it's difficult to estimate which edges were used by a random-walker, since subsequent visits in memory to a "tag" are not being reported once a technician first adds it.
[@Humanmemorysearch_Jun2015] call this an "Initial-visit-emitting" random walk, or INVITE for short. 
To more accurately recover the network, they suggest maximizing the absorption probability for each step of a partial order, individually, knowing which nodes have already been activated. 

### Optimizing absorbing-state probabilities

Say the set of components or concepts that have a corresponding tag in our system is denoted by the node-set $N$.
A user-given set of $T$ [^1] for a specific record can be denoted as a Random Walk (RW) trajectory $\mathbf{t}=\{t_1, t_2, t_3, \cdots t_{T}\}$, where $T\leq N$.
This limit on the size of $T$ assumes tags are a set of unique entries.
Any transitions between previously visited tags in $\mathbf{t}$ will not be directly observed, making the transitions observed in $\mathbf{t}$ strictly non-Markov, and allowing for a *potentially infinite* number of possible paths to arrive at the next tag *through previously visited ones*. 
Instead of directly computing over this intractable model for generating $\mathbf{t}$, the key insight from the original INVITE paper [@Humanmemorysearch_Jun2015] comes from partitioning $\mathbf{t}$ into $T-1$ Markov chains with absorbing states.
Previously visited tags are "transient" states, and unseen tags are "absorbing".
It is then possible to calculate the absorption probability into the $k$ transition ($t_k \rightarrow t_{k+1}$) using the *fundamental matrix* of each partition.
If the partitions at this jump consist of $q$ transient states with transition matrix among themselves $\mathbf{Q}^{(k)}_{q\times q}$, and $r$ absorbing states with transitions into them from $q$ as $\mathbf{R}^{(k)}_{q\times r}$, the Markov transition matrix $\mathbf{M}^{(k)}_{n\times n}$ has the form 
$$
\mathbf{M}^{(k)} =
    \begin{pmatrix}
        \mathbf{Q}^{(k)}  & \mathbf{R}^{(k)} \\
        \mathbf{0}        & \mathbf{I}
    \end{pmatrix}
$${#eq-trans-matrix}

Here $\mathbf{0}$, $\mathbf{I}$ represent lack of
transition between/from absorbing states.
It follows from [@RandomWalksElectric_Doyle2000] that the probability $P$ of a chain starting at $t_k$ being absorbed into state $k+1$, is given as
$$
\begin{gathered}
    P\left(t_{k+1} \middle| t_{1:k},\mathbf{M}\right) =
        \left.\mathbf{N}^{(k)}R^{(k)}\right|_{q,1}\\
\mathbf{N} = \left( \mathbf{I}-\mathbf{Q} \right) ^{-1}
\end{gathered}
$${#eq-absorb}

The probability of being absorbed at $k+1$ conditioned on jumps $1:k$ is thus equivalent to the probability of observing the $k+1$ INVITE tag.
If we approximate an a priori distribution of tag probabilities to initialize our chain as $t_1\sim\text{Cat}(n,\theta)$ (which could be empirically derived or simulated), then the likelihood of our observed tag chain $\mathbf{t}$, given a transition matrix, is:
$$
\mathcal{L}\left(\mathbf{t}| \theta, \mathbf{M}\right) =
        \theta(t_1)\prod_{k=1}^{T-1} P\left(t_{k+1}\,\middle|\ t_{1:k},\mathbf{M}\right)
$$

Finally, if we observe a set of tag lists $\mathbf{C} = \left\{ \mathbf{t}_1, \mathbf{t}_2, \cdots, \mathbf{t}_{c} \right\}$, and assume $\theta$ can be estimated independently of $\mathbf{M}$, then we can frame the problem of structure mining on observed INVITE data as a minimization of negative log-likelihood.
A point estimate for our association network given $\mathbf{M}$ can found as:
$$
    \mathbf{M}^* \leftarrow \operatorname*{argmin}_{\mathbf{M}} \quad
    \sum_{i=1}^{C}
    \sum_{k=1}^{T_i-1}
        -\log \mathcal{L} \left(t^{(i)}_{k+1} \middle| t^{(i)}_{1:k},\mathbf{M}\right)
$$

[^1]: While some sources use "tagging" as a proxy for a set of strictly
    un-ordered labels (as in multi-label classification), we preserve
    the mechanism by which the tags were generated in the first place,
    i.e., in a *specific* order.

### Application

- look, @eq-trans-matrix, @eq-absorb is the same as forest matrix.
- in og work, calculated for tag datasets. 

:::{#fig-excavate-tern}
![](../images/ternary_ntags3_freq5_topn50_thres60_saveTrue.svg)

Semisupervised MWO Classification with Network Label Propagation
:::

## Verbal Fluency Animal Network

[@magicalnumberseven_Miller1956]


{{< embed ../codefigs/qualitative.qmd#fig-fluency-workingmemory >}}

### Edge Connective Effiency and Diversity

{{< embed ../codefigs/qualitative.qmd#fig-fluency-dsmin >}}
{{< embed ../codefigs/qualitative.qmd#fig-fluency-tree >}}
{{< embed ../codefigs/qualitative.qmd#fig-fluency-glassomin >}}
{{< embed ../codefigs/qualitative.qmd#fig-fluency-fpmin >}}

### Thresholded Structure Preservation 

{{< embed ../codefigs/qualitative.qmd#fig-fluency-preservation >}}

<!-- {{< embed ../codefigs/results.qmd#fig-fp-compare >}} -->

### Forest Pursuit as Preprocessing {#sec-fp-preprocess}

{{< embed ../codefigs/qualitative.qmd#fig-fluency-preprocess >}}
