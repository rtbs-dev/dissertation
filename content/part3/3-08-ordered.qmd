# Recovery from Working Memory & Partial Orders {#sec-ordered}

This whole time we have assumed that the ordering of nodes was unknown, or at the very least unreliable.
However, there are frequently cases, especially in text processing applications, where we have some sense of an ordering on activations.
By _partial order_ on a set (a "poset"), we mean that all elements are either comparable as greater (before) or less (after), or incomparable.
The set of posets is therefore precisely isomorphic to the set of _directed acyclic graphs_, based on reachability. 

Our original example with authors might be thought of as a poset: (i) precedes/asks (f) and (j), (j) precedes/asks (b), but (b) and (f) are incomparable.
We don't know if (i) asked (f) before or after (j) asked (b).^[
  Interestingly, in the case where the node activations _are_ given a total order (in the form of _timestamps_), @Inferringnetworksdiffusion_GomezRodriguez2012 derive an algorithm called _NetInf_.
  It utilizes sums over minimum spanning trees that satisfy time constraints, similar to a special case of Forest Pursuit where all activations have a total ordering.
]

Sadly our co-authorship example does not often include the (partial) order of author additions,^[though some domains do imbue special meaning to the listed author order, a fact that might be interesting to investigate in future network recovery efforts!
] but other common network recovery problems do have an inherent order to them.
A very common need is to recover concept association networks, whether from lists of tags or directly from a corpus of written language.
What's needed is an assumption on how the observed partial order of concepts is generated.
@ForagingSemanticFields_Hills2015 proposes a "foraging" mechanism, so that concepts get sequentially recalled from "semantic patches" of nearby concepts in memory.
The partial order comes from our ability to maintain more than one concept in working memory, so that the next concept can be "foraged" from any of the other recently recalled ones[@magicalnumberseven_Miller1956; @Dynamicsearchworking_Hills2012]. 

In this section, we briefly cover a method for network inference by @Humanmemorysearch_Jun2015 that utilizes partial order information from ordered lists of concepts, called INVITE.
We use it to demonstrate improvement over bag-of-words and markov assumptions for downstream technical language processing [@Technicallanguageprocessing_Brundage2021] tasks, as originally demonstrated in [@UsingSemanticFluency_Sexton2019; and @OrganizingTaggedKnowledge_Sexton2020]

Finally, we show that using _Forest Pursuit_ for partially ordered data can still be quite useful for network backboning, and for a fraction of the computational cost.
We investigate a network recovery task from verbal/semantic fluency data [@Estimatingsemanticnetworks_Zemla2018], which involves recovery of a network of animal relationships from memory and recall experiments. 
Even without directly using partial order information, proper data preparation along with previously-discussed (un-ordered) recovery methods can lead to significantly improved network backboning and analysis capability


## Technical Language Processing with INVITE

Maintenance work orders are often represented as categorized data, though increasingly quantitative use of a technician's descriptions is being pursued by maintenance management operations [@BenchmarkingKeywordExtraction_Sexton2018;@CategorizationErrorsData_Sexton2019].
Tags are another way to flexibly structure this otherwise "unstructured" data, which @tbl-mwo shows in comparison to more traditional categorization.

{{< embed ../codefigs/graphs.qmd#tbl-mwo >}}

Whether entered directly or generated from text by keyword extraction, the tags will tend to have ordering information readily available. 
A traditional way to model this kind of text is through either bag-of-words (the co-occurrence node activation data already discussed) or as a sequence of order-n markov model emissions.
Unlike the clique bias from before, assuming markov jumps for each observation leads to a different kind of bias, with higher precision but reduced recall as shown in @fig-stack-markov. 

{{< embed ../codefigs/graphs.qmd#fig-stack-markov >}}

Without knowing the underlying dependency relationships, it's difficult to estimate which edges were used by a random-walker, since subsequent visits in memory to a "tag" are not being reported once a technician first adds it.
[@Humanmemorysearch_Jun2015] call this an "Initial-visit-emitting" random walk, or INVITE for short. 
To more accurately recover the network, they suggest maximizing the absorption probability for each step of a partial order, individually, knowing which nodes have already been activated. 

### Optimizing absorbing-state probabilities

Say the set of components or concepts that have a corresponding tag in our system is denoted by the node-set $N$.
A user-given set of $T$ [^1] for a specific record can be denoted as a Random Walk (RW) trajectory $\mathbf{t}=\{t_1, t_2, t_3, \cdots t_{T}\}$, where $T\leq N$.
This limit on the size of $T$ assumes tags are a set of unique entries.
Any transitions between previously visited tags in $\mathbf{t}$ will not be directly observed, making the transitions observed in $\mathbf{t}$ strictly non-Markov, and allowing for a *potentially infinite* number of possible paths to arrive at the next tag *through previously visited ones*. 
Instead of directly computing over this intractable model for generating $\mathbf{t}$, the key insight from the original INVITE paper [@Humanmemorysearch_Jun2015] comes from partitioning $\mathbf{t}$ into $T-1$ Markov chains with absorbing states.
Previously visited tags are "transient" states, and unseen tags are "absorbing".
It is then possible to calculate the absorption probability into the $k$ transition ($t_k \rightarrow t_{k+1}$) using the *fundamental matrix* of each partition.
If the partitions at this jump consist of $q$ transient states with transition matrix among themselves $\mathbf{Q}^{(k)}_{q\times q}$, and $r$ absorbing states with transitions into them from $q$ as $\mathbf{R}^{(k)}_{q\times r}$, the Markov transition matrix $\mathbf{M}^{(k)}_{n\times n}$ has the form 
$$
\mathbf{M}^{(k)} =
    \begin{pmatrix}
        \mathbf{Q}^{(k)}  & \mathbf{R}^{(k)} \\
        \mathbf{0}        & \mathbf{I}
    \end{pmatrix}
$${#eq-trans-matrix}

Here $\mathbf{0}$, $\mathbf{I}$ represent lack of transition between/from absorbing states.
It follows from [@RandomWalksElectric_Doyle2000] that the probability $P$ of a chain starting at $t_k$ being absorbed into state $k+1$, is given as
$$
\begin{gathered}
    P\left(t_{k+1} \middle| t_{1:k},\mathbf{M}\right) =
        \left.\mathbf{N}^{(k)}R^{(k)}\right|_{q,1}\\
\mathbf{N} = \left( \mathbf{I}-\mathbf{Q} \right) ^{-1}
\end{gathered}
$${#eq-absorb}

The probability of being absorbed at $k+1$ conditioned on jumps $1:k$ is thus equivalent to the probability of observing the $k+1$ INVITE tag.
If we approximate an a priori distribution of tag probabilities to initialize our chain as $t_1\sim\text{Cat}(n,\theta)$ (which could be empirically derived or simulated), then the likelihood of our observed tag chain $\mathbf{t}$, given a transition matrix, is:
$$
\mathcal{L}\left(\mathbf{t}| \theta, \mathbf{M}\right) =
        \theta(t_1)\prod_{k=1}^{T-1} P\left(t_{k+1}\,\middle|\ t_{1:k},\mathbf{M}\right)
$$

Finally, if we observe a set of tag lists $\mathbf{C} = \left\{ \mathbf{t}_1, \mathbf{t}_2, \cdots, \mathbf{t}_{c} \right\}$, and assume $\theta$ can be estimated independently of $\mathbf{M}$, then we can frame the problem of structure mining on observed INVITE data as a minimization of negative log-likelihood.
A point estimate for our association network given $\mathbf{M}$ can found as:
$$
    \mathbf{M}^* \leftarrow \operatorname*{argmin}_{\mathbf{M}} \quad
    \sum_{i=1}^{C}
    \sum_{k=1}^{T_i-1}
        -\log \mathcal{L} \left(t^{(i)}_{k+1} \middle| t^{(i)}_{1:k},\mathbf{M}\right)
$$

This (deeply nested) likelihood can now be optimized using standard solvers, and our reference implementation uses stochastic gradient descent via PyTorch [@AutomaticdifferentiationPyTorch_Paszke2017].^[
  The n-gram markov transition models (MC1,MC2) trained for comparison vs INVITE were trained using `pomegranate`[@Pomegranatefastflexible_Schreiber2018].
]

[^1]: While some sources use "tagging" as a proxy for a set of strictly
    un-ordered labels (as in multi-label classification), we preserve
    the mechanism by which the tags were generated in the first place,
    i.e., in a *specific* order.

### Application: Mining Excavator MWOs

To assess the applicability of the INVITE-based similarity measure to real-world scenarios, we apply our model to tags annotated for a mining dataset pertaining to 8 similarly-sized excavators at various sites across Australia [@Whyautonomousassets_Hodkiewicz2017; @Cleaninghistoricalmaintenance_Hodkiewicz2016].

The tags were created by a subject-matter expert spending 1 hour of time in the annotation assistance tool `nestor` [@NestorToolNatural_Sexton2019], using a methodology outlined in a previous benchmarking study for that annotation method [@BenchmarkingKeywordExtraction_Sexton2018].

That work compared the ability of tags to estimate survival curves and mean time-to-failure, when compared with a custom-designed keyword extraction tool based on classifying the maintenance issues by subsystem.
While certain sets of tags were able to predict time-to-failure with high accuracy for certain subsystems, a key problem identified in that work is knowing a priori which tags best indicate when a subsystem is failing? 

> _Which tags best represent a given subsystem?_

Some tags are sufficient (albeit unnecessary) conditions to indicate a subsystem.
That the "hydraulic" tag indicates a _Hydraulic System_ MWO is obvious, but so might a "valve" tag---"hydraulic" is implied but not present.
Consequently, we can treat the problem of assigning tags to represent a subsystem as a semi-supervised multi-class classification problem in the style of @Semisupervisedlearning_Avrachenkov2017.
Like in that work, we need to know a selection tag$\rightarrow$subsystem assignments, as well as network of weighted tag-tag edges. 

Then, if we can compare the semi-supervised tag classifications to a ground-truth classification by a human annotator (which are available for the excavator dataset thanks to @sharedreliabilitydatabase_Ho2015), we can assess the ability of each network to capture the human annotator's internal/cognitive tag relationship structure.

> _Which network assigns tags to subsystems most like a domain expert?_

### Results

To test the ability of the similarity measures to accomplish this, the top three most common subsystems in the data were used as classes, namely, Hydraulic System, Engine, and Bucket.
The tags "hydraulic", "engine", and "bucket" were assigned to those subsystems as known labels, respectively.
Tags were filtered to only include ones of high-importance and sufficient information: only work orders containing at least 3 unique tags, and only tags that occurred at least 10 unique times within the those work orders, were included for this analysis ($C=263$ MWOs, $N=40$ tags).
Then the number of occurrences for every tag can be compared across subsystems, giving each tag a ground-truth multinomial (categorical) probability distribution for occurring within each subsystem.
We determine ground-truth classification labels as subsystems that account for $\geq60\%$ of each tag's occurrences.
Tags more balanced than that are considered "unknown subsystem".


To perform semi-supervised classification on the recovered relationship graphs, we use a label-spreading algorithm described in [@Learninglocalglobal_Zhou2004], which itself was inspired by spreading activation networks in experimental psychology [@architecturecognition_Anderson2013 @Observationphasetransitions_Shrager1987]. The result of this algorithm is tags having a score for each class, with the classification being the maximally scored class for that tag. These class assignments can then be compared to the ground-truth labels, which we have done by weighted macro-averaging of the $F_1$-score (see @fig-excavate-f1kl).


:::{#fig-excavate-results}

![](../images/F1_KL_ntags3_freq5_topn50_thres60_saveTrue.svg){#fig-excavate-f1kl}

![](../images/ternary_ntags3_freq5_topn50_thres60_saveTrue.svg){#fig-excavate-ternary}

Semisupervised MWO Tag Classification with Network Label Propagation
:::

The classification of the INVITE-based similarity measure far outperforms the other measures as a preprocessor for label-spreading, when measured by average $F_1$-score.
However, since these "classifications" are actually thresholded multinomial distributions (with some tags regularly occurring across multiple subsystems), how do we know if an underlying structure has actually been recovered, rather than simply a black-box classifier that happens to perform well at this setting?

To begin answering this question, we might ask whether the relative scores returned by label-spreading are similar to the original multinomial distributions themselves, rather than the overall classification.
To find out, we use softmax normalization[^2]
to transform each tag's scores into a "predicted multinomial\", before finally calculating the Kullback-Leibler divergence (KLD) between the true and predicted multinomials for every tag.
The total KLD, summed over all tags, is also shown in @fig-excavate-ternary, along with positions of each tag's multinomial as projected onto the 2-simplex for the true and $F_1$-optimal predicted distributions.
Once again, the INVITE performs much better at this task, over a wide range of $\sigma$ (lower is better).

A reason for the performance disparity can be seen in the simplex projections: recovered topology via INVITE-similarity does a much better job of separating the three classes, while not letting any single tag overcompensate by dominating a subsystem's area.
Even the "unknown" tags are correctly placed roughly between Bucket and Hydraulic System regions, reflecting the true topology of the system.

[^2]: For visualization, a temperature parameter was added to softmax, and this was optimized for minimum KLD via Brent's method [@algorithmguaranteedconvergence_Brent1971] for each similarity measure independently to provide an equal footing for comparison.


## Forest Pursuit Animal Network

For our last case study, we return to Forest Pursuit as a useful tool for analysis even when that might mean ignoring partial ordering information. 
Note that @eq-absorb is effectively the same as a ($\beta=1$) forest matrix if the transition (normalized adjacency) matrix was replaced by a (sub-)graph laplacian.
Intuitively, the INVITE loss function is summing up over absorption (log-)probabilities at each new node activation: i.e. each step of a tree's creation.
Because the probability of a sampled tree is precisely proportional to the product of its edge weights, then weighing a tree by its absorption probabilities and running INVITE should have a mathematically similar effect as the FP estimate.
The similarity would be exact whenever the tree that set of nodes traveled along _was_ the mode of the tree distribution: the MST.

In other words, whenever the random walks _did_ use the minimum distance to reach each node, the two methods should be equivalent.
While this isn't happening much individually, the effect of _many_ random walks will average out to this MST, precisely because _it is the tree distribution mode_ from which we assume node activations sample under RSFm. 

To illustrate FP's efficacy in network estimation despite ignoring partial order information, we turn to a classic network recovery problem in this space: _semantic networks from fluency data_[@newdissimilaritymeasure_Prescott2006;@Estimatingsemanticnetworks_Zemla2018].

### Domain-aware preprocessing
Verbal (semantic) fluency tests involve asking participants to list as many items belonging to a prompted category in their available time.
For instance, an "animals" prompt could lead to an answer like "dog, cat, lion, tiger, bear, wolf...", etc. 
Like before, the general idea is that recall of each item derives from a random walk in a cognitive "memory space", with emissions (usually) only when a new animal is encountered.
A participant might have backtracked internally to _dog_ from bear and jumped to wolf, for example.

Using a high-dimensional multilabel embedding space is possible, with one-column-per-animal, but the lists tend to be quite long and give networks with "hairball" tendencies.
However, if our intention is to find _dependencies_ between animals for a large number of participants, we might reasonably limit the co-occurrence to only a nearby subset of each list.
Effectively, we can limit the set of possible co-occurring animals based on our domain knowledge, namely, the limits of working memory.
Conservatively, with a well-cited work claiming 7 $\pm$ 2 semantic units at a time in working memory[@magicalnumberseven_Miller1956], we can limit the set of possible dependencies for a given item to the 5 items on either side.
The 5 previous might have originated a jump to the current term, and the 5 after might be subsequent targets, for a _rolling window_ of 10 terms at-a-time.^[
  This is precisely the logic that leads to the use of Skip-Gram and Continuous-Bag-of-Words transformations of text to weakly supervise word2vec and GloVe models[@GloveGlobalvectors_Pennington2014;@Efficientestimationword_Mikolov2013]. 
]

For our case study, we use lists of animals submitted by participants as described in [@semanticorganizationanimal_Goni2011;@Estimatingsemanticnetworks_Zemla2018]. We limit the animals under consideration to those that occurred in more than 30 lists for a good sample size, as well as lists with at least two animals.
This resulted in 100 animals over 293 fluency lists. 
However, we ignore this filtering when creating the 10-animal rolling windows, to avoid inclusion of unrelated animals into prematurely filtered windows.
After re-applying the filter, 100 animals appear across 8020 working-memory windows. 
@fig-fluency-workingmemory shows the effects of this preprocessing on marginal distributions.

{{< embed ../codefigs/qualitative.qmd#fig-fluency-workingmemory >}}

Doing this preprocessing (for rolling-windows of 10) shifts relative animal frequencies downward (since there are many more "observations" from the rolling window), while also shifting the number of animals-per-observation to be strictly less than 10.
As desired, the pairwise cosine similarity of all vectors $\mathbf{x}'_{j'},
\mathbf{x}'_{j}$ is significantly reduced.
While many participants might cover similar animals _overall_, we want to investigate animal dependencies locally, and we don't expect individuals to always recall animals in the same memory "area" the whole time. 

### Edge Connective Effiency and Diversity

To compare the results of different backboning techniques, we introduce a new simple measure to quantify a network's sparsity, in terms of how many edges more than $n-1$ (the minimum number needed to connect all $n$ nodes) are being used.
$$
\begin{gathered}
\psi_n(e) = \frac{e_{\text{min}}}{e}\frac{e_{\text{max}}-e}{e_{\text{max}}-e_{\text{min}}}\\
e_{\text{max}} = \frac{n(n-1)}{2} \quad e_{\text{min}} = n-1 
\end{gathered}
$${#eq-connect-efficient}  

We call $\psi$ the graph's "connective efficiency", and it will range from 0 when the graph is fully connected, to 1 when it is a tree, to >1 when it has insufficient edges to be connected.
This measure is intended to compare graphs that have had edges removed until it is about to be disconnected, such as with the _Doubly Stochastic Filter_ (DS) [@twostagealgorithm_Slater2009].
However, values greater than 1 also give insight to how much sparser than a tree some graph is. 

The DS animal network is shown in @fig-fluency-dsmin. 
With large, deeply connected clusters centered around contexts animals are found in @fig-fluency-dsmin looks very similar to the network recovered to make Fig. 4 in [@semanticorganizationanimal_Goni2011].
Clusters approximate animal types by their location relative to humans: farm/livestock, ocean/water creatures, "African" and jungle animals, small indoor mammals, small outdoor mammals, etc. 

{{< embed ../codefigs/qualitative.qmd#fig-fluency-dsmin >}}

This is largely how the literature on semantic fluency leaves their network recovery solution, with clusters based on human proximity or physical location.
However, there are other ways people might relate animals than simply by location.
Additionally, clique bias is quite strong in this network: why must every farm animal be mutually connected if its possible to recall any of them through one or two "hub" animals?
This is related to the incredible inefficiency of this backbone, with a $\psi=0.35$ being rather closer to fully connected than sparse. 
Additionally, the two animals that seem to appear _regardless_ of others are _cat_ and _dog_, which ironically makes DS penalize their proximity to any of the clusters.
Both are ironically clustered with ocean animals due to their tendency to be listed near _fish_.  

The Chow-Liu tree network is shown in @fig-fluency-tree, and goes some way to alleviating these issues.
Clusters are largely intact, instead represented by large branches/subtrees off the main group.
However, some community relationships have been sacrificed to maintain strong individual edges, such as _monkey_+_giraffe_ for location similarity at the expense of separating two halves of the pink cluster across a wide distance.
More alternate paths between creatures (i.e. loops) are needed to better represent our perception of animal relationships.  

{{< embed ../codefigs/qualitative.qmd#fig-fluency-tree >}}

The other dependency network recovery method is GLASSO, which we have similarly thresholded at the minimum-connected point.
It only slightly improves on connective efficiency ($\psi=0.44$), though the cliques are replaced with much more dispersed connections throughout the graph.
We also see that reasonable inter-group connections are better represented, such as _rabbit_+_squirrel_, though _cat_ and _dog_ are still isolated due to overrepresentation throughout the dataset. 

{{< embed ../codefigs/qualitative.qmd#fig-fluency-glassomin >}}
Subjectively, the GLASSO network is still difficult for an analyst to synthesize into useful knowledge, with so many edges, while the DS network only really managed to communicate one "kind" of knowledge (the context clusters). 
We would ideally prefer a backbone that provides a wider diversity of important edge "types", for an analyst to better understand the kinds of animal relationships humans perceive.

To illustrate this, we show the _Forest Pursuit_(FP) network in @fig-fluency-fpmin.
It has also been filtered to minimum-connected, like DS and GLASSO, though in this case the connective efficiency to reach that threshold is a staggering $\psi=0.88$. 
{{< embed ../codefigs/qualitative.qmd#fig-fluency-fpmin >}}

Unlike the other networks that push "generalist" nodes like _cat_ and _dog_ onto long, distant chains, those chains are used in the FP network to hold rare subgroups of clusters, treating them as "gated" by the prominent "hubs" of those groups.
For example, _cat_ is correctly linked to _mouse_ and _lion_ (in addition to _bird_), while _lemur_ is pushed down a longer chain of primates, "gated" by _gorilla_.
Similarly with _eel_ through _lobster_ and _crab_.

A much broader edge-type diversity is also made apprarent with many non-context-based relationships made obvious with the improved sparsity.
An analyst has an easier job of creating "edge-type inventories", making the FP backbone an excellent exploratory assistant: animals can be related because they are: 

- Co-located
- Taxonomically similar (_cheetah_+_leopard_)
- Famous predator/prey pairs (_cat_+_mouse_)
- Pop-culture references (_lion_$\rightarrow$_tiger_$\rightarrow$_bear_)[^oz]
- Similar in ecological niche/role (_koala_+_sloth_)
- Related through conservation or public awareness (_panda_+_gorilla_)
- etc.

[^oz]: Note that the dependency-based methods correctly interpred these three as _not_ being mutually connected in a triad, but specifically with this ordering (_tiger_ in the middle). 

This is further reflected in how FP alters the way _centrality_ measures behave.
Replicating @fig-lesmis-centrality for these graphs, @fig-animal-centrality shows the change in rank across the top 15 animals for the DS, GLASSO, and FP networks. 

{{< embed ../codefigs/qualitative.qmd#fig-animal-centrality >}}

The DS centrality finds the most densely connected clique and gives all of its members incredibly high values.
Meanwhile, the none of the top 5 most common animals (_dog,cat,lion,tiger,bear_)[^dog] have high centrality _at all_.
Both GLASSO and DS have farm animals (_chicken,goat,cow_) as the most important, despite the idea that goat likely could be reached from e.g. _cow_ quite often.
FP adds more variety, giving hub-animals from different communities high centrality scores, each of which could lead to a variety of different paths.
While subjective, the ranks from FP appear to be a more holistic inventory of "lynchpin" animals, which provide nearby coverage for a large amount of others. 

[^dog]: Interestingly, _dog_ never appears in centrality measures, and _none_ of the networks connect dog to any other animal than _cat_.
  Meanwhile, _wolf_ is associated more with _fox_, _coyote_, _dingo_, etc., which are notably all predators of farm animals.  
  With the primary community structures being what they are (context/location-based), it seems that humans tend to put dogs in a category all their own. 

### Thresholded Structure Preservation 

Another beneficial feature of _Forest Pursuit_ is how it fares under increased thresholding.
Because co-occurrence methods prioritize cliques, those cliques will remain connected as other edges are removed, effectively destroying the global structure of the animal network past a certain threshold.
As seen in @fig-fluency-preservation, in keeping only the top 2% of edges, they are used to connect separated islands of animal communities. 

{{< embed ../codefigs/qualitative.qmd#fig-fluency-preservation >}}

Meanwhile, the FP network is quite robust to this excessive thresholding, with the global structure preserved at 2%.
The removed edges have simply detached some of the rarer animals from the network entirely.
These isolates not only reflect a more metrologically-sound idea (that rarer nodes would be disconnected at higher certainty thresholds), but are also beneficial to analysts, since manually re-connecting isolates of rare animals is simpler than manually determining a global reconnection strategy for island groups. 

<!-- {{< embed ../codefigs/results.qmd#fig-fp-compare >}} -->

### Forest Pursuit as Preprocessing {#sec-fp-preprocess}

Because _Forest Pursuit_ creates a representation of the observed data in _edge space_, we can use @eq-sparse-approx-tree in the forward direction, creating a "new"design matrix $X\gets BR$.
As discussed in @sec-fp-problem, @eq-sparse-approx-tree will create a design matrix of interaction counts for each node (its degree in the steiner tree approximation), rather than a binary "on/off" indicator.

By supplying _other algorithms_ with this new estimate for $X$, this makes FP a kind of preprocessing on the data itself.
We can do this to bias the other methods toward greater sparsity in the backbone, without explicitly relying on point estimates for each observation (any tree with those node degrees would do the same).
As shown in @fig-fluency-preprocess, GLASSO and DS both increase their connective efficiency under FP preprocessing.

{{< embed ../codefigs/qualitative.qmd#fig-fluency-preprocess >}}

This is likely due to the increase in "signal-to-noise" ratio for each datapoint, since observations are only similar when they have a node interacting the same "amount", not merely when similar nodes are activated. 

Because the entries are now integer counts, FP preprocessing might also give a better path to using distribution-based embedding and clustering techniques, such as Hellinger distances between multinomial sample counts
This goes for many other techniques from text processing that rely on multinomial assumptions (i.e. techniques otherwise inapplicable to binary data).  
FP Preprocessing, with further empirical and theoretical validation, might prove to be a powerful tool for practitioners to flexibly backbone and analyse their networks with a variety of new techniques.   
