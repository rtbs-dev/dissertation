# Introduction

A wide variety of fields show consistent interest in inferring latent network structure from observed interactions, from human cognition and social infection networks, to marketing, traffic, finance, and many others. [@Inferringnetworksdiffusion_GomezRodriguez2012]
However, an increasing number of authors are noting a lack of agreement in how to approach the metrology of this problem. 
This includes rampant disconnects between the theoretical and methodological network analysis sub-communities[@Statisticalinferencelinks_Peel2022], treatment of error as purely aleatory, rather than epistemic [@Measurementerrornetwork_Wang2012], or simply ignoring measurement error in network reconstruction entirely[@ReconstructingNetworksUnknown_Peixoto2018].

## Ambiguous Metrology
Networks in the "wild" rarely exist of and by themsleves. 
Rather, they are a model of interaction or relation _between_ things that were observed.
One of the most beloved examples of a network, the famed _Zachary's Karate Club_[@InformationFlowModel_Zachary1977], is in fact reported as a list of pairwise interactions: every time a club member interacted with another (outside of the club), Zachary recorded it as two integers (the IDs of the members).
The final list of pairs can be _interpreted_ as an "edge list", which can be modeled with a network: a simple graph.
This was famously used to show natural community structure that nicely matches the group separation that eventually took place when the club split into two.[@Communitystructuresocial_Girvan2002]

Note, however, that we could have just as easily taken note of the instigating student for each interaction (i.e. which student initiated conversation, or invited the other to socialize, etc.).
If that relational asymmetry is available, our "edges" are now _directed_, and we might be able to ask questions about the rates that certain students are asked vs. do the asking, and what that implies about group cohesion.
Additionally, the time span is assumed to be "for the duration of observation" (did the students _ever_ interact), but if observation time was significantly longer, say, multiple years, we might question the credulity of treating a social interaction 2 years ago as equally important to an interaction immediately preceding the split.
This is now a "dynamic" graph; or, if we only measure relative to the time of separation, at the very least a "weighted" one.  

{{< embed codefigs/graphs.qmd#fig-karate-club >}}


_We do not know if any of these are true_.
In fact, as illustrated in [@fig-karate-club],  we do not know if the network being described from the original edge data even has 77 or 78 edges, due to ambiguous reporting in the original work.
Lacking a precise definition of what the graph's components (i.e. it's edges) are, _as measurable entities_, means we cannot estimate the measurement error in the graph.  


## Indirect Network Measurement

While the karate club graph has unquantified edge uncertainty derived from ambiguous edge measurements, we are fortunate that we _have edge measurements_.
Regardless of how the data was collected, it is de facto reported as a list of pairs.
In many cases, we simply do not have such luxury.
Instead, our edges are only measured _indirectly_, and instead we are left with lists of node co-ocurrences.
Networks connecting movies as being "similar" might be derived from data that lists sets of movies watched by each user;
networks of disease spread pathways might be implied from patient infection records;
famously, we might build a network of collaboration strength between academic authors by mining datasets of the papers they co-author together. 

Such networks are derived from what we will call _node activation_ data, i.e., records of what entities happened "together", whether contemporaneously, or in some other context or artifact.

{{< embed codefigs/graphs.qmd#fig-obs-set >}}

These are naturally represented as "bipartite" networks, having separate entites for, say, "papers" and "authors", and connecting them with edges (paper 1 is "connected" to its authors E,H,C, etc.).
But analysts are typically seeking the collaboration network connecting authors (or papers) themselves!
Networks of relationships in this situation are not directly observed, but which _if recovered_ could provide estimates for community structure, importances of individual authors (e.g. as controlling flow of information), and the "distances" that separate authors from each other, in their respective domains. [@Scientificcollaborationnetworks._Newman2001]
Common practice assumes that co-authorship in any paper is sufficient evidence of at least some level of social "acquaintance", where more papers shared means more "connected".  

:::{#fig-toy layout="[1,1]"}
{{< embed codefigs/graphs.qmd#fig-collab  >}}

{{< embed codefigs/graphs.qmd#fig-colleague >}}

Co-authorship vs. collaborator network
:::

Thus our social collaboration network in @fig-collab is borne out of indirect measurements: author connection is implied through "occasions when co-authorship occurred".
However, authors of papers may recall times that others were added, not by their choice, but by someone else already involved.
In fact, the final author list of most papers is reasonably a result of individuals choosing to invite others, not a unanimous, simultaneous decision by all members. 
Let's imagine we wished to study the social network of collaboration more directly: if we had the luxury of being in situ as, say, a sociologist performing an academic ethnography, we might have been more strict with our definition of "connection". 
If the goal is a meaningful social network reflecting the strength of interraction between colleages, perhaps the we prefer our edges  represent "mutual willingness to collaborate".
Edge "measurement", then, could involve records of events that show willingness to seek or participate in collaboration event, such as:

- _author (g) asked (e), (h), and (c) to co-author a paper, all of whom agreed_
- _(i) asked (f) and (j), but (j) wanted to add (b)'s expertise before writing one of the sections_

and so on.
Each time two colleagues had an opportunity to work together _and it was seized upon_ we might conclude that evidence of their relationship strengthed.
With data like this, we could be more confident in claiming our collaboration network can serve as "ground truth," as far as empirically confirmed collaborations go.
However, even if the underlying "activations" are identical, our new, directly measured graph looks very different.  


Fundamentally, the network in [@fig-colleague] shows which relationships the authors _depend_ on to accomplish their publishing activity.
When causal relations between nodes are being modeled as edges, we call such a graph a _dependency network_.
We will investigate this idea further later on, but ultimately, if a network of dependencies is desired (or implied, based on analysis needs), then the critical problem remaining is _how do we recover dependency networks from node activations?_
Additionally, what goes wrong when we use co-occurence/activation data to estimate the dependency network, especially when we wish to use it for metrics like centrality, shortest path distances, and community belonging?


{{< embed codefigs/graphs.qmd#fig-recover  >}}
 

Even more practically, networks created directly from bipartite-style data are notorious for quickly becoming far too dense for useful analysis, earning them the (not-so-)loving moniker "hairballs".
Network "backboning," as it has come to be called tries to find a subset of edges in this hairball that still captures it's core topology in a way that's easier to visualize.[@twostagealgorithm_Slater2009;@backbonebipartiteprojections_Neal2014]
Meanwhile, underlying networks of dependencies that _cause_ node activation patterns can provide this: they are almost always more sparse than their hairballs.
Accessing the dependency _backbone_ in a principled way is difficult, but doing so in a rapid, scalable manner is critical for practitioners to be able to make use of it to trim their hairballs.  

## Scope of this work

The purpose of this thesis is to provide a solid foundation for basic edge metrology when our data consists of binary node activations, by framing network analysis as a problem of _inference_, as suggested by @Statisticalinferencelinks_Peel2022.
We give special focus to binary activations that occur due to spreading processes, such as random walks or cascades on an underlying carrier graph.
Recovering the carrier, or, "dependency" network from node activations is of great interest to the network backboning and causal modeling communities, but often involves either unspoken sources of epistemic and aleatory error, or  high computation costs (or both). 
To begin addressing these issues, we present a guide to current practices, pitfalls, and how common statistical tools apply to the network recovery problem: a _Practitioner's Guide to Network Recovery_.
We cover what "measurement" means in our context, and specifically the ways we encode observations, operations, and uncertainties numerically.
Clarifying what different versions of what "relation" means (whether proximity or incidence) is critical, since network structure is intended to encode such relations as mathematical objects, despite common ambiguities and confusion around what practitioners intend on communicating through them.
Then we use this structure to present a cohesive framework for selecting a useful network recovery technique, based on the available data and where in the data processing pipeline is acceptable to admit either extra modeling assumptions or information loss.

Next, building on a gap found in the first part, we present a novel method, _Forest Pursuit_, to extract dependency networks when we know a _spreading process_ causes node activation (e.g. paper co-authorship caused by collaboration requests).
We create a new reference dataset to enable community benchmarking of network recovery techniques, and use it show greatly improved accuracy over many other widely-used methods.
Forest Pursuit in its simplest form scales linearly with the size of active-node sets, being trivially parallelizable and streamable over dataset size, and agnostic to network size overall.
We then expand our analysis to re-imagine Forest Pursuit as a Bayesian probabilistic model, _Latent Forest Allocation_, which has an easily-implemented Expectation Maximization scheme for posterior estimation.
This significantly improves upon the accuracy results of Forest Pursuit, at the cost of some speed and scalability, giving analysts multiple options to adapt to their needs.

Last, we apply Forest Pursuit to several qualitative case-studies, including a scientific collaboration network, and the verbal fluency "animals" network recovery problem, which dramatically change interpretation under use of our method.
We investigate its use as a low-cost preprocessor for other methods of network recovery,like GLASSO, improving their stability and interpretability. 
Finally we discuss the special case when node activations are reported as an ordered set, where accounting for cascade-like effects becomes crucial to balance false positive and false-negative edge prediction.
Along with application of this idea to knowledge-graph creation from technical language in the form maintenance work-order data, we discuss more broadly the future needs of network recovery, specifically in the context of embeddings and gradient-based machine learning toolkits.




