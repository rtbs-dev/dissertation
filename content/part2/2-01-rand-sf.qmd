# Graph Reduce & Desire Paths

Addressing gaps discussed in the previous section to reach a generative model for network recovery requires careful attention to the generation mechanism for node activations.
While there are many ways we might imagine bipartite data to be be generated, presuming the existence of a dependency graph that _causes_ activation patterns will give us useful ways to narrow down the generative specification.

First, we will investigate the common assumption that pairwise co-occurrences can serve as proxies for measuring relatedness, and how this "gambit of the group" is, in fact, a strong bias toward dense, clique-filled recovered networks.
Because we wish to model our node activations as being _caused_ by other nodes (that they depend on), we draw a connection to a class of models for _spreading_, or, _diffusive processes_.
We outline how random-walks are related to these diffusive  models of graph traversal, enabled by an investigation of the graph's "regularized laplacian" from @Semisupervisedlearning_Avrachenkov2017.
Then we use the implicit causal dependency tree structure of each observation, together with the Matrix Forest Theorem [@MatrixForestTheorem_Chebotarev2006;@Countingrootedforests_Knill2013] to more generally define our generative node activation model.
This leads to a generative model for binary activation data as rooted random spanning forests on the underlying dependency graph.

## The Gambit of the Inner Product 

As we saw repeatedly in @sec-lit-review, networks are regularly assumed to arise from co-occurrences, whether directly as counts or weighted in some way.
This assumption can be a significant a source of bias in the measurement of edges.
_Why_ a flat count of co-occurrence leads to "hairballs" and bias for dense clusters can be related to the use of inner products on node activation vectors. 


### Gambit of the Group

It seems reasonable, when interactions are unobserved, to assume some evidence for all possible interactions is implied by co-occurrence.
However, similar to the use of uniform priors in other types of inference, if we don't have a good reason to employ a fully-connected co-occurrence prior on interaction dependencies, we are adding systematic bias to our inference.
Whether co-occurrence observations can be used to infer interaction networks directly was discussed at length in @Techniquesanalyzingvertebrate_Whitehead1999, where Whitehead and Dufault call this the _gambit of the group_.

> So, consiously or unconsciously, many ethnologists studying social organization makr what might be called the "gambit of the group": the assume that animals which are clustered [...] are interacting with one another and then use membership of the same cluster [...] to define association.

This work was rediscovered in the context of measuring assortativity for social networks,^[Assortativity is, roughly, the correlation between node degree and the degrees of its neighbors.] where the author of @PerceivedAssortativitySocial_Fisher2017 advises that "group-based methods" can introduce sampling bias to the calculation of assortativity, namely, systematic overestimation when the sample count is low. 
 
The general problems with failing to specify a model of what "edges" actually _are_ get analyzed in more depth in @Statisticalinferencelinks_Peel2022.
They include a warning not to naively use correlational measures with a threshold, since even simple 3-node systems will easily yield false positives edges.
Still, it would be helpful for practitioners to have a more explicit mental model of _why_ co-occurence-based models yield systematic bias,  


### Inner-Product Projections and "Clique Bias"

Underlying correlation and co-occurrence models for edge strength is a reliance on inner products between  node occurrence vectors.
They all use gram matrices (or centered/scaled versions of them), which were brought up in @sec-products.
The matrix multiplication performed represents inner products between all pairs of feature vectors.
For $X(i,j)\in\mathbb{B}$, these inner products sum together the times in each observation that two nodes were activated together.


{{< embed ../codefigs/graphs.qmd#fig-stack-outerprod >}}

However, another (equivalent) way to view matrix multiplication is as a sum of outer products
$$
G(j,j') = X^T X = \sum_{i=1}^m X(i,j)X(i,j')= \sum_{i=1}^m \mathbf{x}_i\mathbf{x}_i^T
$$
Those outer products of binary vectors create $m\times m$ matrices that have a 1 in every $i,j$ entry where nodes $i,j$ both occurred, shown in @fig-stack-outerprod.
Each outer product is effectively operating as a $D_i+A_i$ with degrees normalized to 1.
If the off-diagonals can be seen as adjacency matrices, they would strictly represent a clique on nodes activated in the $i$th observation
In this sense, any method that involves transforming or re-weighting a gram matrix, is implicitly believing that the $k$th observation was a _complete graph_.
This is illustrated in @fig-stacked-graphs. 

:::{#fig-stacked-graphs layout="[[1,2]]"  layout-valign="center"}

{{< embed ../codefigs/graphs.qmd#fig-obs-set >}}
{{< embed ../codefigs/graphs.qmd#fig-stack-bow >}}

Inner-product projections as sums of cliques illustrating "clique bias". 
:::

For many modeling scenarios, this paradigm allows practitioners to make a more straight-forward intuition-check: do clique observations _make sense_ here?
When a list of authors for a paper is finished, does that imply that all authors mutually interacted with all others directly to equally arrive at the decision to publish?
This would be similar to assuming the authors might simultaneously enter the same room, look at a number of others (who all look exclusively at each other, as well), and _all at once_ decide to publish together.
In our introduction, we described a more likely scenario we could expect from an observer on the ground: a researcher asks a colleague or two to collaborate, who might know a couple more with relevant expertise, and so on. 


## Networks as Desire Path Density Estimates

Unfortunately, methods based on inner-product thresholding are still incredibly common, in no small part due to how _easy_ it is to create them from occurrence data, regardless of this "clique-bias".
The ability to map an operation onto every observation, e.g. in parallel, and then reduce all the observations into an aggregate edge estimate is a highly desirable algorithmic trait.
This may be a reason so many projection and backboning techniques attempt two re-weight (but retain) the same basic structure, time and again. 

What we need is a way to maintain the ease-of-use of inner-product network creation:

- map an operation onto each observation
- reduce to an aggregate edge guess over all observations

but with a more domain-appropriate operator at the observation level. 

Let's start with replacements for the clique assumption.
There are many non-clique classes of graphs we might believe local interactions occur on: path-graphs, trees, or any number of graphs that reflect the topolgy or mechanism of local interactions in our domain of interest.
 Authors have proposed classes of graphs that mirror human perception of set shapes [RELATIVE NEIGHBORHOOD]^[
  e.g. for dependencies based on perception, such as human decision making tendencies, or causes based on color names.
], or graphs whose modeled dependencies are strictly planar [planar maximally filtered graps]^[
  e.g. when interactions are limited to planar dependencies, like inferring ancient geographic borders.
].
Alternatively, the interactions might be scale free, small-world, trees, or samples from stochastic block models.[CITE] 
In any case, these assumptions provide an explicit way to describe the set of _possible_ interaction graphs we believe our individual observations are sampled from.

### Subgraph Distributions

Let's use the notation from @eq-edge-vectors, such that each observation of nodes $\mathbf{x}_i$ is implicitly derived from a set of activated edges $\mathbf{r_i}$.
To start, our current belief about what overall structure the whole network can take is $G^*=(V,E,B^*)$, where $B^*$ might always return $1$ to start out (the complete graph).
From this, we construct a $B_i$ for each observation. 
Each $\mathbf{x}_i$ will induce a subgraph $g_i = G^*[V_i]$, where $V_i = \{v\in \mathcal{V} | X(i,\mathcal{V})=1\}$, while our domain knowledge takes the form of a constraint on edges, which will induce a family of subgraphs again.
The "family of subgraphs," in general, can't directly use the edge constraint to create $B_i(e,v)=\mathbf{1}_{V_i}(v)\mathbf{1}_{E_i}(e)$, because edges $E_i$ are not directly observed.
Instead, we model them as inducing a family  $\mathcal{C}_i$ belonging to the relevant class of graphs $\mathcal{C}$ on $V$, i.e.

$$
\begin{gathered}
\mathcal{C}_i = \{(V,E,B_i) \in\mathcal{C}|B_i(e,v)=B^*(e,v)\mathbf{1}_{V_i}(v)\mathbf{1}_{E_i}(e)\}\\
E_i\in\{\mathcal{E}\in\mathcal{P}(E)| g_i[\mathcal{E}]\in\mathcal{C}\}
V_i = \{v\in \mathcal{V} | X(i,\mathcal{V})=1\}
\end{gathered}
$${#eq-subgraph-family}^[$\mathcal{P}(A)$ is the _powerset_ of $A$, i.e. the set of all subsets of $A$. ]

In other words, the edges that "caused" to the node activations in a given observation must _together_ belong to a graph that, in turn, belongs to our desired class.

Assuming we can define an associated measure for each $\mu_i(c)$ on $\mathcal{C}_i$, we are able to define a probability distribution over subgraphs in the class.^[
 This is certainly not a trivial assumption, and might either be ill-defined or require techniques like the Gumbel trick[@GradientEstimationStochastic_Paulus2020] to approximate, unless the partition function $\mathbb{Z}$ has a closed form, or $\mu$ is already a probability measure on some $\sigma$-algebra over $\mathcal{C}$.
 Closed-form $\mathcal{Z}$ values on $\mathcal{C}$ are known for a handful of graph classes, such as the space of spanning trees, $\mathcal{C}=\mathcal{T}$.
 However, another way this might be accomplished is through random geometric graphs[CITE], or geometric spanners like Urqhart[CITE] and Relative Neighborhood [CITE] graphs on a "sprinkling" of points that preserves their observed pairwise distances.   
 This is further discussed in @sec-future-hyperbolic.
]
Using notation similar to distributions over spanning trees in @EfficientComputationExpectations_Zmigrod2021: 

$$
\begin{gathered}
p_i(c) = \frac{\mu_i(c)}{\mathbb{Z}_i}\\
\mathbb{Z}_i = \sum_{c\in\mathcal{C}_i} \mu_i(c)
\end{gathered}
$$ {#eq-subgraph-prob}


To further constrain the problem, let us assume that node activation is noiseless: any activated nodes were truly activated, and unactivated nodes were truly inactive (no false negative or false positive activations).^[
 Hidden nodes (unobserved nodes beyond the $n$) are outside the scope of this work, though could potentially be implied for certain structures e.g. when the metric is known to be tree-like.
 @TreeIam_Sonthalia2020 use a greedy embedding that minimizes distortion to estimate the need for added _Steiner_ nodes. 
]
If a set of nodes $V_i\subseteq V$ were observed in the $i$th observation, this assumption narrows the distribution to only the subgraphs $c\in\mathcal{C}^{V_i}$.
This can be represented using the vectorization in @eq-edge-vectors, due to the one-to-one correspondence established, so that 
$$\mathbf{r}_i|\mathbf{x}_i \sim p_i(c) $${#eq-edgevec-prob}

So we may not have an exact vector, but we have established a way to specify a family of edge vectors that could be responsible.
With $p_i(c)$, we also have a mechanism to sample them when a partition function is available (or able to be approximated).

With these mechanics in place, we see the choice of a clique (implied by the inner product) is a trivial application of @eq-edgevec-prob, since that is equivalent to selecting the class of cliques on $V_i$ nodes.
This has only one element ($\|\mathcal{C}_{\text{clique}}\|=1$), there is only 1 possible selection, with probability $p_i(K^{V_i})=1$.  

[ENTROPY?]

<!-- Once an analyst has provided epistemic justification for a _class of graphs_ to model --> 
<!-- We propose that the computationally-efficient inner-product networks can still be used, but could be made far more effective by counting edge observation counts with something more appropriate than cliques. --> 



### Graph Unions as Desire Paths

With a distribution over subgraphs each observation, we are potentially able to sambple from them for bootstrap or Monte Carlo estimation purposes, or simply find a maximum likelihood estimate for each distribution.
Assuming this is true, we may now sample or approximate a matrix $R(i,e):I\times E \rightarrow \mathbb{B}$.  
Methods for doing this efficiently in certain cases are the focus of @sec-FP and @sec-LFA-gibbs.
However, once $R(i,e)$ is estimated, a reasonable mechanism for estimating the support of the set of edges is to use $\frac{\text{count}}{\text{exposure}}$, but with a few needed modifications.

First, while the nodes counts in $\sum_i B(i,\cdot)$ are by assumption _not_ independent, or even pairwise independent, the _edge traversal_ counts $\sum_i R(i,\cdot)$ could more reasonably be considered so.
A model certainly could be constructed where edge existence depends on other edges existing (or not).
But nothing is inherently self-inconsistent with a model that assumes the traversability of individual edges will be independent of one another. 

let P(e) be the probability that an edge is traversed (in any observation), and P(u,v) the probability that nodes $u,v$ co-occur.
To approximate the overall traversability of an edge, we can calculate an empirical estimate for the conditional probability $P(e|(u,v))=\frac{P(e)\cap P(u,v)}{P(u,v)}$ that an edge is traversed, given that the two incident nodes are activated.
This estimate can use the same beta-bernoulli distribution from @eq-beta-binomial.

How do we ensure our estimate is approximating traversability, so that the probability that an edge probability converges toward 1 as long as it _has to be possible_ for $e$ to be traversed?
Recall from the introduction that, from a measurement perspective, the underlying networks rarely "exist" in the sense that we never truly find the underlying network, but only samples from (or caused by) it.
Imagine that the "real" network is a set of paved sidewalks: our procedure is similar to watching people walk along paths between locations, and wanting to estimate which of the paths would be tread "enough" to be paved. 
This is where we build on an intuition based on the popular idea of _desire paths_ which is a colloquial name for paths that form when individuals walk over grass or dirt enough to carve a trail.
In network analysis and recovery, we usually are only allowed to see the desire paths that might have formed from our data, never the actual "pavement".

If presented with two equivalent paths, an individual is likely to choose the one that tread more often i.e. that looks the most clearly defined.
So, we don't want a probability that an edge has been traversed, but a probability over fractions of the time we expect an edge to have been traversed _more often than not_.
This is accomplished by forcing $\alpha, \beta\lt 1$.
For the case where $\alpha=\beta=0.5$, we call this special case an _arcsine_ distribution.
In a coin tossing game where each "heads" gives a point to player A and "tails" to player B, then the point differential is modeled as a random walk.
The arcsine distribution $\text{Beta}(\tfrac{1}{2},\tfrac{1}{2})$ is exactly the probability distribution for the fraction of time we expect one player to be "ahead" of the other. [@WhatisArcsine_Ackelsberg2018]  

> "_Contrary to popular opinion, it is quite likely that in a long coin-tossing game one of the players remains practically the whole time on the winning side, the other on the losing side."_
> 
> William Feller[@IntroductionProbabilityTheory_Feller1968, Chapter III]

This is desirable behavior for a distribution over edge traversability!
We expect the vast majority of edges to have a 0 or 1 as the most likely values, with the expected fraction of observations that an edge being traversed was "ahead" of it being _not_ traversed matching our empirical count.

We generalize this with $\alpha = 1-\beta$, with $\alpha + \beta = 1$, such that the new beta-binomial posterior from @eq-beta-binomial with $s$ successes over $k$ trials is:

$$
\begin{gathered}
p_{DP} \sim \text{Beta}(\alpha + c, 1-\alpha-c) \\
c = \frac{s-ak}{k+1}\\
E[p_{DP}|s,k] = \alpha + c
\end{gathered}
$${#eq-desirepath-binom} 

Important to note is that $k$ is measured over the _co-occurrences_ $(u,v)$, and successes are the traversals $e$ derived from our samples in $R$.


