# Forest Pursuit: Approximate Recovery in Near-linear Time 

In this chapter, we build on the notion of "Desire Path" estimation of a dependency network from node activations --- sampling from a class of subgraphs constrained to active nodes, then merging them.
We present _Forest Pursuit_ (FP), a method that is scalable, trivially parallelizes, and offline-capable, while also outperforming commonly used algorithms across a battery of standardized tests and metrics. 
The key application for using FP is in domains where node activation can be reasonably modeled as arising due to _random walks_---or similar spreading process---on an underlying dependency graph. 

First, we build an intuition for the use of trees as an unbiased estimator for desire path estimation when spreading processes are at work on the latent network.
Then, the groundwork for FP is laid by combining sparse approximation through _matching pursuit_ with a loss function modeled after the Chow Liu representation for joint probability distributions.
The approximate complexity for FP is linear in the spreading rate of the modeled random walks, and linear in dataset size, while running in $O(1)$ time with respect to the network size.
This departs dramatically from other methods in the space, all of which assume to scale in the number of nodes. 
We then test FP against an array of alternative methods (including GLASSO) with MENDR, a newly-developed standard reference dataset and testbench for network recovery.
FP outperforms other tested algorithms in nearly every case, and empirically confirms its complexity scaling for sub-thousand network sizes. 

## Random Walks as Spanning Forests 

The class of diffusive processes we focus on "spread" from one node to another.
If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes "depend" on others to be activated.
In this case, a node activates when another node it depends on spreads their state to it.
These single-cause activations are often modeled as a random-walk on the dependency graph: visiting a node leads to its activation.

### Random Walk Activations 
Random walks are regularly employed to model spreading and diffusive processes on networks.
If a network consists of locations, states, agents, etc. as "nodes", and relationships between nodes as "edges", then random walks consist of a stochastic process that "visits" nodes by randomly "walking" between them along connecting edges.
Epidemiological models, cognitive search in semantic networks, stock price influences, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. [CITE...lots?]

When network structure is known, the dynamics of random-walks are used to capture the network structure via sampling [LITTLEBALLOFFUR, etc], estimate node importance's[PAGERANK], or predict phase-changes in node states (e.g. infected vs. uninfected)[SIR I think].
In our case, Since we have been encoding the activations as binary activation vectors, the "jump" information is lost---activations are "emitted" for observation only upon the random walker's initial visit.[@Humanmemorysearch_Jun2015]
Further, the ordering of emissions has been removed in our binary vector representation, leaving only co-occurrence groups in each $\mathbf{x}_i$.^[For a brief treatment of the case that INVITE emission order is preserved, see @sec-ordered]
In many cases, however, the existence of relationships is not known already, and analysts might *assume* their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.

### Finite Activations & Spanning Forest

As a general setting, the number of node activations (e.g. for datasets like co-authorship) is much smaller than the set of nodes ($\|\mathbf{x}_i\in\mathbb{S}^n\|_0 \ll n$)^[
  $\|\cdot\|_0$ is the $\ell_0$ "pseudonorm", counting non-zero elements (the support) of its argument.
]  
@Semisupervisedlearning_Avrachenkov2017 go to some length describing discrete- and continuous-time random walk models that can give rise to binary activation vectors like our $X(i,j):I\times J\rightarrow \mathbb{B}$.
The _regularized laplacian_ (or _forest_)kernel of a graph[@SimilaritiesgraphsKernels_Avrachenkov2019] plays a central role in their analysis, as it will in our discussion going forward.

$$
Q_{\beta} = (I+\beta L)^{-1}
$${#eq-regulap}

In that work, it is discussed as the optimal solution to the semi-supervised "node labeling" problem, having a regularization parameter $\beta$, though its uses go far beyond this.[@GraphLaplacianRegularization_Pang2017;@Countingrootedforests_Knill2013;@MatrixForestTheorem_Chebotarev2006]
$Q$ generalizes the so-called "heat kernel" $\exp{(-t\tilde{L})}$, in the sense that it solves a lagrangian relaxation of a loss function based on the heat equation.
This can be related to the PageRank ($\exp{(-tL^{\text{rw}})}$) kernel as well, which is explicitly based on random walk transition probabilities.

In fact, $Q$ can be viewed as a transition matrix for a random walk having a geometrically distributed number of steps, giving us a small expected support for $\mathbf{x}_i$, as needed.^[$Q$ can also be interpreted as a continuous-time random walk location probability, after exponentially distributed time, if spending exponentially-distributed time in each node.]
However we interpret $Q$, a remarkable fact emerges due to a theorem by Chebotarev: each entry $q=Q(u,v)$ is equal to the probability^[edge weights scaled by \beta] that nodes $u,v$ are connected in a randomly sampled _spanning rooted forest_

In other words, co-occurring node activations due to a random walk or heat kernel are deeply tied to the chance that those nodes find themselves _on the same tree in a forest_.

### Spreading Dependencies as Trees
The whole graph isn't a tree....Every data point is.


{{< embed ../codefigs/graphs.qmd#fig-stack-tree >}}




## Sparse Approximation

### Problem Specification

### Matching Pursuit

### Space of Spanning Forests

## Forest Pursuit: Approximate Recovery in Near-linear Time {#sec-FP}

I.e. the PLOS paper (modified basis-pursuit via MSTs)
### Algorithm Summary

### Uncertainty Estimation 

### Approximate Complexity

## Simulation Study {#sec-FP-experiments}

### Method

### Results - Scoring

{{< embed ../codefigs/results.qmd#fig-fp-overall >}}

{{< embed ../codefigs/results.qmd#fig-fp-compare >}}

{{< embed ../codefigs/results.qmd#fig-partials-mcc >}}

### Results - Performance

{{< embed ../codefigs/results.qmd#fig-runtime >}}

{{< embed ../codefigs/results.qmd#fig-partials-runtime >}}
## Discussion


### Interaction Probability

![](../images/PR.svg)
