# Forest Pursuit: Approximate Recovery in Near-linear Time 

In this chapter, we build on the notion of "Desire Path" estimation of a dependency network from node activations --- sampling from a class of subgraphs constrained to active nodes, then merging them.
We present _Forest Pursuit_ (FP), a method that is scalable, trivially parallelizes, and offline-capable, while also outperforming commonly used algorithms across a battery of standardized tests and metrics. 
The key application for using FP is in domains where node activation can be reasonably modeled as arising due to _random walks_---or similar spreading process---on an underlying dependency graph. 

First, we build an intuition for the use of trees as an unbiased estimator for desire path estimation when spreading processes are at work on the latent network.
Then, the groundwork for FP is laid by combining sparse approximation through _matching pursuit_ with a loss function modeled after the Chow Liu representation for joint probability distributions.
The approximate complexity for FP is linear in the spreading rate of the modeled random walks, and linear in dataset size, while running in $O(1)$ time with respect to the network size.
This departs dramatically from other methods in the space, all of which assume to scale in the number of nodes. 
We then test FP against an array of alternative methods (including GLASSO) with MENDR, a newly-developed standard reference dataset and testbench for network recovery.
FP outperforms other tested algorithms in nearly every case, and empirically confirms its complexity scaling for sub-thousand network sizes. 

## Random Walks as Spanning Forests 

The class of diffusive processes we focus on "spread" from one node to another.
If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes "depend" on others to be activated.
In this case, a node activates when another node it depends on spreads their state to it.
These single-cause activations are often modeled as a random-walk on the dependency graph: visiting a node leads to its activation.

### Random Walk Activations 
Random walks are regularly employed to model spreading and diffusive processes on networks.
If a network consists of locations, states, agents, etc. as "nodes", and relationships between nodes as "edges", then random walks consist of a stochastic process that "visits" nodes by randomly "walking" between them along connecting edges.
Epidemiological models, cognitive search in semantic networks, stock price influences, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. [CITE...lots?]

When network structure is known, the dynamics of random-walks are used to capture the network structure via sampling [LITTLEBALLOFFUR, etc], estimate node importance's[PAGERANK], or predict phase-changes in node states (e.g. infected vs. uninfected)[SIR I think].
In our case, Since we have been encoding the activations as binary activation vectors, the "jump" information is lost---activations are "emitted" for observation only upon the random walker's initial visit.[@Humanmemorysearch_Jun2015]
Further, the ordering of emissions has been removed in our binary vector representation, leaving only co-occurrence groups in each $\mathbf{x}_i$.^[For a brief treatment of the case that INVITE emission order is preserved, see @sec-ordered]
In many cases, however, the existence of relationships is not known already, and analysts might *assume* their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.

### Activations in a Forest

As a general setting, the number of node activations (e.g. for datasets like co-authorship) is much smaller than the set of nodes ($\|\mathbf{x}_i\in\mathbb{S}^n\|_0 \ll n$)^[
  $\|\cdot\|_0$ is the $\ell_0$ "pseudonorm", counting non-zero elements (the support) of its argument.
]  
@Semisupervisedlearning_Avrachenkov2017 go to some length describing discrete- and continuous-time random walk models that can give rise to binary activation vectors like our $X(i,j):I\times J\rightarrow \mathbb{B}$.
The _regularized laplacian_ (or _forest_)kernel of a graph[@SimilaritiesgraphsKernels_Avrachenkov2019] plays a central role in their analysis, as it will in our discussion going forward.

$$
Q_{\beta} = (I+\beta L)^{-1}
$${#eq-regulap}

In that work, it is discussed as the optimal solution to the semi-supervised "node labeling" problem, having a regularization parameter $\beta$, though its uses go far beyond this.[@GraphLaplacianRegularization_Pang2017;@Countingrootedforests_Knill2013;@MatrixForestTheorem_Chebotarev2006]
$Q$ generalizes the so-called "heat kernel" $\exp{(-t\tilde{L})}$, in the sense that it solves a lagrangian relaxation of a loss function based on the heat equation.
This can be related to the PageRank ($\exp{(-tL^{\text{rw}})}$) kernel as well, which is explicitly based on random walk transition probabilities.

In fact, $Q$ can be viewed as a transition matrix for a random walk having a geometrically distributed number of steps, giving us a small expected support for $\mathbf{x}_i$, as needed.^[$Q$ can also be interpreted as a continuous-time random walk location probability, after exponentially distributed time, if spending exponentially-distributed time in each node.]
However we interpret $Q$, a remarkable fact emerges due to a theorem by Chebotarev: each entry $q=Q(u,v)$ is equal to the probability^[edge weights scaled by \beta] that nodes $u,v$ are connected in a randomly sampled _spanning rooted forest_

In other words, co-occurring node activations due to a random walk or heat kernel are deeply tied to the chance that those nodes find themselves _on the same tree in a forest_.

### Spreading Dependencies as Trees

{{< embed ../codefigs/graphs.qmd#fig-stack-tree >}}

With the overt link from spreading processes to counts of trees made, there's room for a more intuitive bridge.

For single-cause, single source spreading process activations---on a graph---the activation dependency graph for each observation/spread/random walk _must be a tree_.
With a single cause (the "root"), which is the starting position of a random walker, a node can only be reached (activated) by another currently activated node.
If the random walk jumps from one visited node to another, previously visited one, that transition did not result in an activation , so the _dependency_ count for that edge should not increase.
This description of a random walk, where subsequent visits do not "store" the incoming transition, is roughly equivalent to one more commonly described as a _Loop-Erased_ random walk.
It is precisely used to uniformly sample the space of spanning trees on a graph.[@Generatingrandomspanning_Wilson1996]

Much like a reluctant co-author "worn down" by multiple requests, we can even include random walks that "receive" activation potential from more than one source.
Say a node is activated when some fraction of its neighbors have all originated a random walk transition to it, or a node activates on its second visit, or similar. 
We simply count (as dependency evidence) the ultimate transition that precipitated activation.
This could be justified from an empirical perspective as well: say we observe an author turn down requests for one paper from two individuals, but accept a third.
We could actually infer a _lowered_ dependency on the first two, _despite_ the eventual coauthorship.
Only the interaction that was observed as successful necessarily counts toward  success-dependency, barring any contradicting information.

It's important to add here that _mutual convincing_ by multiple collaborators simultaneously (or over time) is expressly left out.
In other words, only pairwise interactions are permitted.
This is not an additional assumption, but a key limitation of our use of graphs in the first place!
As Torres et al. go to great lengths elaborating in [@WhyHowWhen_Torres2021], it is critical to correctly model dependencies when selecting a structural representation of our problem to avoid data loss.
The possibility for multi-way interactions would necessitate the use of either a simplicial complex or a hypergraph as the carrier structure, _not a graph_. 

@fig-stack-tree demonstrates the use of trees as the distribution for subgraphs, instead of outer-products/cliques.  



## Sparse Approximation



### Problem Specification
- min 0 norm [@EfficientimplementationK_Rubinstein2008]


-  Matching Pursuit

### Max. Spanning (Steiner) Trees

- aggregate/marginalize over roots in a rooted tree (efficient expectations)...also MI is symmetric (same tree for any root)
- total tree: overcounting of independence is at most the spanning tree (hunter worsely)
- generally max spanning tree is the mode for distribution, so there's our point estimate.

MST for each node set

- KL-divergence is convex (we can minimize sum of dists, not just dist of sum)
- steiner tree and approx with closure...which we don't have 
- oh look! counting trees is similar. use cosine because geometric pooling

## Forest Pursuit: Approximate Recovery in Near-linear Time {#sec-FP}

I.e. the PLOS paper (modified basis-pursuit via MSTs)

### Algorithm Summary

code box

example:

```pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\Procedure{Quicksort}{$A, p, r$}
  \If{$p < r$}
    \State $q = $ \Call{Partition}{$A, p, r$}
    \State \Call{Quicksort}{$A, p, q - 1$}
    \State \Call{Quicksort}{$A, q + 1, r$}
  \EndIf
\EndProcedure
\Procedure{Partition}{$A, p, r$}
  \State $x = A[r]$
  \State $i = p - 1$
  \For{$j = p$ \To $r - 1$}
    \If{$A[j] < x$}
      \State $i = i + 1$
      \State exchange
      $A[i]$ with     $A[j]$
    \EndIf
    \State exchange $A[i]$ with $A[r]$
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
```
### Approximate Complexity


## Simulation Study {#sec-FP-experiments}

MENDR + affinis

### Method

what kinds, distributions, etc. 
- glasso gets 5CV to select reg param

### Results - Scoring

{{< embed ../codefigs/results.qmd#fig-fp-overall >}}

{{< embed ../codefigs/results.qmd#fig-fp-compare >}}

{{< embed ../codefigs/results.qmd#fig-partials-mcc >}}

### Results - Performance

{{< embed ../codefigs/results.qmd#fig-runtime >}}

{{< embed ../codefigs/results.qmd#fig-partials-runtime >}}
## Discussion


### Interaction Probability

![](../images/PR.svg)
