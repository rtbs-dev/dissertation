# Forest Pursuit: Approximate Recovery in Near-linear Time 

In this chapter, we build on the notion of "Desire Path" estimation of a dependency network from node activations --- sampling from a class of subgraphs constrained to active nodes, then merging them.
We present _Forest Pursuit_ (FP), a method that is scalable, trivially parallelizes, and offline-capable, while also outperforming commonly used algorithms across a battery of standardized tests and metrics. 
The key application for using FP is in domains where node activation can be reasonably modeled as arising due to _random walks_---or similar spreading process---on an underlying dependency graph. 

First, we build an intuition for the use of trees as an unbiased estimator for desire path estimation when spreading processes are at work on the latent network.
Then, the groundwork for FP is laid by combining sparse approximation through _matching pursuit_ with a loss function modeled after the Chow Liu representation for joint probability distributions.
The approximate complexity for FP is linear in the spreading rate of the modeled random walks, and linear in dataset size, while running in $O(1)$ time with respect to the network size.
This departs dramatically from other methods in the space, all of which assume to scale in the number of nodes. 
We then test FP against an array of alternative methods (including GLASSO) with MENDR, a newly-developed standard reference dataset and testbench for network recovery.
FP outperforms other tested algorithms in nearly every case, and empirically confirms its complexity scaling for sub-thousand network sizes. 

## Random Walks as Spanning Forests 

The class of diffusive processes we focus on "spread" from one node to another.
If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes "depend" on others to be activated.
In this case, a node activates when another node it depends on spreads their state to it.
These single-cause activations are often modeled as a random-walk on the dependency graph: visiting a node leads to its activation.

### Random Walk Activations 
Random walks are regularly employed to model spreading and diffusive processes on networks.
If a network consists of locations, states, agents, etc. as "nodes", and relationships between nodes as "edges", then random walks consist of a stochastic process that "visits" nodes by randomly "walking" between them along connecting edges.
Epidemiological models, cognitive search in semantic networks, stock price influences, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. [CITE...lots?]

When network structure is known, the dynamics of random-walks are used to capture the network structure via sampling [LITTLEBALLOFFUR, etc], estimate node importance's[PAGERANK], or predict phase-changes in node states (e.g. infected vs. uninfected)[SIR I think].
In our case, Since we have been encoding the activations as binary activation vectors, the "jump" information is lost---activations are "emitted" for observation only upon the random walker's initial visit.[@Humanmemorysearch_Jun2015]
Further, the ordering of emissions has been removed in our binary vector representation, leaving only co-occurrence groups in each $\mathbf{x}_i$.^[For a brief treatment of the case that INVITE emission order is preserved, see @sec-ordered]
In many cases, however, the existence of relationships is not known already, and analysts might *assume* their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.

### Activations in a Forest

As a general setting, the number of node activations (e.g. for datasets like co-authorship) is much smaller than the set of nodes ($\|\mathbf{x}_i\in\mathbb{S}^n\|_0 \ll n$)^[
  $\|\cdot\|_0$ is the $\ell_0$ "pseudonorm", counting non-zero elements (the support) of its argument.
]  
@Semisupervisedlearning_Avrachenkov2017 go to some length describing discrete- and continuous-time random walk models that can give rise to binary activation vectors like our $X(i,j):I\times J\rightarrow \mathbb{B}$.
The _regularized laplacian_ (or _forest_)kernel of a graph[@SimilaritiesgraphsKernels_Avrachenkov2019] plays a central role in their analysis, as it will in our discussion going forward.

$$
Q_{\beta} = (I+\beta L)^{-1}
$${#eq-regulap}

In that work, it is discussed as the optimal solution to the semi-supervised "node labeling" problem, having a regularization parameter $\beta$, though its uses go far beyond this.[@GraphLaplacianRegularization_Pang2017;@Countingrootedforests_Knill2013;@MatrixForestTheorem_Chebotarev2006]
$Q$ generalizes the so-called "heat kernel" $\exp{(-t\tilde{L})}$, in the sense that it solves a lagrangian relaxation of a loss function based on the heat equation.
This can be related to the PageRank ($\exp{(-tL^{\text{rw}})}$) kernel as well, which is explicitly based on random walk transition probabilities.

In fact, $Q$ can be viewed as a transition matrix for a random walk having a geometrically distributed number of steps, giving us a small expected support for $\mathbf{x}_i$, as needed.^[$Q$ can also be interpreted as a continuous-time random walk location probability, after exponentially distributed time, if spending exponentially-distributed time in each node.]
However we interpret $Q$, a remarkable fact emerges due to a theorem by Chebotarev: each entry $q=Q(u,v)$ is equal to the probability^[edge weights scaled by \beta] that nodes $u,v$ are connected in a randomly sampled _spanning rooted forest_

In other words, co-occurring node activations due to a random walk or heat kernel are deeply tied to the chance that those nodes find themselves _on the same tree in a forest_.

### Spreading Dependencies as Trees

{{< embed ../codefigs/graphs.qmd#fig-stack-tree >}}

With the overt link from spreading processes to counts of trees made, there's room for a more intuitive bridge.

For single-cause, single source spreading process activations---on a graph---the activation dependency graph for each observation/spread/random walk _must be a tree_.
With a single cause (the "root"), which is the starting position of a random walker, a node can only be reached (activated) by another currently activated node.
If the random walk jumps from one visited node to another, previously visited one, that transition did not result in an activation , so the _dependency_ count for that edge should not increase.
This description of a random walk, where subsequent visits do not "store" the incoming transition, is roughly equivalent to one more commonly described as a _Loop-Erased_ random walk.
It is precisely used to uniformly sample the space of spanning trees on a graph.[@Generatingrandomspanning_Wilson1996]

Much like a reluctant co-author "worn down" by multiple requests, we can even include random walks that "receive" activation potential from more than one source.
Say a node is activated when some fraction of its neighbors have all originated a random walk transition to it, or a node activates on its second visit, or similar. 
We simply count (as dependency evidence) the ultimate transition that precipitated activation.
This could be justified from an empirical perspective as well: say we observe an author turn down requests for one paper from two individuals, but accept a third.
We could actually infer a _lowered_ dependency on the first two, _despite_ the eventual coauthorship.
Only the interaction that was observed as successful necessarily counts toward  success-dependency, barring any contradicting information.

It's important to add here that _mutual convincing_ by multiple collaborators simultaneously (or over time) is expressly left out.
In other words, only pairwise interactions are permitted.
This is not an additional assumption, but a key limitation of our use of graphs in the first place!
As Torres et al. go to great lengths elaborating in [@WhyHowWhen_Torres2021], it is critical to correctly model dependencies when selecting a structural representation of our problem to avoid data loss.
The possibility for multi-way interactions would necessitate the use of either a simplicial complex or a hypergraph as the carrier structure, _not a graph_. 

@fig-stack-tree demonstrates the use of trees as the distribution for subgraphs, instead of outer-products/cliques.  



## Sparse Approximation

As indicated previously, we desire a representation of each observation that takes the "node space" vectors ($\mathbf{x}_i$) to "edge space" ones ($\mathbf{r}_i$).
We have separated each observation with the intention of finding a point-estimate for the "best" edges, such that the edge vector induces a subgraph belonging to a desired class.
If we assume that each edge vector is in $\mathbb{B}^{\omega}$, so that the interactions are unweighted, undirected, simple graphs, then for any family of subgraphs we will be selecting from at most $\omega\leq {n\choose 2}$ edges.
 
Representing a vector as sparse combination of a known set of vectors (also known as "atoms") is called _sparse approximation_.

### Problem Specification

Sparse approximation of a vector $\mathbf{x}$ as a representation $\mathbf{r}$ using a dictionary of atoms (columns of $D$) is specified more concretely as [@EfficientimplementationK_Rubinstein2008]: 
$$\mathbf{\hat{r}} = \operatorname*{argmin}_{\mathbf{r}}{\|\mathbf{x}-D\mathbf{r} \|_2^2} \quad \text{s.t.} \|\mathbf{r}\|_0\leq N $${#eq-sparse-approx}
where $N$ serves as a sparsity constraint (at most $N$ non-zero entries).
This is known to be NP-hard, though a number of efficient methods to approximate a solution are well-studies and widely used.
Solving the lagrangian form of @eq-sparse-approx, with an $\ell_1$-norm in place of $\ell_0$, is known as _Basis Pursuit[@SparseApproximateSolutions_Natarajan1995], while greedily solving for the non-zeros of $\mathbf{r}$ one-at-a-time is called _matching pursuit_[@Matchingpursuitstime_Mallat1993].
In that work, each iteration selects the atom with the largest inner product $\langle \mathbf{d}_{i'},\mathbf{x}\rangle$.

We take an approach similar to this, but with the insight that the inner product will not result in desired sparsity (namely, a tree).
Our dictionary in this case will be the set of edges given by $B$ (see @sec-subgraph-dists), while our sparsity is given by the relationship of the numbers of nodes and edges in a tree:
$$
\mathbf{\hat{r}} = \operatorname*{argmin}_{\mathbf{r}}{\|\mathbf{x}-B^T\mathbf{r} \|_2^2} \quad \text{s.t.} \|\mathbf{r}\|_0\leq \|\mathbf{x}\|_0 - 1
$${#eq-sparse-approx-tree}

There are some oddities to take into account here.
As a linear operator (see @sec-lin-ops), $B^T$ takes a vector of edges to node-space, counting the number of edges each node was incident to.
This means that, even with a ground-truth set of interactions, $B^T$ would take them to a new matrix $X_{\text{deg}}(i,j):I\times J \rightarrow \mathbb{N}$, which has entries of the number of interactions each individual in observation $i$ was involved in.
While very useful for downstream analysis (see @sec-fp-preprocess), the MSE loss in @eq-sparse-approx-tree will never be zero, since $X_{\text{deg}}$ entries are not boolean.
Large-degree "hub" nodes in the true graph would give a large residual. 

It might be possible to utilize a specific semiring, such as $(\min,+)$, to enforce inner products (see @sec-products) that take us back to a binary vector.^[
  This is more than a simple hack, and belies a great depth of possible connection to the problem at hand.
  It is known that "lines" (arising from equations of the inner product) in tropical projective space _are trees_.[@tropicalGrassmannian_2004]
  In addition, the tropical equivalent to Kirchoff's polynomial (which counts over all possible spanning trees), is the direct computation of the minimum spanning tree.[@TropicalKirchhoffsformula_Jukna2021]
  For treatment of sparse approximation using tropical matrix factorization, see @Sparsedataembedding_Omanovic2021
] 
Instead, we will take an empirical bayes approach to the estimation of sparse vectors.[@EmpiricalBayesianStrategy_Wipf2007]
As a probabilistic graphical model, we assume each observation is emitted from a (tree-structured) MRF [explained?] on the activated nodes.
This is underdetermined (any spanning tree could equally emit the observed activations), so we use an empirical prior as a form of shrinkage: the co-occurrences of nodes across all observed activation patterns.
This let's us optimize a likelihood from @eq-edgevec-prob, for the distribution of spanning trees on the subgraph of $G^*$ inducted by $\mathbf{x}$. 
$$
\mathbf{\hat{r}} = \operatorname*{argmax}_{\mathbf{r}}{\mathcal{L}(\mathbf{r}|\mathbf{x})} \quad \text{s.t.}\quad \mathbf{r}\sim \mathcal{T}(G^*[\mathbf{x}])
$${#eq-sparse-approx-tree}

### Max. Spanning (Steiner) Trees
The point estimate $\hat{\mathbf{r}}$ is therefore the mode of a distribution over trees, which is precisely the maximum spanning tree.[@EfficientComputationExpectations_Zmigrod2021]
If we allow the use of all observations $X$ to find an empirical prior for $\mathbf{r}$, then we can calculate a value for the mutual information for the activated nodes, and use this to directly calculate the Chow-Liu estimate.
One algorithm for finding a maximum spanning tree is Prim's[CITE?], which effectively performs the matching pursuit technique of greedily adding an edge (i.e. non-zero entry in our vector) one-by-one.
In this way, we effectively _do_ perform matching pursuit, but minimizing the KL-divergence between observed co-occurrences of nodes 


- aggregate/marginalize over roots in a rooted tree (efficient expectations)...also MI is symmetric (same tree for any root)
- total tree: overcounting of independence is at most the spanning tree (hunter worsely)
- generally max spanning tree is the mode for distribution, so there's our point estimate.

MST for each node set

- KL-divergence is convex (we can minimize sum of dists, not just dist of sum)
- steiner tree and approx with closure...which we don't have 
- oh look! counting trees is similar. use cosine because geometric pooling

## Forest Pursuit: Approximate Recovery in Near-linear Time {#sec-FP}

I.e. the PLOS paper (modified basis-pursuit via MSTs)

### Algorithm Summary

code box

example:

```pseudocode
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-placement: "htb!"
#| pdf-line-number: true

\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\Procedure{Quicksort}{$A, p, r$}
  \If{$p < r$}
    \State $q = $ \Call{Partition}{$A, p, r$}
    \State \Call{Quicksort}{$A, p, q - 1$}
    \State \Call{Quicksort}{$A, q + 1, r$}
  \EndIf
\EndProcedure
\Procedure{Partition}{$A, p, r$}
  \State $x = A[r]$
  \State $i = p - 1$
  \For{$j = p$ \To $r - 1$}
    \If{$A[j] < x$}
      \State $i = i + 1$
      \State exchange
      $A[i]$ with     $A[j]$
    \EndIf
    \State exchange $A[i]$ with $A[r]$
  \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
```
### Approximate Complexity


## Simulation Study {#sec-FP-experiments}

MENDR + affinis

### Method

what kinds, distributions, etc. 
- glasso gets 5CV to select reg param

### Results - Scoring

{{< embed ../codefigs/results.qmd#fig-fp-overall >}}

{{< embed ../codefigs/results.qmd#fig-fp-compare >}}

{{< embed ../codefigs/results.qmd#fig-partials-mcc >}}

### Results - Performance

{{< embed ../codefigs/results.qmd#fig-runtime >}}

{{< embed ../codefigs/results.qmd#fig-partials-runtime >}}

## Discussion


### Interaction Probability

![](../images/PR.svg)

### Conclusion
