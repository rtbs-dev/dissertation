[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Measuring Network Dependencies from Node Activations",
    "section": "",
    "text": "Preface\n\n\nForeward\n\n\nAcknowledgements",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "content/0-intro.html",
    "href": "content/0-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ambiguous Metrology\nA wide variety of fields show consistent interest in inferring latent network structure from observed interactions, from human cognition and social infection networks, to marketing, traffic, finance, and many others. [1] However, an increasing number of authors are noting a lack of agreement in how to approach the metrology of this problem. This includes rampant disconnects between the theoretical and methodological network analysis sub-communities[2], treatment of error as purely aleatory, rather than epistemic [3], or simply ignoring measurement error in network reconstruction entirely[4].\nNetworks in the “wild” rarely exist of and by themsleves. Rather, they are a model of interaction or relation between things that were observed. One of the most beloved examples of a network, the famed Zachary’s Karate Club[5], is in fact reported as a list of pairwise interactions: every time a club member interacted with another (outside of the club), Zachary recorded it as two integers (the IDs of the members). The final list of pairs can be interpreted as an “edge list”, which can be modeled with a network: a simple graph. This was famously used to show natural community structure that nicely matches the group separation that eventually took place when the club split into two.[6]\nNote, however, that we could have just as easily taken note of the instigating student for each interaction (i.e. which student initiated conversation, or invited the other to socialize, etc.). If that relational asymmetry is available, our “edges” are now directed, and we might be able to ask questions about the rates that certain students are asked vs. do the asking, and what that implies about group cohesion. Additionally, the time span is assumed to be “for the duration of observation” (did the students ever interact), but if observation time was significantly longer, say, multiple years, we might question the credulity of treating a social interaction 2 years ago as equally important to an interaction immediately preceding the split. This is now a “dynamic” graph; or, if we only measure relative to the time of separation, at the very least a “weighted” one.\nWe do not know if any of these are true. In fact, as illustrated in Figure 1.1, we do not know if the network being described from the original edge data even has 77 or 78 edges, due to ambiguous reporting in the original work. Lacking a precise definition of what the graph’s components (i.e. it’s edges) are, as measurable entities, means we cannot estimate the measurement error in the graph.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/0-intro.html#ambiguous-metrology",
    "href": "content/0-intro.html#ambiguous-metrology",
    "title": "Introduction",
    "section": "",
    "text": "[5] W. W. Zachary, “An information flow model for conflict and fission in small groups,” Journal of Anthropological Research, vol. 33, no. 4, pp. 452–473, Dec. 1977, doi: 10.1086/jar.33.4.3629752.\n\n[6] M. Girvan and M. E. J. Newman, “Community structure in social and biological networks,” Proceedings of the National Academy of Sciences, vol. 99, no. 12, pp. 7821–7826, Jun. 2002, doi: 10.1073/pnas.122653799.\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Zachary’s Karate Club, with ambiguously extant edge 78 highlighted.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/0-intro.html#indirect-network-measurement",
    "href": "content/0-intro.html#indirect-network-measurement",
    "title": "Introduction",
    "section": "Indirect Network Measurement",
    "text": "Indirect Network Measurement\nWhile the karate club graph has unquantified edge uncertainty derived from ambiguous edge measurements, we are fortunate that we have edge measurements. Regardless of how the data was collected, it is de facto reported as a list of pairs. In many cases, we simply do not have such luxury. Instead, our edges are only measured indirectly, and instead we are left with lists of node co-ocurrences. Networks connecting movies as being “similar” might be derived from data that lists sets of movies watched by each user; networks of disease spread pathways might be implied from patient infection records; famously, we might build a network of collaboration strength between academic authors by mining datasets of the papers they co-author together.\nSuch networks are derived from what we will call node activation data, i.e., records of what entities happened “together”, whether contemporaneously, or in some other context or artifact.\n\n\n\n\n\n\n\n\n\nFigure 1.2\n\n\n\n\n\n\nThese are naturally represented as “bipartite” networks, having separate entites for, say, “papers” and “authors”, and connecting them with edges (paper 1 is “connected” to its authors E,H,C, etc.). But analysts are typically seeking the collaboration network connecting authors (or papers) themselves! Networks of relationships in this situation are not directly observed, but which if recovered could provide estimates for community structure, importances of individual authors (e.g. as controlling flow of information), and the “distances” that separate authors from each other, in their respective domains. [7] Common practice assumes that co-authorship in any paper is sufficient evidence of at least some level of social “acquaintance”, where more papers shared means more “connected”.\n\n[7] M. E. J. Newman, “Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality,” Physical Review E, vol. 64, no. 1, p. 016132, Jun. 2001, doi: 10.1103/physreve.64.016132.\n\n\n\n\n\n\n\n\n\nFigure 1.3\n\n\n\n\n\n\nThus our social collaboration network is borne out of indirect measurements: author connection is implied through “occasions when co-authorship occurred”. However, authors of papers may recall times that others were added, not by their choice, but by someone else already involved. In fact, the final author list of most papers is reasonably a result of individuals choosing to invite others, not a unanimous, simultaneous decision by all members. Let’s imagine we wished to study the social network of collaboration more directly: if we had the luxury of being in situ as, say, a sociologist performing an academic ethnography, we might have been more strict with our definition of “connection”. If the goal is a meaningful social network reflecting the strength of interraction between colleages, perhaps the we prefer our edges represent “mutual willingness to collaborate”. Edge “measurement”, then, could involve records of events that show willingness to seek or participate in collaboration event, such as:\n\nauthor (g) asked (e), (h), and (c) to co-author a paper, all of whom agreed\n(i) asked (f) and (j), but (j) wanted to add (b)’s expertise before writing one of the sections\n\nand so on. Each time two colleagues had an opportunity to work together and it was seized upon we might conclude that evidence of their relationship strengthed. With data like this, we could be more confident in claiming our collaboration network can serve as “ground truth,” as far as empirically confirmed collaborations go. However, even if the underlying “activations” are identical, our new, directly measured graph looks very different.\n\n\n\n\n\n\n\n\n\nFigure 1.4: graph of mutual collaboration relationships i.e. the “ground truth” social network\n\n\n\n\n\n\nFundamentally, the network in Figure 1.4 shows which relationships the authors depend on to accomplish their publishing activity. When causal relations between nodes are being modeled as edges, we call such a graph a dependency network. We will investigate this idea further later on, but ultimately, if a network of dependencies is desired (or implied, based on analysis needs), then the critical problem remaining is how do we recover dependency networks from node activations? Additionally, what goes wrong when we use co-occurence/activation data to estimate the dependency network, especially when we wish to use it for metrics like centrality, shortest path distances, and community belonging?\n\n\n\n\n\n\n\n\n\nFigure 1.5: Recovering underlying dependency networks from node-cooccurrences.\n\n\n\n\n\n\nEven more practically, networks created directly from bipartite-style data are notorious for quickly becoming far too dense for useful analysis, earning them the (not-so-)loving moniker “hairballs”. Network “backboning,” as it has come to be called tries to find a subset of edges in this hairball that still captures it’s core topology in a way that’s easier to visualize.[8], [9] Meanwhile, underlying networks of dependencies that cause node activation patterns can provide this: they are almost always more sparse than their hairballs. Accessing the dependency backbone in a principled way is difficult, but doing so in a rapid, scalable manner is critical for practitioners to be able to make use of it to trim their hairballs.\n\n[8] P. B. Slater, “A two-stage algorithm for extracting the multiscale backbone of complex weighted networks,” Proceedings of the National Academy of Sciences, vol. 106, no. 26, pp. E66–E66, Jun. 2009, doi: 10.1073/pnas.0904725106.\n\n[9] Z. Neal, “The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors,” Social Networks, vol. 39, pp. 84–97, Oct. 2014, doi: 10.1016/j.socnet.2014.06.001.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/0-intro.html#scope-of-this-work",
    "href": "content/0-intro.html#scope-of-this-work",
    "title": "Introduction",
    "section": "Scope of this work",
    "text": "Scope of this work\nThe purpose of this thesis is to provide a solid foundation for basic edge metrology when our data consists of binary node activations, by framing network analysis as a problem of inference, as suggested by [2]. We give special focus to binary activations that occur due to spreading processes, such as random walks or cascades on an underlying carrier graph. Recovering the carrier, or, “dependency” network from node activations is of great interest to the network backboning and causal modeling communities, but often involves either unspoken sources of epistemic and aleatory error, or high computation costs (or both). To begin addressing these issues, we present a guide to current practices, pitfalls, and how common statistical tools apply to the network recovery problem: a Practitioner’s Guide to Network Recovery. We cover what “measurement” means in our context, and specifically the ways we encode observations, operations, and uncertainties numerically. Clarifying what different versions of what “relation” means (whether proximity or incidence) is critical, since network structure is intended to encode such relations as mathematical objects, despite common ambiguities and confusion around what practitioners intend on communicating through them. Then we use this structure to present a cohesive framework for selecting a useful network recovery technique, based on the available data and where in the data processing pipeline is acceptable to admit either extra modeling assumptions or information loss.\n\n[2] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\nNext, building on a gap found in the first part, we present a novel method, Forest Pursuit, to extract dependency networks when we know a spreading process causes node activation (e.g. paper co-authorship caused by collaboration requests). We create a new reference dataset to enable community benchmarking of network recovery techniques, and use it show greatly improved accuracy over many other widely-used methods. Forest Pursuit in its simplest form scales linearly with the size of active-node sets, being trivially parallelizable and streamable over dataset size, and agnostic to network size overall. We then expand our analysis to re-imagine Forest Pursuit as a Bayesian probabilistic model, Latent Forest Allocation, which has an easily-implemented Expectation Maximization scheme for posterior estimation. This significantly improves upon the accuracy results of Forest Pursuit, at the cost of some speed and scalability, giving analysts multiple options to adapt to their needs.\nLast, we apply Forest Pursuit to several qualitative case-studies, including a scientific collaboration network, and the verbal fluency “animals” network recovery problem, which dramatically change interpretation under use of our method. We investigate its use as a low-cost preprocessor for other methods of network recovery,like GLASSO, improving their stability and interpretability. Finally we discuss the special case when node activations are reported as an ordered set, where accounting for cascade-like effects becomes crucial to balance false positive and false-negative edge prediction. Along with application of this idea to knowledge-graph creation from technical language in the form maintenance work-order data, we discuss more broadly the future needs of network recovery, specifically in the context of embeddings and gradient-based machine learning toolkits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html",
    "href": "content/part1/1-01-matrix-meas.html",
    "title": "Metrology as matrices",
    "section": "",
    "text": "Observation and feature “spaces”\nWhere metrology is concerned, the actual unit of observation and how it is encoded for us is critical to how analysts may proceed with quantifying, modeling, and measuring uncertainty around observed phenomena. Experiment and observation tends to be organized as inputs and outputs, or, independent variables and dependent variables, specifically. Independent variables are observed, multiple times (“observations”), and changes in outcome for each can be compared to the varying values associated with the independent variable input (“features”). For generality, say a practitioner records their measurements as scalar values, i.e. \\(x\\in\\mathbb{S}\\in\\{\\mathbb{R,Z,N},\\cdots\\}\\). The structure most often used to record scalar values of \\(n\\) independent/input variable features over the course of \\(m\\) observations is called a design matrix \\(X:\\mathbb{S}^{m\\times n}\\).1\nIf we index a set of observations and features, respectively, as \\[ i\\in I=\\{1,\\cdots,m\\}, \\quad j\\in J=\\{1,\\cdots,n\\},\\qquad I,J:\\mathbb{N}\\] then the design matrix can map the index of an observation and a feature to the corresponding measurement. \\[\nx=X(i,j)\\qquad X : I\\times J \\rightarrow \\mathbb{S}\n\\tag{2.1}\\] i.e. the measured value of the \\(j\\)th independent variable from the \\(i\\)th observation.2 In this scheme, an “observation” is a single row vector of features in \\(\\mathbb{S}^{n\\times 1}\\) (or simply \\(\\mathbb{S}^{n}\\)), such that each observation encodes a position in the space defined by the features, i.e. the feature space, and extracting a specific observation vector \\(i\\) from the entire matrix can be denoted as \\[\\mathbf{x}_i=X(i,\\cdot),\\quad \\mathbf{x}:J\\rightarrow\\mathbb{S}\\] Similarly, every “feature” is associated with a single column vector in \\(\\mathbb{S}^{1\\times m}\\), which can likewise be interpreted as a position in the space of observations (the data space): \\[\\mathbf{x}_j'=X(\\cdot,j),\\quad \\mathbf{x}':I\\rightarrow\\mathbb{S}\\] Note that this definition could be swapped without loss of generality. In other words, \\(\\mathbf{x}\\) and \\(\\mathbf{x}'\\) being in row and column spaces is somewhat arbitrary, having more to do with the logistics of experiment design and data collection. We could have measured our feature vectors one-at-a-time, measuring their values over an entire “population”, in effect treating that as the independent variable set.3\nTo illustrate this formalism in a relevant domain, let’s take another look at co-citation networks. For \\(m\\) papers we might be aware of \\(n\\) total authors. For a given paper, we are able to see which authors are involved, and we say those authors “activated” for that paper. It makes sense that our observations are individual papers, while the features might be the set of possible authors. However, we are not given information about which author was invited by which other one, or when each author signed on. In other words, the measured values are strictly boolean, and we can structure our dataset as a design matrix \\(X:\\mathbb{B}^{m\\times n}\\). We can then think of the \\(i^{\\mathrm{th}}\\) paper as being represented by a vector \\(\\mathbf{x}_i:\\mathbb{B}^n\\), and proceed using it in our various statistical models. If we desired to analyze the set of authors, say, in order to determine their relative neighborhoods or latent author communities, we could equally use the feature vectors for each paper, this time represented in a vector \\(\\mathbf{x}'_j:\\mathbb{B}^{1\\times m}\\).",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-matrix-notation",
    "href": "content/part1/1-01-matrix-meas.html#sec-matrix-notation",
    "title": "Metrology as matrices",
    "section": "",
    "text": "2  This notation is adapted from the sparse linear algebraic treatment of graphs in [1] and [2]. \n[1] J. Kepner and J. Gilbert, Graph algorithms in the language of linear algebra. Philadelphia: Society for Industrial; Applied Mathematics, 2011.\n\n[2] J. Kepner et al., “Mathematical foundations of the GraphBLAS,” in 2016 IEEE high performance extreme computing conference ( HPEC), Sep. 2016, pp. 1–9. doi: 10.1109/HPEC.2016.7761646.\n3  In fact, vectors are often said to be in the column-space of a matrix, especially when using them as transformations in physics or deep learning layers. We generally follow a one-observation-per-row rule, unless otherwise stated.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-lin-ops",
    "href": "content/part1/1-01-matrix-meas.html#sec-lin-ops",
    "title": "Metrology as matrices",
    "section": "Models & linear operators",
    "text": "Models & linear operators\nAnother powerful tool an analyst has is modeling the observation process. This is relevant when the observed data is hypothesized to be generated by a process we can represent mathematically, but we do not know the parameter values to best represent the observations (or the observations are “noisy” and we want to find a “best” parameters that account for this noise). This is applicable to much of scientific inquiry, though one common use-case is the de-blurring of observed images (or de-noising of signals), since we might have a model for how blurring “operated” on the original image to give us the blurred one. We call this “blurring” a linear operator if it can be represented as a matrix4, and applying it to a model with \\(l\\) parameters is called the forward map: \\[\\mathbf{x} = F\\mathbf{p}\\qquad F:\\mathbb{R}^{l}\\rightarrow \\mathbb{R}^n\\] where \\(P\\) is the space of possible parameter vectors, i.e. the model space. The forward map takes a modeled vector and predicts a location in data space.\n4 in the finite-dimensional caseOf critical importance, then, is our ability to recover some model parameters from our observed data, e.g. if our images were blurred through convolution with a blurring kernel, then we are interested in deconvolution. If \\(F\\) is invertible, the most direct solution might be to apply the operator to the data, as the adjoint map: \\[ \\mathbf{p} = F^H\\mathbf{x}\\qquad F^H:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^l\\] which removes the effect of \\(F\\) from the data \\(\\mathbf{x}\\) to recover the desired model \\(\\mathbf{p}\\).\nTrivially we might have an orthogonal matrix \\(F\\), so \\(F^H=F^{-1}\\) is available directly. In practice, other approaches are used to minimize the residual: \\(\\hat{\\mathbf{p}}^=\\min_{\\mathbf{p}} F\\mathbf{p}-\\mathbf{x}\\). Setting the gradient to 0 yields the normal equation, such that \\[ \\hat{\\mathbf{p}}=(F^TF)^{-1}F^T\\mathbf{x}\\] This should be familiar to readers as equivalent to solving ordinary least-squares (OLS). However, in that case it is more often shown as having the design matrix \\(X\\) in place of the operator \\(F\\).\nThis is a critical distinction to make: OLS as a “supervised” learning method treats some of the observed data we represented as a design matrix previously as a target to be modeled, \\(y=X(\\cdot,j)\\), and the rest maps parameters into data space, \\(F=X(\\cdot,J/j)\\). With this paradigm, only the target is being “modeled” and the rest of the data is used to create the operator. In the citation network example, it would be equivalent to trying to predict one variable, like citation count or a specific author’s participation in every paper, given every other author’s participation in them.\nFor simplicity, most work in the supervised setting treats the reduced data matrix as X, opting to treat \\(y\\) as a separate dependent variable. However, our setting will remain unsupervised, since no single target variable is of specific interest—all observations are “data”. In this, we more closely align with the deconvolution literature, such that we are seeking a model and an operation on it that will produce the observed behavior in an “optimal” way.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-smooth-err",
    "href": "content/part1/1-01-matrix-meas.html#sec-smooth-err",
    "title": "Metrology as matrices",
    "section": "Measurement quantification & error",
    "text": "Measurement quantification & error\nIn binary data, such as what we have been considering, it is common to model observables as so-called “Bernoulli trials”: events with two possible outcomes (on, off; yes, no; true, false), and one outcome has probability \\(p\\). These can be thought of as weighted coin-flips: “heads” with probability \\(p\\), and “tails” \\(1-p\\). If \\(k\\) trials are performed (the “exposure”), we say the number of successes \\(s\\) (the “count”) is distributed as a binomial distribution \\(s\\sim Bin(p,k)\\). The empirical estimate for the success probability is \\(\\hat{p}=\\tfrac{s}{k}\\).\nNote that this naturally resembles marginal sums on our design matrix \\(X\\), if we treat columns (or rows!) as an array of samples from independent Bernoulli trials: \\(\\hat{p}_j = \\frac{\\sum_{i\\in I} X(i,j)}{m}\\). Many probability estimates involving repeated measurements of binary variables (not simply the row/column variables) have this sort of \\(\\frac{\\textrm{count}}{\\textrm{exposure}}\\) structure, as will become useful in later sections.\nHowever, if we are “measuring” a probability, we run into issues when we need to quantify our uncertainty about it. For instance, an event might be quite rare, but if in our specific sample we never see it, we still do not generally accept a probability of zero.\n\nAdditive Smoothing\nOne approach to dealing with this involves adding pseudocounts that smooth out our estimates for count/exposure, from which we get the name “additive smoothing”.[CITE?] \\[\\hat{p} = \\frac{s+\\alpha}{k+2\\alpha} \\] Adding 1 success and 1 failure (\\(\\alpha=1\\)) as pseudocounts to our observations is called Laplace’s Rule of Succession, or simply “Laplace smoothing,”5 while adding \\(\\alpha=0.5\\) successes and failures is called using Jeffrey’s Prior. It’s so-called because this pseudocount turns out to be a special case of selecting a Bayesian prior on the binomial probability (a.k.a. a Beta-Binomial distribution) \\(p\\sim \\textrm{Beta}(\\alpha, \\beta)\\), such that the posterior distribution after our success/failure counts is \\(\\textrm{Beta}(s+\\alpha, k-s+\\beta)\\), which has the mean: \\[E[p|s,k]=\\frac{s+\\alpha}{k+\\alpha+\\beta}\\]\n5  derived when Laplace desired estimates of probability for unobserved phenomena, such as the sun (not) rising tomorrow.6  A useful comparison of the two priors (1, 0.5) is to ask, given all of the trials we have seen so far, whether we believe we are near the “end” or “middle” of an average run of trials. For \\(\\alpha=1\\), we believe nearly all evidence has been collected, but for \\(\\alpha=0.5\\), only half of expected evidence has been observed.\nThis exactly recovers additive smoothing with a Jeffrey’s prior for \\(\\alpha=\\beta=0.5\\).6 This generalization allows us to be more flexible, and specify our prior expectations on counts or exposure with more precision. Such models provide both an estimate of the aleatory uncertainty (via the posterior distribution), and a form of “shrinkage” that prevents sampling noise from unduly affecting parameter estimates (via the prior distribution). Despite being a simple foundation, this treatment of “counts” and “exposure” can be built upon in many ways.\n\n\nConditional Probabilities & Contingencies\nIn dependency/structure recovery, since our goal involves estimating (at least) pairwise relationships, the independence assumption required to estimate node occurrences as Beta-Binomial is clearly violated7.\n7  In fact, a recent method from [3] models probabilistic binary observations, with dependencies, by generalizing the mechanics overviewed here to a fully multivariate Bernoulli distribution, capable of including 3rd- and higher-order interractions, not just pairwise.\n\n[3] B. Dai, S. Ding, and G. Wahba, “Multivariate bernoulli distribution,” Bernoulli, vol. 19, no. 4, Sep. 2013, doi: 10.3150/12-bejsp10.\nHowever, it’s natural to make use of joint (\\(P(A\\cup B)\\), how often does A happen with B, out of all data?) and conditional (\\(P(A|B)\\) how often A given B; or \\(P(B|A)\\), how often B given A) probabilities between nodes, while trying to estimate dependencies. Once again, we can estimate the base probabilities for each node from marginal sums, but the joint and conditional probabilities can instead be estimated using matrix multiplication using the Gram matrix, discussed below. It encodes pair-wise co-occurrence counts, such that \\(G(i,i'):\\mathbb{Z}^{n\\times n}\\) has the co-occurrence count for node \\(i\\) with \\(i'\\).\nThe co-occurrence probability for each pair can be approximated with the beta-binomial scheme mentioned above, but care must be taken not to confuse this with the edge strength connecting two nodes. First, nodes that rarely activate (low node probability) may nonetheless reliably connect to others when they do occur (high edge probability). In fact, without direct observation of edges, we are not able to estimate their count, or their exposure, which can be a source of systemic error from epistemic uncertainty. We don’t know when edges are used, directly, and we also don’t have a reliable way to estimate the opportunities each edge had to activate (their exposure), either. This is especially true when we wish to know whether an edge even can be traversed, i.e. the edge support. Support, as used in this sense, is the set of inputs for which we expect a non-zero output. Intuitively, this idea captures the sense that we might care more about whether an edge/dependency exists, not how important it is. For that, we have to re-assess our simple model: even if we could count the number of times an edge might have been traversed, how do we estimate the opportunities it had to be available for traversal (it’s “exposure”)?\nAssuming this kind of epistemic uncertainty can be adequately addressed through modeling—attempts at which will be discussed in more detail in Roads to Network Recovery—conditional probability/contingency tables will again be useful for validation. When comparing estimated edge probability to some known “true” edge existence (if we have that), we can count the number of correct predictions, as well as type I (false positive) and type II (false negative) errors. We can do this at every probability/weight threshold value, as well, and we will return to ways to aggregate all of these values into useful scoring metrics in Simulation Study.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-products",
    "href": "content/part1/1-01-matrix-meas.html#sec-products",
    "title": "Metrology as matrices",
    "section": "Proximity vs. Incidence",
    "text": "Proximity vs. Incidence\nAs we have already seen, operations from linear algebra make many counting and combinatoric tasks easier, while unifying disparate concepts to a common set of mechanics. In addition to having a map from integer indices to sets of interest, these design matrices/vectors are implicitly assumed to have entries that exist in a field \\(F=(\\mathbb{S},\\oplus,\\otimes)\\). equipped with operators analogous to addition (\\(\\oplus\\)) and multiplication (\\(\\otimes\\)).8 With this, we are able to define generalized inner products that take pairs vectors in a vector space \\(\\mathbf{x}\\in V\\), such that \\(\\langle\\cdot,\\cdot\\rangle_F:\\mathbb{S}^n\\times \\mathbb{S}^n\\rightarrow \\mathbb{S}\\). \\[\n\\langle\\mathbf{x}_a,\\mathbf{x}_b\\rangle_{F} = \\bigoplus_{j=1}^n \\mathbf{x}_a(j)\\otimes\\mathbf{x}_b(j)\n\\]\n8  Or, more generally, a semiring if inverse operations for \\(\\oplus,\\otimes\\) don’t exist.We can use this to perform “contractions” along any matching dimensions of matrices as well, since the sum index is well-defined. \\[\n\\begin{aligned}\nX\\in\\mathbb{S}^{m\\times n}\\quad Y\\in\\mathbb{S}^{n\\times m} \\\\\nZ(i,j)=X\\oplus,\\otimes Y = \\bigoplus_{j=1}^{n} X(i,j) \\otimes Y(j,k) = XY\n\\end{aligned}\n\\] For ease-of-use, we will assume the standard field for any given set \\((\\mathbb{S},+,\\times)\\) if not specified otherwise, which recovers standard inner products \\(\\langle\\cdot,\\cdot\\rangle\\). However, [2] illustrates the usefulness of various fields (or semirings). They allow linear-algebraic representation of many graph operations, such as shortest paths through inner products over \\((\\mathbb{R}\\cup -\\inf,\\textrm{min}, +)\\). This works because discrete/boolean edge weights will not accumulate extra strength beyond 1 under contraction over observations.\n\nKernels & distances\nAs alluded to in the previous section, co-occurrence have a deep connection to a Gram matrix, which is a matrix of all pairwise inner products over a set of vectors.\n\\[\nX^TX=G(j,j')=\\langle\\mathbf{x'}_j,\\mathbf{x}_{j'}'\\rangle = \\sum_{i=1}^{m} X(i,j)X(i,j')\n\\tag{2.2}\\]\nMatrices that can be decomposed into another matrix and its transpose are symmetric, and positive semidefinite, making every gram matrix positive semidefinite. They are directly related to (squared) euclidean distances through the polarization identity[CITE?]:9 \\[\nd^2(j,j') = \\|\\mathbf{x}_{j'}'-\\mathbf{x}_{j'}'\\|^2 = G(j,j) - 2G(j,j') + G(j',j')\n\\tag{2.3}\\]\n9 Important: these definitions are all using the \\(\\mathbf{x}'\\) notation to indicate that these measurements are almost exclusively being done in the data space, i.e. on column vectors. While most definitions work on distances in terms of the measurements/objects/data, for inverse problems (like network recovery, structure learning, etc.) they are more often applied in terms of the features (here, the nodes. This can be seen in statistics as well, where covariance and correlation matrices (which are related to the gram and distance matrix definitions above), are defined as relationships between features/dimensions, not individual samples.In our example from before, the gram matrix will have entries showing the number of papers shared by two authors (or total papers by each, on the diagonal). This is because an inner product between two author (column) vectors will add 1 for each paper in the sum only if it has both authors in common. This is called a bipartite projection[CITE] into the authors “mode”, and is illustrated visually in Figure 1.3.\n\n\nIncidence structures & dependency\nTODO\nfoundational model of graph theory and incidence structures more broadly. More to come, but get the terminology down.\n\nSpring example, road example, etc.\npartial correlations\n\n\n\nImplications for networks\nTODO\nHow “close” or “far away” things are…. [4] et al.  Usually dependencies are taken as causing or enabling proximity. E.g. shortest paths, vs. edges.\n\n[4] K. Avrachenkov, P. Chebotarev, and D. Rubanov, “Similarities on graphs: Kernels versus proximity measures,” European Journal of Combinatorics, vol. 80, pp. 47–56, Aug. 2019, doi: 10.1016/j.ejc.2018.02.002.\n\nDiscuss Complex Systems and their representation.\n\nThe approach taken by researchers/investigators…do they assume a level of interchangeability between the two kinds of “relation”? Do they define Or do they",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html",
    "href": "content/part1/1-02-graph-obs.html",
    "title": "Incidence through vector representation",
    "section": "",
    "text": "Graphs as incidence structures\nTo provide a sufficiently rigorous foundation for network recovery from binary occurrence, we will need a rigorous way to represent networks and occurrences that lends itself to building structured ways both connect to each other. We build on the incidence structure and matrix product formalism from the previous chapter, introducing various ways to build graphs as incidence structures that have direct representations as matrices. This can be extended to representing occurrences as matrices of hyperedge vectors. This view allows us to interpret different representations of graphs (or hypergraphs) as connected by simple matrix operations.\nTraditionally[1], [2], we might say a graph on nodes (or, “vertices”) \\(v\\in V=\\{1,\\cdots,n\\}\\) and “edges” \\(E\\) is a tuple: \\[\nG=(V,E) \\quad \\textrm{s.t.} \\quad E\\subseteq V \\times V\n\\]\nThe adjacency matrix \\(A\\) of \\(G\\), degree matrix \\(D\\), and graph/discrete Laplacian \\(L\\) are then defined as:1 \\[\n\\begin{aligned}\nA(u,v) & =\\mathbf{1}_E((u,v)) \\quad &A : V\\times V\\rightarrow \\mathbb{B} \\\\\nD(u,v) & =\\mathrm{diag}({\\textstyle\\sum}_V A(u,v))\\quad &D : V\\times V\\rightarrow \\mathbb{N} \\\\\nL(u,v) & = D(u,v) - A(u,v) \\quad &L : V\\times V\\rightarrow \\mathbb{Z}\n\\end{aligned}\n\\]\nHowever, if edges and their recovery is so important to us, defining them explicitly as pairs of nodes can be problematic when we wish to estimate their existence (or not) when pairs of nodes co-occur. Additionally, we have to be very careful to distinguish whether our graph is (un)directed, weighted, simple, etc., and hope that the edge set has been filtered to a subset of \\(N\\times N\\) for each case. Instead, we propose a less ambiguous framework for vectorizing graphs, based on their underlying incidence structure.\nInstead, we give edges their own set of identifiers, \\(e\\in E=\\{1,\\cdots \\omega\\}\\). Now, define graphs as incidence structures between nodes and edges such that every edge is incident to either two nodes, or none:\n\\[\nG = (V,E,\\mathcal{I}) \\quad s.t. \\quad \\mathcal{I} \\subseteq E\\times V\n\\tag{3.1}\\]\nVariations on graphs can often be conveniently defined as constraints on \\(\\mathcal{I}\\):\nTogether, these constraints define “simple” graphs. Similarly, we can equip Equation 3.1 with a function \\(B\\) that allows \\(\\mathcal{I}\\) to encode information about the specific kinds of incidence relations under discussion. We give \\(B\\) a range of possible flag values \\(S\\):\n\\[\nG = (V,E,\\mathcal{I},B) \\quad s.t. \\quad \\mathcal{I} \\subseteq E\\times V\\quad B:\\mathcal{I}\\rightarrow S\n\\tag{3.2}\\]\nIf the “absence” of incidence needs to be modeled explicitly, a “null” stand-in (0,False) can be added to each \\(S\\), which is useful for representing each structure as arrays for use with linear algebra (i.e. \\(\\{0,1\\}\\),\\(\\{-1,0,-1\\}\\),\\(\\mathbb{R}^+_0\\), and \\(\\mathbb{R}\\), respectively). By doing so, we can also place an exact limit on the maximum possible size of \\(\\omega=\\|E\\|\\) in the simple graph case, and indicate edges by their unique ID, such that \\(\\mathcal{I}= E\\times V\\) is no longer a subset relation for \\(E=\\{1,\\cdots,{n\\choose2} \\}\\). Instead of existence in \\(\\mathcal{I}\\), we explicitly use incidence relation \\(S\\) to tell us whether each possible edge “exists” or not, simplifying our graph definition further4:\n\\[\n\\begin{aligned}\nG  = (V,E,B) \\quad s.t. \\quad & B : E\\times V \\rightarrow S\\\\\n& v \\in V = \\{1,\\cdots, n\\}\\quad \\\\\n& e \\in E = \\left\\{1,\\cdots, {n\\choose 2} \\right\\}\n\\end{aligned}\n\\tag{3.3}\\]",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Incidence through vector representation</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html#graphs-as-incidence-structures",
    "href": "content/part1/1-02-graph-obs.html#graphs-as-incidence-structures",
    "title": "Incidence through vector representation",
    "section": "",
    "text": "Self loops can be prohibited by only allowing unique flags for a given relation2\nMultigraphs are similarly described by whether we allow pairs of vertices to appear with more than one edge3\n\n2  never two flags with the same pair, i.e. \\(\\mathcal{I}\\) is a set, not a multiset.3  in the set of flags containing nodes \\(u\\) or \\(v\\), only one \\(e\\) may be incident to both of them.\n\n\nUndirected, unweighted graphs only need single elements: “incidence exists” i.e.\\(S=\\{1\\}\\)\nDirected graphs can use two elements e.g. a “spin” for \\(S=\\{-1,1\\}\\)\nWeighted, undirected graphs are supported on positive scalars e.g. \\(S=\\mathbb{R}^+\\)\nWeighted, directed graphs are supported on any scalar e.g. \\(S=\\mathbb{R}_{\\neq0}\\)\n\n\n4  if we allow multi-edges, then \n\nEmbedding in vector space\nThe representation of \\(B\\) in Equation 3.3 bears a remarkable similarity to our original description of design matrices in Equation 2.1. In fact, as a matrix, \\(B(e,v)\\) is called the incidence matrix: every row has two non-zero entries, with every column containing a number of non-zero entries equal to that corresponding node’s degree in \\(G\\). Traditionally, we use an oriented incidence matrix, such that each row has exactly one positive (non-zero) value, and one negative (non-zero) value.5 Even for undirected graphs, the selection of which entry is positive or negative is left to be ambiguous, since much of the math used later is symmetric w.r.t. direction6.\n5  In fact, this would make B^*(v,e) equivalent to a graphical matroid, another common formalism that generalizes graph-like structures to vector space representations.6 though not always!However, we can be more precise by selecting each row(edge) vector, and partitioning it into two: one for each non-zero column (node) that edge is incident to. This makes every incidence be embedded as standard basis vector \\(\\mathbf{e}\\) in the feature space of \\(B\\), scaled by the corresponding value of \\(S\\). Let \\(V_e\\) be the set of nodes with (non-zero) incidence to edge \\(e\\). Then the incidence vectors are:\n\\[\\delta_e(v) = B(e,v)\\mathbf{e}_v \\quad \\forall v\\in V_e\\] The unoriented incidence matrix is easily defined as having rows that are sums over the incidence vectors for each edge: \\(\\mathbf{b}^+_e = \\sum_{v\\in V_e} \\delta_e(v)\\)\nTo build the oriented incidence matrix, one could define undirected graphs as equivalent to multidigraphs, where each edge is really two directed edges, in opposing directions. This does allow the matrix \\(B\\) to have the correct range for its entries in this formalism (the directed graph range, \\(S=\\{-1,0,1\\}\\)), and the edge definition based on sums would hold. The resulting set of incidences would have twice the number of edges than our combinatoric limit for simple graphs, however. Plus, it would necessitate averaging of weights over different edge ID’s to be prior to use of inner products, and many other implementation difficulties.\nInstead we would like to allow for systematic differences between the incidence vectors, without ambiguity. But, now that we have removed the information on “which nodes an edge connects” from our definition (since every edge is a scalar ID), how do we construct \\(V_e\\) without a costly search over all incidences?\nBecause of our unique identification of edges up to the combinatoric limit, we can still actually provide a unique ordering of the nodes it each edge connects. Using an identity from [3], we have a closed-form equation both to retrieve the IDs of nodes \\(u,v\\) (given an edge \\(e\\)), and an edge \\(e\\) (given two nodes \\(u,v\\)), for any simple graph with \\(n\\) vertices: \\[\n\\begin{aligned}\n    u_n(e) & = n - 2 - \\left\\lfloor\\frac{\\sqrt{-8e + 4n(n - 1) - 7}-1}{2}\\right\\rfloor\\\\\n    v_n(e) & = e + u_n(e) + 1 -\\frac{1}{2} \\left(n (n - 1) + (n - u_n(e))^2 - n+u_n(e)\\right)\\\\\n    e_n(u,v) & = \\frac{1}{2}\\left(n(n - 1) - ((n - u)^2 - n+u)\\right) + v - u - 1\n\\end{aligned}\n\\tag{3.4}\\]\n\n[3] M. Angeletti, J.-M. Bonny, and J. Koko, “Parallel Euclidean distance matrix computation on big datasets,” 2019. Available: https://hal.science/hal-02047514\nUsing this, we can unambiguously define a partition between incidences on \\(e\\), without needing other metadata or flags to distinguish \\(u\\) from \\(v\\), since those are defined through Equation 3.4 unambiguously w.r.t. \\(e\\). \\[\\mathbf{b}_e = \\delta_e(u)-\\delta_e(v)\\]\nThis could be called an “edge-centric” view of simple graphs, since the nodes involved in a graph are now actually derived w.r.t. the edges and our mapping \\(B\\), uniquely.\n\n\n\nInner products on \\(B\\)\nTODO\nEquation 2.3\nLaplacian as inner product on incidence observations. Associated objects (degree vector, o)\nRescaling to achieve normaalization.\nUse to define kernels (and application e.g. soft-cosine measure)\n…\n\n\nMetrological Considerations: Interaction Vectors\nTODO\nStrictly speaking, we can’t say that nodes are directly observed in this space… edges are. Collections of nodes are measured two-at-a-time (one-per-edge being traversed).\nAnother way to approach is to view inner products as a sum of outer products. A each edge uniquely corresponds to 2 nodes (in a simple graph). Use triangle unfolding for closed form bijection.\nUnrolling 3D tensor of subgraphs along eads to a secondary representation of graphs as an edgelist, having binary activation vectors on edges rather than nodes. Then each observation in this model is necessarily a set of activated edges. The non-zero (visited) nodes are found using the incidence matrix as an operator.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Incidence through vector representation</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html#graphs-and-node-occurrences",
    "href": "content/part1/1-02-graph-obs.html#graphs-and-node-occurrences",
    "title": "Incidence through vector representation",
    "section": "Graphs and node occurrences",
    "text": "Graphs and node occurrences\nTODO\n\n\n\nHyperedge Relation Observational Model\n\n\n\nHyperedges as vectors of node occurrence\n\n\n\n\n\n\n\n\n\\[\n%X(\\{1,2,3,4,\\cdots\\})=\\\\\n\\begin{array}{c c}\n& \\begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\\\ \\end{array} \\\\\n\\begin{array}{c c } x_1\\\\x_2\\\\x_3\\\\x_4\\\\ \\vdots \\end{array} &\n\\left[\n\\begin{array}{c c c c c c c c c c}\n  0  &  0  &  1  &  0  &  1  &  0  &  1  &  1  &  0  &  0 \\\\\n  1  &  0  &  0  &  0  &  1  &  1  &  0  &  0  &  0  &  0 \\\\\n  0  &  1  &  0  &  0  &  0  &  1  &  0  &  0  &  1  &  1 \\\\\n  0  &  0  &  0  &  1  &  1  &  0  &  0  &  1  &  0  &  0 \\\\\n  &&&& \\vdots &&&&&\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Bipartite representation of node “activation” data\n\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\nInner product on Hyperedges\nRoundabout way of describing binary/occurrence data. Inner product is co-occurrences.\nLeads to correlation/covariance, etc.\n\n\nCombining Occurrence & Dependence\n\nsoft cosine\nkernels on graphs (incl. coscia euclidean)\nRetrieving one from the other is hard.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Incidence through vector representation</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html",
    "href": "content/part1/1-03-recovery-road.html",
    "title": "Roads to Network Recovery",
    "section": "",
    "text": "Choosing a structure recovery method",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#choosing-a-structure-recovery-method",
    "href": "content/part1/1-03-recovery-road.html#choosing-a-structure-recovery-method",
    "title": "Roads to Network Recovery",
    "section": "",
    "text": "Takeaway: a way to organize existing algorithms, AND highlight unique set of problems we set out to solve",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#organizing-recovery-methods",
    "href": "content/part1/1-03-recovery-road.html#organizing-recovery-methods",
    "title": "Roads to Network Recovery",
    "section": "Organizing Recovery Methods",
    "text": "Organizing Recovery Methods\ni.e. Network Recovery as an Inverse Problem, and what information is had at each point.\n\n\n\nRelating Graphs and Hypergraph/bipartite structures as adjoint operators\n\n\n\nObserving Nodes vs Edges\n\n\nEmbeddings, Inner Products, & Preprocessing",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#tracing-information-loss-paths",
    "href": "content/part1/1-03-recovery-road.html#tracing-information-loss-paths",
    "title": "Roads to Network Recovery",
    "section": "Tracing Information Loss Paths",
    "text": "Tracing Information Loss Paths\n\nTable of Existing Approaches\n\nObservation-level loss (starting with the inner product or kernel)\nNon-generative model loss (no projection of data into model space)\nno uncertainty quantification\n\n\n\nA Path Forward\nSorting algorithms… none address all three!\ni.e. MOTIVATES FOREST PURSUIT",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html",
    "href": "content/part2/2-01-rand-sf.html",
    "title": "Desire Paths and Spanning Forests",
    "section": "",
    "text": "The Gambit of the Inner Product\nAddressing gaps discussed in the previous section to reach a generative model for network recovery requires careful attention to the generation mechanism for node activations. While there are many ways we might imagine bipartite data to be be generated, presuming the existence of a dependency graph that causes activation patterns will give us useful ways to narrow down the generative specification.\nFirst, we will investigate the common assumption that pairwise co-occurrences can serve as proxies for measuring relatedness, and how this “gambit of the group” is, in fact, a strong bias toward dense, clique-filled recovered networks. Because we wish to model our node activations as being caused by other nodes (that they depend on), we draw a connection to a class of models for spreading, or, diffusive processes. We outline how random-walks are related to these diffusive models of graph traversal, enabled by an investigation of the graph’s “regularized laplacian” from [1]. Then we use the implicit causal dependency tree structure of each observation, together with the Matrix Forest Theorem [2], [3] to more generally define our generative node activation model. This leads to a generative model for binary activation data as rooted random spanning forests on the underlying dependency graph.\nAs we saw repeatedly in Roads to Network Recovery, networks are regularly assumed to arise from co-occurrences, whether directly as counts or weighted in some way. This assumption can be a significant a source of bias in the measurement of edges. Why a flat count of co-occurrence leads to “hairballs” and bias for dense clusters can be related to the use of inner products on node activation vectors.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Desire Paths and Spanning Forests</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html#the-gambit-of-the-inner-product",
    "href": "content/part2/2-01-rand-sf.html#the-gambit-of-the-inner-product",
    "title": "Desire Paths and Spanning Forests",
    "section": "",
    "text": "Gambit of the Group\nIt seems reasonable, when interactions are unobserved, to assume some evidence for all possible interactions is implied by co-occurrence. However, similar to the use of uniform priors in other types of inference, if we don’t have a good reason to employ a fully-connected co-occurrence prior on interaction dependencies, we are adding systematic bias to our inference. Whether co-occurrence observations can be used to infer interaction networks directly was discussed at length in [4], where Whitehead and Dufault call this the gambit of the group.\n\n[4] H. Whitehead and S. Dufault, “Techniques for analyzing vertebrate social structure using identified individuals: Review and recommendations,” Advances in the Study of Behavior, vol. 28, no. 28, pp. 33–74, 1999.\n\nSo, consiously or unconsciously, many ethnologists studying social organization makr what might be called the “gambit of the group”: the assume that animals which are clustered […] are interacting with one another and then use membership of the same cluster […] to define association.\n\nThis work was rediscovered in the context of measuring assortativity for social networks,1 where the author of [5] advises that “group-based methods” can introduce sampling bias to the calculation of assortativity, namely, systematic overestimation when the sample count is low.\n1 Assortativity is, roughly, the correlation between node degree and the degrees of its neighbors.\n[5] D. N. Fisher, M. J. Silk, and D. W. Franks, “The perceived assortativity of social networks: Methodological problems and solutions,” in Trends in social network analysis, Springer International Publishing, 2017, pp. 1–19. doi: 10.1007/978-3-319-53420-6_1.\n\n[6] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\nThe general problems with failing to specify a model of what “edges” actually are get analyzed in more depth in [6]. They include a warning not to naively use correlational measures with a threshold, since even simple 3-node systems will easily yield false positives edges. Still, it would be helpful for practitioners to have a more explicit mental model of why co-occurence-based models yield systematic bias,\n\n\nInner Products as Sums of Cliques\nUnderlying correlation and co-occurrence models for edge strength is a reliance on inner products between node occurrence vectors. They all use gram matrices (or centered/scaled versions of them), which were brought up in Proximity vs. Incidence. The matrix multiplication performed represents inner products between all pairs of feature vectors. For \\(X(i,j)\\in\\mathbb{B}\\), these inner products sum together the times in each observation that two nodes were activated together.\nHowever, another (equivalent) way to view matrix multiplication is as a sum of outer products \\[ G(i,j) = X^T X = \\sum_{i=1}^k X(i,k)X(j,k)= \\sum_{i=1}^k \\mathbf{x}_k\\mathbf{x}_k^T \\] Those outer products of binary vectors create \\(m\\times m\\) matrices that have a 1 in every \\(i,j\\) entry where nodes \\(i,j\\) both occurred—i.e., they can be seen as adjacency matrices of the clique on nodes activated in the \\(k\\)th observation In this sense, any method that involves transforming or re-weighting a gram matrix, is implicitly believing that the \\(k\\)th observation was a complete graph. This is illustrated in Figure 5.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Edge Measurements with Group Gambit (BoW) assumption\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Bipartite representation of node “activation” data\n\n\n\n\n\n\n\n\n\nFigure 5.1: Inner-product projections imply observations of complete graphs, summed.\n\n\n\nFor many modeling scenarios, this paradigm allows practitioners to make a more straight-forward intuition-check: do clique observations make sense here? When a list of authors for a paper is finished, does that imply that all authors mutually interacted with all others directly to equally arrive at the decision to publish? This would be similar to assuming the authors might simultaneously enter a room, look at a number of others (who all look exclusively at each other, as well), and at once decide to publish together. In our introduction, we described a more likely scenario we could expect from an observer on the ground: a researcher asks a colleague or two to collaborate, who might know a couple more with relevant expertise, and so on.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Desire Paths and Spanning Forests</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html#networks-as-desire-path-density-estimates",
    "href": "content/part2/2-01-rand-sf.html#networks-as-desire-path-density-estimates",
    "title": "Desire Paths and Spanning Forests",
    "section": "Networks as Desire Path Density Estimates",
    "text": "Networks as Desire Path Density Estimates\nUnfortunately, methods based on inner-product thresholding are still incredibly common, in no small part due to how easy it is to create them from occurrence data. What we need is a way to retain the ease of use of inner-product network creation, with a more domain-appropriate graph at the observation level. Of course there are many classes of graphs we might believe local interactions occur on: path-graphs, trees, or any number of graphs that reflect the topolgy or mechanism of local interactions in our domain of interest. Authors have proposed classes of graphs that mirror human perception of set shapes [RELATIVE NEIGHBORHOOD]2, or graphs whose modeled dependencies are strictly planar [planar maximally filtered graps]3. Alternatively, the interactions might be scale free, small-world, or samples from stochastic block models.[CITE]\n2  e.g. for dependencies based on perception, such as human decision making tendencies, or causes based on color names.3  e.g. when interactions are limited to planar dependencies, like inferring ancient geographic borders.4 Assuming we can even find one, something we must return to shortlyIn any case, these assumptions provide an explicit way to describe the set of possible interaction graphs we believe our individual observations are sampled from. In other words, we limit our possible interaction graph to a set of all possible graphs in our class \\(\\mathcal{C}\\), and model the interactions allowed to be inferred from the activated nodes as \\(c\\in\\mathcal{C}\\). With an associated probability measure \\(\\mu_{\\mathcal{C}}(c)\\) defined on subgraphs of the complete graph on our node set V4, we are able to say our interactions are sampled from a distribution over graphs in the class.\nFor simplicity, since unreported/hidden-but-activated nodes is outide the scope of our work, we narrow the distribution to be only on the induced subgraph \\(C(S_k) \\in G[S_k])\\), where \\(S_k\\) is the set of activated nodes in \\(\\mathbf{x}_k\\). More specifically, we say that an observation of \\(S\\subset V=\\{s_1,\\cdots, s_t\\},\\quad t=|S|\\) activated nodes implies a distribution over edge vectors we could hav observed, determined by the chosen class,\n\\[\\mathbf{x}^E_k \\in \\]\nOnce an analyst has provided epistemic justification for a class of graphs to model We propose that the computationally-efficient inner-product networks can still be used, but could be made far more effective by counting edge observation counts with something more appropriate than cliques.\nThe class of diffusive processes we focus on “spread” from one node to another. If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes “depend” on others to be activated. In this case, a node activates when another node it depends on spreads their state to it. These single-cause activations are equivalent to imagining a random-walk on the dependency graph, where visiting a node activates it.\n\nRandom Walk Activations\nRandom walks are regularly employed to model spreading and diffusive processes on networks. If a network consists of locations, states, agents, etc. as “nodes”, and relationships between nodes as “edges”, then random walks consist of a stochastic process that “visits” nodes by randomly “walking” between them along connecting edges. Epidemiological models, cognitive search in semantic networks, stock price influences, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. [CITE…lots?]\nWhen network structure is known, the dynamics of random-walks are used to capture the network structure via sampling [LITTLEBALLOFFUR, etc], estimate node importance’s[PAGERANK], or predict phase-changes in node states (e.g. infected vs. uninfected)[SIR I think] In our case, Since we have been encoding the activations as binary activation vectors, the “jump” information is lost—activations are “emitted” for observation only upon the random walker’s initial visit. [CITE INVITE] In many cases, however, the existence of relationships is not known already, and analysts might assume their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.\n\nuseful tool for analysis of our data: reg laplacian\ninterpretations\n\n\n\nDependencies as Trees\nThe whole graph isn’t a tree….Every data point is.\n\n\n\n\n\n\n\n\n\nFigure 5.2: Edge Measurements with true (tree) dependencies known\n\n\n\n\n\n\n[GRAPHIC 1 - my data]\n[GRAPHIC 2 - infection vector from meta node]\n\n\nMatrix Tree and Forest Theorems\n\none from kirchoff\none from Chebotarv",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Desire Paths and Spanning Forests</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html#generative-model-specification",
    "href": "content/part2/2-01-rand-sf.html#generative-model-specification",
    "title": "Desire Paths and Spanning Forests",
    "section": "Generative Model Specification",
    "text": "Generative Model Specification\n - hierarchical model - marginalize over the root node.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Desire Paths and Spanning Forests</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html",
    "href": "content/part2/2-02-forest-pursuit.html",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "",
    "text": "Sparse Dictionary Learning",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#sparse-dictionary-learning",
    "href": "content/part2/2-02-forest-pursuit.html#sparse-dictionary-learning",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "",
    "text": "Problem Specification\n\n\nMatching Pursuit\n\n\nSpace of Spanning Forests",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#forest-pursuit-approximate-recovery-in-near-linear-time",
    "href": "content/part2/2-02-forest-pursuit.html#forest-pursuit-approximate-recovery-in-near-linear-time",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "text": "Forest Pursuit: Approximate Recovery in Near-linear Time\nI.e. the PLOS paper (modified basis-pursuit via MSTs) ### Algorithm Summary\n\nUncertainty Estimation\n\n\nApproximate Complexity",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#sec-FP-experiments",
    "href": "content/part2/2-02-forest-pursuit.html#sec-FP-experiments",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Simulation Study",
    "text": "Simulation Study\n\nMethod\n\n\nResults - Scoring\n\n\n\n\n\n\n\n\n\nFigure 6.1: Comparison of MENDR recovery scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Comparison of MENDR Recovery Scores by Graph Type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Partial Residuals (regression on E[MCC])\n\n\n\n\n\n\n\n\nResults - Performance\n\n\n\n\n\n\n\n\n\nFigure 6.4: Runtime Scaling (Forest-Pursuit vs GLASSO)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.5: Partial Residuals (regression on computation time)",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#discussion",
    "href": "content/part2/2-02-forest-pursuit.html#discussion",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Discussion",
    "text": "Discussion\n\nInteraction Probability",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html",
    "href": "content/part2/2-03-latent-forest-alloc.html",
    "title": "LFA: Latent Forest Allocation",
    "section": "",
    "text": "Radom Spanning Trees",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#radom-spanning-trees",
    "href": "content/part2/2-03-latent-forest-alloc.html#radom-spanning-trees",
    "title": "LFA: Latent Forest Allocation",
    "section": "",
    "text": "Methods for sampling i.e. wilson’s and Duan’s (other? Energy paper?)\nTree Likelihoods, other facts",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#bayesian-estimation-by-gibbs-sampling",
    "href": "content/part2/2-03-latent-forest-alloc.html#bayesian-estimation-by-gibbs-sampling",
    "title": "LFA: Latent Forest Allocation",
    "section": "Bayesian Estimation by Gibbs Sampling",
    "text": "Bayesian Estimation by Gibbs Sampling\n\ncomparison with LDA\nSimplifying Assumptions (conditional prob IS prob for this)\n\nI.e. the unwritten paper, modifying technique by [1] for RSF instead of RSTs\n\n[1] L. L. Duan and D. B. Dunson, “Bayesian spanning tree: Estimating the backbone of the dependence graph,” arXiv, arXiv:2106.16120, Jun. 2021. doi: 10.48550/arXiv.2106.16120.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#simulation-study",
    "href": "content/part2/2-03-latent-forest-alloc.html#simulation-study",
    "title": "LFA: Latent Forest Allocation",
    "section": "Simulation Study",
    "text": "Simulation Study\n\nScore Improvement\n\n\n\n\n\n\n\n\n\nFigure 7.1: Change in Expected MCC (EFM vs FP)\n\n\n\n\n\n\n\n\nOdds of Individual Edge Improvement\n\n\n\n\n\n\n\n\n\nFigure 7.2: Logistic Regression Coef. (EFM - FP) vs. (Ground Truth)",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part3/3-06-qualitative.html",
    "href": "content/part3/3-06-qualitative.html",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "",
    "text": "Network Science Collaboration Network",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-06-qualitative.html#network-science-collaboration-network",
    "href": "content/part3/3-06-qualitative.html#network-science-collaboration-network",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "",
    "text": "Figure 8.1: 134 Network scientists from [NEWMAN;BOCCALETTI;SNEPPEN], connected by co-authorship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Max. likelihood tree dependency structure to explain co-authorships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Forest Pursuit estimate of NetSci collaborator dependency relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-06-qualitative.html#les-miserables-character-network",
    "href": "content/part3/3-06-qualitative.html#les-miserables-character-network",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "Les Miserables Character Network",
    "text": "Les Miserables Character Network\n\nBackboning\n\n\n\n\n\n\n\n\n\nFigure 8.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6\n\n\n\n\n\n\n\n\nCharacter Importance Estimation\n\n\n\n\n\n\n\n\n\nFigure 8.7",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-ordered.html",
    "href": "content/part3/3-07-ordered.html",
    "title": "Recovery from Partial Orders",
    "section": "",
    "text": "Technical Language Processing\nLike before, but with the added twist of knowing our nodes were activated with a particular partial order.",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-ordered.html#technical-language-processing",
    "href": "content/part3/3-07-ordered.html#technical-language-processing",
    "title": "Recovery from Partial Orders",
    "section": "",
    "text": "insert from [1], [2]\n[1] R. Sexton and M. Fuge, “Organizing tagged knowledge: Similarity measures and semantic fluency in structure mining,” Journal of Mechanical Design, vol. 142, no. 3, Jan. 2020, doi: 10.1115/1.4045686.\n\n[2] R. Sexton and M. Fuge, “Using semantic fluency models improves network reconstruction accuracy of tacit engineering knowledge,” in Volume 2A: 45th design automation conference, American Society of Mechanical Engineers, Aug. 2019. doi: 10.1115/detc2019-98429.",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-ordered.html#verbal-fluency-animal-network",
    "href": "content/part3/3-07-ordered.html#verbal-fluency-animal-network",
    "title": "Recovery from Partial Orders",
    "section": "Verbal Fluency Animal Network",
    "text": "Verbal Fluency Animal Network\n\nEdge Connective Effiency and Diversity\n\n\n\n\n\n\n\n\n\nFigure 9.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.4: Comparison of backboning/dependency recovery methods tested vs. Forest Pursuit\n\n\n\n\n\n\n\n\nThresholded Structure Preservation\n\n\nDifferences in structural preservation with increased thresholding.\n\n\n\n\n\n\n\n\n\n\n(a) co-occurrence methods will retain local communities at the cost of global structure\n\n\n\n\n\n\n\n\n\n\n\n(b) dependency network drops rarer nodes from the preserved central structure at higher uncetainty cutoffs\n\n\n\n\n\n\nFigure 9.5: When only retaining the top 2% of edge strengths, blah\n\n\n\n\n\n\n\n\nForest Pursuit as Preprocessing\n\n\nDifferences in structural preservation with increased thresholding.\nRetaining the top 2% of edges, co-occurrence retains local communities\nat the cost of global structure.\n\n\n\n\n\n\n\n\n\n\n(a) Islands of local structure remain (doubly-stochastic)\n\n\n\n\n\n\n\n\n\n\n\n(b) Intact global structure with isolates\n\n\n\n\n\n\nFigure 9.6: We might prefer to drop low-certainty/rare nodes from a preserved central structure.",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Partial Orders</span>"
    ]
  }
]