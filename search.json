[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Measuring Network Dependencies from Node Activations",
    "section": "",
    "text": "Preface\n\n\nForeward\n\n\nAcknowledgements",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "content/0-intro.html",
    "href": "content/0-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ambiguous Metrology\nA wide variety of fields show consistent interest in inferring latent network structure from observed interactions, from human cognition and social infection networks, to marketing, traffic, finance, and many others. [1] However, an increasing number of authors are noting a lack of agreement in how to approach the metrology of this problem. This includes rampant disconnects between the theoretical and methodological network analysis sub-communities[2], treatment of error as purely aleatory, rather than epistemic [3], or simply ignoring measurement error in network reconstruction entirely[4].\nNetworks in the “wild” rarely exist of and by themsleves. Rather, they are a model of interaction or relation between things that were observed. One of the most beloved examples of a network, the famed Zachary’s Karate Club[5], is in fact reported as a list of pairwise interactions: every time a club member interacted with another (outside of the club), Zachary recorded it as two integers (the IDs of the members). The final list of pairs can be interpreted as an “edge list”, which can be modeled with a network: a simple graph. This was famously used to show natural community structure that nicely matches the group separation that eventually took place when the club split into two.[6]\nNote, however, that we could have just as easily taken note of the instigating student for each interaction (i.e. which student initiated conversation, or invited the other to socialize, etc.). If that relational asymmetry is available, our “edges” are now directed, and we might be able to ask questions about the rates that certain students are asked vs. do the asking, and what that implies about group cohesion. Additionally, the time span is assumed to be “for the duration of observation” (did the students ever interact), but if observation time was significantly longer, say, multiple years, we might question the credulity of treating a social interaction 2 years ago as equally important to an interaction immediately preceding the split. This is now a “dynamic” graph; or, if we only measure relative to the time of separation, at the very least a “weighted” one.\nWe do not know if any of these are true. In fact, as illustrated in Figure 1.1, we do not know if the network being described from the original edge data even has 77 or 78 edges, due to ambiguous reporting in the original work. Lacking a precise definition of what the graph’s components (i.e. it’s edges) are, as measurable entities, means we cannot estimate the measurement error in the graph.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/0-intro.html#ambiguous-metrology",
    "href": "content/0-intro.html#ambiguous-metrology",
    "title": "Introduction",
    "section": "",
    "text": "[5] W. W. Zachary, “An information flow model for conflict and fission in small groups,” Journal of Anthropological Research, vol. 33, no. 4, pp. 452–473, Dec. 1977, doi: 10.1086/jar.33.4.3629752.\n\n[6] M. Girvan and M. E. J. Newman, “Community structure in social and biological networks,” Proceedings of the National Academy of Sciences, vol. 99, no. 12, pp. 7821–7826, Jun. 2002, doi: 10.1073/pnas.122653799.\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Zachary’s Karate Club, with ambiguously extant edge 78 highlighted.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/0-intro.html#indirect-network-measurement",
    "href": "content/0-intro.html#indirect-network-measurement",
    "title": "Introduction",
    "section": "Indirect Network Measurement",
    "text": "Indirect Network Measurement\nWhile the karate club graph has unquantified edge uncertainty derived from ambiguous edge measurements, we are fortunate that we have edge measurements. Regardless of how the data was collected, it is de facto reported as a list of pairs. In many cases, we simply do not have such luxury. Instead, our edges are only measured indirectly, and instead we are left with lists of node co-ocurrences. Networks connecting movies as being “similar” might be derived from data that lists sets of movies watched by each user; networks of disease spread pathways might be implied from patient infection records; famously, we might build a network of collaboration strength between academic authors by mining datasets of the papers they co-author together.\nSuch networks are derived from what we will call node activation data, i.e., records of what entities happened “together”, whether contemporaneously, or in some other context or artifact.\n\n\n\n\n\n\n\n\n\nFigure 1.2\n\n\n\n\n\n\nThese are naturally represented as “bipartite” networks, having separate entites for, say, “papers” and “authors”, and connecting them with edges (paper 1 is “connected” to its authors E,H,C, etc.). But analysts are typically seeking the collaboration network connecting authors (or papers) themselves! Networks of relationships in this situation are not directly observed, but which if recovered could provide estimates for community structure, importances of individual authors (e.g. as controlling flow of information), and the “distances” that separate authors from each other, in their respective domains. [7] Common practice assumes that co-authorship in any paper is sufficient evidence of at least some level of social “acquaintance”, where more papers shared means more “connected”.\n\n[7] M. E. J. Newman, “Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality,” Physical Review E, vol. 64, no. 1, p. 016132, Jun. 2001, doi: 10.1103/physreve.64.016132.\n\n\n\n\n\n\n\n\n\nFigure 1.3\n\n\n\n\n\n\nThus our social collaboration network is borne out of indirect measurements: author connection is implied through “occasions when co-authorship occurred”. However, authors of papers may recall times that others were added, not by their choice, but by someone else already involved. In fact, the final author list of most papers is reasonably a result of individuals choosing to invite others, not a unanimous, simultaneous decision by all members. Let’s imagine we wished to study the social network of collaboration more directly: if we had the luxury of being in situ as, say, a sociologist performing an academic ethnography, we might have been more strict with our definition of “connection”. If the goal is a meaningful social network reflecting the strength of interraction between colleages, perhaps the we prefer our edges represent “mutual willingness to collaborate”. Edge “measurement”, then, could involve records of events that show willingness to seek or participate in collaboration event, such as:\n\nauthor (g) asked (e), (h), and (c) to co-author a paper, all of whom agreed\n(i) asked (f) and (j), but (j) wanted to add (b)’s expertise before writing one of the sections\n\nand so on. Each time two colleagues had an opportunity to work together and it was seized upon we might conclude that evidence of their relationship strengthed. With data like this, we could be more confident in claiming our collaboration network can serve as “ground truth,” as far as empirically confirmed collaborations go. However, even if the underlying “activations” are identical, our new, directly measured graph looks very different.\n\n\n\n\n\n\n\n\n\nFigure 1.4: graph of mutual collaboration relationships i.e. the “ground truth” social network\n\n\n\n\n\n\nFundamentally, the network in Figure 1.4 shows which relationships the authors depend on to accomplish their publishing activity. When causal relations between nodes are being modeled as edges, we call such a graph a dependency network. We will investigate this idea further later on, but ultimately, if a network of dependencies is desired (or implied, based on analysis needs), then the critical problem remaining is how do we recover dependency networks from node activations? Additionally, what goes wrong when we use co-occurence/activation data to estimate the dependency network, especially when we wish to use it for metrics like centrality, shortest path distances, and community belonging?\n\n\n\n\n\n\n\n\n\nFigure 1.5: Recovering underlying dependency networks from node-cooccurrences.\n\n\n\n\n\n\nEven more practically, networks created directly from bipartite-style data are notorious for quickly becoming far too dense for useful analysis, earning them the (not-so-)loving moniker “hairballs”. Network “backboning,” as it has come to be called tries to find a subset of edges in this hairball that still captures it’s core topology in a way that’s easier to visualize.[8], [9] Meanwhile, underlying networks of dependencies that cause node activation patterns can provide this: they are almost always more sparse than their hairballs. Accessing the dependency backbone in a principled way is difficult, but doing so in a rapid, scalable manner is critical for practitioners to be able to make use of it to trim their hairballs.\n\n[8] P. B. Slater, “A two-stage algorithm for extracting the multiscale backbone of complex weighted networks,” Proceedings of the National Academy of Sciences, vol. 106, no. 26, pp. E66–E66, Jun. 2009, doi: 10.1073/pnas.0904725106.\n\n[9] Z. Neal, “The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors,” Social Networks, vol. 39, pp. 84–97, Oct. 2014, doi: 10.1016/j.socnet.2014.06.001.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/0-intro.html#scope-of-this-work",
    "href": "content/0-intro.html#scope-of-this-work",
    "title": "Introduction",
    "section": "Scope of this work",
    "text": "Scope of this work\nThe purpose of this thesis is to provide a solid foundation for basic edge metrology when our data consists of binary node activations, by framing network analysis as a problem of inference, as suggested by [2]. We give special focus to binary activations that occur due to spreading processes, such as random walks or cascades on an underlying carrier graph. Recovering the carrier, or, “dependency” network from node activations is of great interest to the network backboning and causal modeling communities, but often involves either unspoken sources of epistemic and aleatory error, or high computation costs (or both). To begin addressing these issues, we present a guide to current practices, pitfalls, and how common statistical tools apply to the network recovery problem: a Practitioner’s Guide to Network Recovery. We cover what “measurement” means in our context, and specifically the ways we encode observations, operations, and uncertainties numerically. Clarifying what different versions of what “relation” means (whether proximity or incidence) is critical, since network structure is intended to encode such relations as mathematical objects, despite common ambiguities and confusion around what practitioners intend on communicating through them. Then we use this structure to present a cohesive framework for selecting a useful network recovery technique, based on the available data and where in the data processing pipeline is acceptable to admit either extra modeling assumptions or information loss.\n\n[2] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\nNext, building on a gap found in the first part, we present a novel method, Forest Pursuit, to extract dependency networks when we know a spreading process causes node activation (e.g. paper co-authorship caused by collaboration requests). We create a new reference dataset to enable community benchmarking of network recovery techniques, and use it show greatly improved accuracy over many other widely-used methods. Forest Pursuit in its simplest form scales linearly with the size of active-node sets, being trivially parallelizable and streamable over dataset size, and agnostic to network size overall. We then expand our analysis to re-imagine Forest Pursuit as a Bayesian probabilistic model, Latent Forest Allocation, which has an easily-implemented Expectation Maximization scheme for posterior estimation. This significantly improves upon the accuracy results of Forest Pursuit, at the cost of some speed and scalability, giving analysts multiple options to adapt to their needs.\nLast, we apply Forest Pursuit to several qualitative case-studies, including a scientific collaboration network, and the verbal fluency “animals” network recovery problem, which dramatically change interpretation under use of our method. We investigate its use as a low-cost preprocessor for other methods of network recovery,like GLASSO, improving their stability and interpretability. Finally we discuss the special case when node activations are reported as an ordered set, where accounting for cascade-like effects becomes crucial to balance false positive and false-negative edge prediction. Along with application of this idea to knowledge-graph creation from technical language in the form maintenance work-order data, we discuss more broadly the future needs of network recovery, specifically in the context of embeddings and gradient-based machine learning toolkits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html",
    "href": "content/part1/1-01-matrix-meas.html",
    "title": "Metrology as matrices",
    "section": "",
    "text": "Observation and feature “spaces”\nWhere metrology is concerned, the actual unit of observation and how it is encoded for us is critical to how analysts may proceed with quantifying, modeling, and measuring uncertainty around observed phenomena. Experiment and observation tends to be organized as inputs and outputs, or, independent variables and dependent variables, specifically. Independent variables are observed, multiple times (“observations”), and changes in outcome for each can be compared to the varying values associated with the independent variable input (“features”). For generality, say a practitioner records their measurements as scalar values, i.e. \\(x\\in\\mathbb{S}\\in\\{\\mathbb{R,Z,N},\\cdots\\}\\). The structure most often used to record scalar values of \\(n\\) independent/input variable features over the course of \\(m\\) observations is called a design matrix \\(X:\\mathbb{S}^{m\\times n}\\).1\nIf we index a set of observations and features, respectively, as \\[ i\\in I=\\{1,\\cdots,m\\}, \\quad j\\in J=\\{1,\\cdots,n\\},\\qquad I,J:\\mathbb{N}\\] then the design matrix can map the index of an observation and a feature to the corresponding measurement. \\[\nx=X(i,j)\\qquad X : I\\times J \\rightarrow \\mathbb{S}\n\\tag{2.1}\\] i.e. the measured value of the \\(j\\)th independent variable from the \\(i\\)th observation.2 In this scheme, an “observation” is a single row vector of features in \\(\\mathbb{S}^{n\\times 1}\\) (or simply \\(\\mathbb{S}^{n}\\)), such that each observation encodes a position in the space defined by the features, i.e. the feature space, and extracting a specific observation vector \\(i\\) from the entire matrix can be denoted as \\[\\mathbf{x}_i=X(i,\\cdot),\\quad \\mathbf{x}:J\\rightarrow\\mathbb{S}\\] Similarly, every “feature” is associated with a single column vector in \\(\\mathbb{S}^{1\\times m}\\), which can likewise be interpreted as a position in the space of observations (the data space): \\[\\mathbf{x}_j'=X(\\cdot,j),\\quad \\mathbf{x}':I\\rightarrow\\mathbb{S}\\] Note that this definition could be swapped without loss of generality. In other words, \\(\\mathbf{x}\\) and \\(\\mathbf{x}'\\) being in row and column spaces is somewhat arbitrary, having more to do with the logistics of experiment design and data collection. We could have measured our feature vectors one-at-a-time, measuring their values over an entire “population”, in effect treating that as the independent variable set.3\nTo illustrate this formalism in a relevant domain, let’s take another look at co-citation networks. For \\(m\\) papers we might be aware of \\(n\\) total authors. For a given paper, we are able to see which authors are involved, and we say those authors “activated” for that paper. It makes sense that our observations are individual papers, while the features might be the set of possible authors. However, we are not given information about which author was invited by which other one, or when each author signed on. In other words, the measured values are strictly boolean, and we can structure our dataset as a design matrix \\(X:\\mathbb{B}^{m\\times n}\\). We can then think of the \\(i^{\\mathrm{th}}\\) paper as being represented by a vector \\(\\mathbf{x}_i:\\mathbb{B}^n\\), and proceed using it in our various statistical models. If we desired to analyze the set of authors, say, in order to determine their relative neighborhoods or latent author communities, we could equally use the feature vectors for each paper, this time represented in a vector \\(\\mathbf{x}'_j:\\mathbb{B}^{1\\times m}\\).",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-matrix-notation",
    "href": "content/part1/1-01-matrix-meas.html#sec-matrix-notation",
    "title": "Metrology as matrices",
    "section": "",
    "text": "2  This notation is adapted from the sparse linear algebraic treatment of graphs in [1] and [2]. \n[1] J. Kepner and J. Gilbert, Graph algorithms in the language of linear algebra. Philadelphia: Society for Industrial; Applied Mathematics, 2011.\n\n[2] J. Kepner et al., “Mathematical foundations of the GraphBLAS,” in 2016 IEEE high performance extreme computing conference ( HPEC), Sep. 2016, pp. 1–9. doi: 10.1109/HPEC.2016.7761646.\n3  In fact, vectors are often said to be in the column-space of a matrix, especially when using them as transformations in physics or deep learning layers. We generally follow a one-observation-per-row rule, unless otherwise stated.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-lin-ops",
    "href": "content/part1/1-01-matrix-meas.html#sec-lin-ops",
    "title": "Metrology as matrices",
    "section": "Models & linear operators",
    "text": "Models & linear operators\nAnother powerful tool an analyst has is modeling the observation process. This is relevant when the observed data is hypothesized to be generated by a process we can represent mathematically, but we do not know the parameter values to best represent the observations (or the observations are “noisy” and we want to find a “best” parameters that account for this noise). This is applicable to much of scientific inquiry, though one common use-case is the de-blurring of observed images (or de-noising of signals), since we might have a model for how blurring “operated” on the original image to give us the blurred one. We call this “blurring” a linear operator if it can be represented as a matrix4, and applying it to a model with \\(l\\) parameters is called the forward map: \\[\\mathbf{x} = F\\mathbf{p}\\qquad F:\\mathbb{R}^{l}\\rightarrow \\mathbb{R}^n\\] where \\(P\\) is the space of possible parameter vectors, i.e. the model space. The forward map takes a modeled vector and predicts a location in data space.\n4 in the finite-dimensional caseOf critical importance, then, is our ability to recover some model parameters from our observed data, e.g. if our images were blurred through convolution with a blurring kernel, then we are interested in deconvolution. If \\(F\\) is invertible, the most direct solution might be to apply the operator to the data, as the adjoint map: \\[ \\mathbf{p} = F^H\\mathbf{x}\\qquad F^H:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^l\\] which removes the effect of \\(F\\) from the data \\(\\mathbf{x}\\) to recover the desired model \\(\\mathbf{p}\\).\nTrivially we might have an orthogonal matrix \\(F\\), so \\(F^H=F^{-1}\\) is available directly. In practice, other approaches are used to minimize the residual: \\(\\hat{\\mathbf{p}}^=\\min_{\\mathbf{p}} F\\mathbf{p}-\\mathbf{x}\\). Setting the gradient to 0 yields the normal equation, such that \\[ \\hat{\\mathbf{p}}=(F^TF)^{-1}F^T\\mathbf{x}\\] This should be familiar to readers as equivalent to solving ordinary least-squares (OLS). However, in that case it is more often shown as having the design matrix \\(X\\) in place of the operator \\(F\\).\nThis is a critical distinction to make: OLS as a “supervised” learning method treats some of the observed data we represented as a design matrix previously as a target to be modeled, \\(y=X(\\cdot,j)\\), and the rest maps parameters into data space, \\(F=X(\\cdot,J/j)\\). With this paradigm, only the target is being “modeled” and the rest of the data is used to create the operator. In the citation network example, it would be equivalent to trying to predict one variable, like citation count or a specific author’s participation in every paper, given every other author’s participation in them.\nFor simplicity, most work in the supervised setting treats the reduced data matrix as X, opting to treat \\(y\\) as a separate dependent variable. However, our setting will remain unsupervised, since no single target variable is of specific interest—all observations are “data”. In this, we more closely align with the deconvolution literature, such that we are seeking a model and an operation on it that will produce the observed behavior in an “optimal” way.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-smooth-err",
    "href": "content/part1/1-01-matrix-meas.html#sec-smooth-err",
    "title": "Metrology as matrices",
    "section": "Measurement quantification & error",
    "text": "Measurement quantification & error\nIn binary data, such as what we have been considering, it is common to model observables as so-called “Bernoulli trials”: events with two possible outcomes (on, off; yes, no; true, false), and one outcome has probability \\(p\\). These can be thought of as weighted coin-flips: “heads” with probability \\(p\\), and “tails” \\(1-p\\). If \\(k\\) trials are performed (the “exposure”), we say the number of successes \\(s\\) (the “count”) is distributed as a binomial distribution \\(s\\sim Bin(p,k)\\). The empirical estimate for the success probability is \\(\\hat{p}=\\tfrac{s}{k}\\).\nNote that this naturally resembles marginal sums on our design matrix \\(X\\), if we treat columns (or rows!) as an array of samples from independent Bernoulli trials: \\(\\hat{p}_j = \\frac{\\sum_{i\\in I} X(i,j)}{m}\\). Many probability estimates involving repeated measurements of binary variables (not simply the row/column variables) have this sort of \\(\\frac{\\textrm{count}}{\\textrm{exposure}}\\) structure, as will become useful in later sections.\nHowever, if we are “measuring” a probability, we run into issues when we need to quantify our uncertainty about it. For instance, an event might be quite rare, but if in our specific sample we never see it, we still do not generally accept a probability of zero.\n\nAdditive Smoothing\nOne approach to dealing with this involves adding pseudocounts that smooth out our estimates for count/exposure, from which we get the name “additive smoothing”.[CITE?] \\[\\hat{p} = \\frac{s+\\alpha}{k+2\\alpha} \\] Adding 1 success and 1 failure (\\(\\alpha=1\\)) as pseudocounts to our observations is called Laplace’s Rule of Succession, or simply “Laplace smoothing,”5 while adding \\(\\alpha=0.5\\) successes and failures is called using Jeffrey’s Prior. It’s so-called because this pseudocount turns out to be a special case of selecting a Beta prior on the Bernoulli probability \\(p\\sim \\textrm{Beta}(\\alpha, \\beta)\\), such that the posterior distribution for \\(p\\) after our observations is \\(\\textrm{Beta}(s+\\alpha, k-s+\\beta)\\), which has the mean: \\[\nE[p|s,k]=\\frac{s+\\alpha}{k-\\alpha+\\beta}\n\\tag{2.2}\\]\n5  derived when Laplace desired estimates of probability for unobserved phenomena, such as the sun (not) rising tomorrow.6  A useful comparison of the two priors (1, 0.5) is to ask, given all of the trials we have seen so far, whether we believe we are near the “end” or “middle” of an average run of trials. For \\(\\alpha=1\\), we believe nearly all evidence has been collected, but for \\(\\alpha=0.5\\), only half of expected evidence has been observed.\nThis exactly recovers additive smoothing with a Jeffrey’s prior for \\(\\alpha=\\beta=0.5\\).6 This generalization allows us to be more flexible, and specify our prior expectations on counts or exposure with more precision. Such models provide both an estimate of the aleatory uncertainty (via the posterior distribution), and a form of “shrinkage” that prevents sampling noise from unduly affecting parameter estimates (via the prior distribution). Despite being a simple foundation, this treatment of “counts” and “exposure” can be built upon in many ways.\n\n\nConditional Probabilities & Contingencies\nIn dependency/structure recovery, since our goal involves estimating (at least) pairwise relationships, the independence assumption required to estimate node occurrences as Beta-Binomial is clearly violated.7\n7  In fact, a recent method from [3] models probabilistic binary observations, with dependencies, by generalizing the mechanics overviewed here to a fully multivariate Bernoulli distribution, capable of including 3rd- and higher-order interractions, not just pairwise.\n\n[3] B. Dai, S. Ding, and G. Wahba, “Multivariate bernoulli distribution,” Bernoulli, vol. 19, no. 4, Sep. 2013, doi: 10.3150/12-bejsp10.\nHowever, it’s common to estimate how similar two random variables \\(A,B\\) are, e.g. if samples of each correspond to columns of binary \\(X\\). For instance, the joint probabilities \\(P(A\\cap B)\\) answer “how often does A happen with B, out of all data?” Conditional probabilities \\(P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\) measure how often A occures given B happened. Once again, we can estimate the base probabilities \\(P(A)\\) and \\(P(B)\\) with methods like Equation 2.2 for each marginal sums \\(X(\\cdot,A)\\) or \\(X(\\cdot,B)\\), but the joint and conditional probabilities can instead be estimated using matrix multiplication using the Gram matrix, discussed below. It encodes pair-wise co-occurrence counts, such that \\(G(i,i'):\\mathbb{Z}^{n\\times n}\\) has the co-occurrence count for node \\(i\\) with \\(i'\\).\nThe co-occurrence probability \\(P(A\\cap B)\\) for each pair can also be approximated with the beta-binomial scheme mentioned above, but care must be taken not to confuse this with the edge strength connecting two nodes. First, nodes that rarely activate (low node probability) may nonetheless reliably connect to others when they do occur (high edge probability). In fact, without direct observation of edges, we are not able to estimate their count, or their exposure, which can be a source of systemic error from epistemic uncertainty. We don’t know when edges are used, directly, and we also don’t have a reliable way to estimate the opportunities each edge had to activate (their exposure), either. This is especially true when we wish to know whether an edge even can be traversed, i.e. the edge support. Support, as used in this sense, is the set of inputs for which we expect a non-zero output. Intuitively, this idea captures the sense that we might care more about whether an edge/dependency exists, not how important it is. For that, we have to re-assess our simple model: even if we could count the number of times an edge might have been traversed, how do we estimate the opportunities it had to be available for traversal (it’s “exposure”)?\nAssuming this kind of epistemic uncertainty can be adequately addressed through modeling—attempts at which will be discussed in more detail in Roads to Network Recovery—conditional probability/contingency tables will again be useful for validation. When comparing estimated edge probability to some known “true” edge existence (if we have that), we can count the number of correct predictions, as well as type I (false positive) and type II (false negative) errors. We can do this at every probability/weight threshold value, as well, and we will return to ways to aggregate all of these values into useful scoring metrics in Simulation Study.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-products",
    "href": "content/part1/1-01-matrix-meas.html#sec-products",
    "title": "Metrology as matrices",
    "section": "Distance vs. Incidence",
    "text": "Distance vs. Incidence\nAs we have already seen, operations from linear algebra make many counting and combinatoric tasks easier, while unifying disparate concepts to a common set of mechanics. In addition to having a map from integer indices to sets of interest, these design matrices/vectors are implicitly assumed to have entries that exist in a field \\(F=(\\mathbb{S},\\oplus,\\otimes)\\). equipped with operators analogous to addition (\\(\\oplus\\)) and multiplication (\\(\\otimes\\)).8 With this, we are able to define generalized inner products that take pairs vectors in a vector space \\(\\mathbf{x}\\in V\\), such that \\(\\langle\\cdot,\\cdot\\rangle_F:\\mathbb{S}^n\\times \\mathbb{S}^n\\rightarrow \\mathbb{S}\\). \\[\n\\langle\\mathbf{x}_a,\\mathbf{x}_b\\rangle_{F} = \\bigoplus_{j=1}^n \\mathbf{x}_a(j)\\otimes\\mathbf{x}_b(j)\n\\]\n8  Or, more generally, a semiring if inverse operations for \\(\\oplus,\\otimes\\) don’t exist.We can use this to perform “contractions” along any matching dimensions of matrices as well, since the sum index is well-defined. \\[\n\\begin{aligned}\nX\\in\\mathbb{S}^{m\\times n}\\quad Y\\in\\mathbb{S}^{n\\times m} \\\\\nZ(i,j)=X\\oplus,\\otimes Y = \\bigoplus_{j=1}^{n} X(i,j) \\otimes Y(j,k) = XY\n\\end{aligned}\n\\] For ease-of-use, we will assume the standard field for any given set \\((\\mathbb{S},+,\\times)\\) if not specified otherwise, which recovers standard inner products \\(\\langle\\cdot,\\cdot\\rangle\\). However, [2] illustrates the usefulness of various fields (or semirings). They allow linear-algebraic representation of many graph operations, such as shortest paths through inner products over \\((\\mathbb{R}\\cup -\\infty,\\textrm{min}, +)\\). This works because discrete/boolean edge weights will not accumulate extra strength beyond 1 under contraction over observations.\n\nKernels & distances\nAs alluded to in the previous section, co-occurrence have a deep connection to a Gram matrix, which is a matrix of all pairwise inner products over a set of vectors.\n\\[\nX^TX=G(j,j')=\\langle\\mathbf{x'}_j,\\mathbf{x}_{j'}'\\rangle = \\sum_{i=1}^{m} X(i,j)X(i,j')\n\\tag{2.3}\\]\nMatrices that can be decomposed into another matrix and its transpose are symmetric, and positive semidefinite (PSD), making every gram matrix PSD. They are directly related to (squared) euclidean distances through the polarization identity[CITE?]:9 \\[\nd^2(j,j') = \\|\\mathbf{x}_{j'}'-\\mathbf{x}_{j'}'\\|^2 = G(j,j) - 2G(j,j') + G(j',j')\n\\tag{2.4}\\]\n9 Important: these definitions are all using the \\(\\mathbf{x}'\\) notation to indicate that these measurements are almost exclusively being done in the data space, i.e. on column vectors. While most definitions work on distances in terms of the measurements/objects/data, for inverse problems (like network recovery, structure learning, etc.) they are more often applied in terms of the features (here, the nodes. This can be seen in statistics as well, where covariance and correlation matrices (which are related to the gram and distance matrix definitions above), are defined as relationships between features/dimensions, not individual samples.In our example from before, the gram matrix will have entries showing the number of papers shared by two authors (or total papers by each, on the diagonal). This is because an inner product between two author (column) vectors will add 1 for each paper in the sum only if it has both authors in common. This is called a bipartite projection[CITE] into the authors “mode”, and is illustrated visually in Figure 1.3.\nDue to [CITE Shoenberg/Mercer], we can generalize Equation 2.4 such that any function “kernel” function \\(\\kappa(x,y)\\) that creates PSD matrix \\(K(j,j')\\in\\mathbb{S}^{n\\times n}\\). It says that such a PSD matrix can always be decomposed into a form \\(K=R^TR\\) for any matrix \\(R(i,j)\\in \\mathbb{S}^{m\\times n}\\), thus letting us use the polarization identity to create arbitrary distance metrics. on \\(\\mathbb{S}^n\\) [4]10 \\[\nd_K(j,j') = \\tfrac{1}{2}\\left(K(j,j)+K(j',j'))\\right)-K(j,j')\n\\tag{2.5}\\]\n\n[4] K. Avrachenkov, P. Chebotarev, and D. Rubanov, “Similarities on graphs: Kernels versus proximity measures,” European Journal of Combinatorics, vol. 80, pp. 47–56, Aug. 2019, doi: 10.1016/j.ejc.2018.02.002.\n10  Distance metric, here, means that \\(d(x,y)\\) satisfies the triangle inequality for all \\(x,y\\).This ability to create valid distance measures from arbitrary kernel functions is the core of a vast area of machine learning and statistics that employs the so-called kernel trick. [CITE?] Different kernels yield different properties useful for distinguishing points having specific properties. One class of kernels are normalized to the range \\([0,1]\\), such that we ensure that equality along any one dimension is given a weight of \\(\\tilde{K}(j,j)=1\\). Such a kernel matrix can be derived from any other kernel, and is often combined with a logarithmic similarity measure \\(\\tilde{\\kappa}(x,y)=\\log{s(x,y)}\\). \\[\n\\begin{aligned}\n\\tilde{K}(j,j') &= \\frac{K(j,j')}{\\sqrt{K(j,j)^2K(j',j')^2}}\\\\\nd_{\\tilde{K}}(j,j') &= -\\log{\\tilde{K}(j,j')}\n\\end{aligned}\n\\tag{2.6}\\] Since this is equivalent to applying Equation 2.5 to \\(s\\) directly. This normalization should be familiar as the way cosine similarities and correlation matrices are made as well (also having 1s along their diagonal), and illustrates how non-metric similarities can be potentially made into (pseudo)metrics.\n\n\nIncidence structures & dependency\nRather than how “close” or “far” to points are in vector space, which is described with the kernels and distances above, whether something “touches”—or, is incident to—something else is usually described abstractly as an incidence structure. This is an abstract way to describe how things “touch”, such as when a set of points lie on a line/plane, or nodes touch an edge. We say an incidence structure is a triple of sets called (for historical reasons) points \\(\\mathfrak{P}\\), lines \\(\\mathfrak{L}\\), and flags \\(\\mathfrak{I}\\).[5] \\[(\\mathfrak{P,L,I})\\quad \\mathfrak{I}\\subseteq \\mathfrak{P}\\times\\mathfrak{L}\\]\n\n[5] G. E. Moorhouse, “Incidence geometry,” University of Wyoming, pp. 3–5, 2007.\nRepresenting these as matrices will be further explored in Graphs as incidence structures. But, the discrete nature of these incidence sets makes it clear that estimating the size and elements of \\(\\mathfrak{I}\\), is a very different question from estimating the similarity/distance between two entities in a vector space.\nIn statistics, such discrete structures usually arise when we are concerned with direct dependence from indirect. Take as an example a set of masses connected together by springs. If we shake one mass, all masses will also shake some amount, depending on the spring constants of the springs each mass is “transmitted” force through, and the losses due to friction or air resistance. While the amount of movement over time depends on how “close” in this spring network two masses are, the movement itself can only be transmitted through springs that the masses are incident to. Movement could be modeled through similarity/distance measurements like correlation, since none of the masses are independent (all move when any do), but incidence in terms of spring force transmission is modeled in terms of conditional (in)dependence. If we hold all but two masses still, and moving one doesn’t move the other, then we know they are conditionally independent: no spring connects them!\nThis idea gets formalized as probabilistic graphical models, which are networks that define “incidence” between two variables as conditional dependence. Letting the random variables in the column-space of \\(X\\) be \\(A,B\\) and the remaining columns be \\(C=X(\\cdot,J\\setminus \\{A,B\\})\\), then \\[\nP(A\\cap B |C ) = P(A|C)P(B|C)\\implies (A\\perp B|C)\\implies (A,B)\\notin \\mathfrak{I}\n\\tag{2.7}\\] for a set of incidences \\(\\mathfrak{I}\\) defining a PGM that was sampled as \\(X\\).\n[SPRING GRAPHIC?]\n\n\nConclusion",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology as matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html",
    "href": "content/part1/1-02-graph-obs.html",
    "title": "Incidence through vector representation",
    "section": "",
    "text": "Graphs as incidence structures\nTo provide a sufficiently rigorous foundation for network recovery from binary occurrence, we will need a rigorous way to represent networks and occurrences that lends itself to building structured ways both connect to each other. We build on the incidence structure and matrix product formalism from the previous chapter, introducing various ways to build graphs as incidence structures that have direct representations as matrices. This can be extended to representing occurrences as matrices of hyperedge vectors. This view allows us to interpret different representations of graphs (or hypergraphs) as connected by simple matrix operations.\nTraditionally[1], [2], we might say a graph on nodes (or, “vertices”) \\(v\\in V=\\{1,\\cdots,n\\}\\) and “edges” \\(E\\) is a tuple: \\[\nG=(V,E) \\quad \\textrm{s.t.} \\quad E\\subseteq V \\times V\n\\]\nIn incidence geometry terms, this would be similar to making two duplicate sets of the same nodes, and defining a graph as the set of incidences between nodes. The adjacency matrix \\(A\\) of \\(G\\), degree matrix \\(D\\), and graph/discrete Laplacian \\(L\\) are then defined as:1 \\[\n\\begin{aligned}\nA(u,v) & =\\mathbf{1}_E((u,v)) \\quad &A : V\\times V\\rightarrow \\mathbb{B} \\\\\nD(u,v) & =\\mathrm{diag}({\\textstyle\\sum}_V A(u,v))\\quad &D : V\\times V\\rightarrow \\mathbb{N} \\\\\nL(u,v) & = D(u,v) - A(u,v) \\quad &L : V\\times V\\rightarrow \\mathbb{Z}\n\\end{aligned}\n\\]\nHowever, if edges and their recovery is so important to us, defining them explicitly as nodes-node incidences can be problematic when we wish to estimate edge existence (or not), given noisy pairs of node co-occurrences. Additionally, we have to be very careful to distinguish whether our graph is (un)directed, weighted, simple, etc., and hope that the edge set has been filtered to a subset of \\(N\\times N\\) for each case. Instead, we propose a less ambiguous framework for vectorizing graphs, based on their underlying incidence structure.\nInstead, we give edges their own set of identifiers, \\(e\\in E=\\{1,\\cdots \\omega\\}\\). Now, define graphs as incidence structures between nodes and edges such that every edge is incident to either two nodes, or none:\n\\[\nG = (V,E,\\mathcal{I}) \\quad s.t. \\quad \\mathcal{I} \\subseteq E\\times V\n\\tag{3.1}\\]\nVariations on graphs can often be conveniently defined as constraints on \\(\\mathcal{I}\\):\nTogether, these constraints define “simple” graphs. Similarly, we can equip Equation 3.1 with a function \\(B\\) that allows \\(\\mathcal{I}\\) to encode information about the specific kinds of incidence relations under discussion. We give \\(B\\) a range of possible flag values \\(S\\):\n\\[\nG = (V,E,\\mathcal{I},B) \\quad s.t. \\quad \\mathcal{I} \\subseteq E\\times V\\quad B:\\mathcal{I}\\rightarrow S\n\\tag{3.2}\\]\nIf the “absence” of incidence needs to be modeled explicitly, a “null” stand-in (0,False) can be added to each \\(S\\), which is useful for representing each structure as arrays for use with linear algebra (i.e. \\(\\{0,1\\}\\),\\(\\{-1,0,-1\\}\\),\\(\\mathbb{R}^+_0\\), and \\(\\mathbb{R}\\), respectively). By doing so, we can also place an exact limit on the maximum possible size of \\(\\omega=\\|E\\|\\) in the simple graph case, and indicate edges by their unique ID, such that \\(\\mathcal{I}= E\\times V\\) is no longer a subset relation for \\(E=\\{1,\\cdots,{n\\choose2} \\}\\). Instead of existence in \\(\\mathcal{I}\\), we explicitly use incidence relation \\(S\\) to tell us whether each possible edge “exists” or not, simplifying our graph definition further4:\n\\[\n\\begin{gathered}\nG  = (V,E,B) \\quad s.t. \\quad B : E\\times V \\rightarrow S\\\\\nv \\in V = \\{1,\\cdots, n\\}\\quad \\\\\ne \\in E = \\left\\{1,\\cdots, {n\\choose 2} \\right\\}\n\\end{gathered}\n\\tag{3.3}\\]\nThe representation of \\(B\\) in Equation 3.3 bears a remarkable similarity to our original description of design matrices in Equation 2.1. In fact, as a matrix, \\(B(e,v)\\) is called the incidence matrix: every row has two non-zero entries, with every column containing a number of non-zero entries equal to that corresponding node’s degree in \\(G\\). Traditionally, we use an oriented incidence matrix, such that each row has exactly one positive (non-zero) value, and one negative (non-zero) value.5 Even for undirected graphs, the selection of which entry is positive or negative is left to be ambiguous, since much of the math used later is symmetric w.r.t. direction6.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Incidence through vector representation</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html#sec-incidence-vec",
    "href": "content/part1/1-02-graph-obs.html#sec-incidence-vec",
    "title": "Incidence through vector representation",
    "section": "",
    "text": "Self loops can be prohibited by only allowing unique flags for a given relation2\nMultigraphs are similarly described by whether we allow pairs of vertices to appear with more than one edge3\n\n2  never two flags with the same pair, i.e. \\(\\mathcal{I}\\) is a set, not a multiset.3  in the set of flags containing nodes \\(u\\) or \\(v\\), only one \\(e\\) may be incident to both of them.\n\n\nUndirected, unweighted graphs only need single elements: “incidence exists” i.e.\\(S=\\{1\\}\\)\nDirected graphs can use two elements e.g. a “spin” for \\(S=\\{-1,1\\}\\)\nWeighted, undirected graphs are supported on positive scalars e.g. \\(S=\\mathbb{R}^+\\)\nWeighted, directed graphs are supported on any scalar e.g. \\(S=\\mathbb{R}_{\\neq0}\\)\n\n\n4  if we allomulti-edges , then \n\n5  In fact, this would make B^*(v,e) equivalent to a graphical matroid, another common formalism that generalizes graph-like structures to vector space representations.6 though not always!\nEmbedding incidences in vector space\nA formalism for graphs that starts with incidence matrices would benefit from a canonical oriented incidence matrix, rather than the family that is ambiguous w.r.t. edge orientation. To start, we can be more precise by selecting each row(edge) vector, and partitioning it into two: one for each non-zero column (node) that edge is incident to. Every incidence can be represented individually as standard basis vector \\(\\mathbf{e}\\) in the feature space of \\(B\\), scaled by the corresponding value of \\(S\\).\nLet \\(V_e\\) be the set of nodes with (non-zero) incidence to edge \\(e\\). Then the incidence vectors are:\n\\[\n\\delta_e(v) = B(e,v)\\mathbf{e}_v \\quad \\forall v\\in V_e\n\\tag{3.4}\\] And the (unoriented) incidence matrix vectors are recovered as sums over the incidence vectors for each edge: \\[\n\\mathbf{b}_e = \\sum_{v\\in V_e} \\delta_e(v)\n\\tag{3.5}\\]\nA traditional approach might then define undirected graphs as equivalent, in some sense, to multidigraphs, where each edge is really two directed edges, in opposing directions. This does allow the matrix \\(B\\) to have the correct range for its entries in this formalism (the directed graph range, \\(S=\\{-1,0,1\\}\\)), and the edge identity based on sums would hold. However, the resulting set of incidences would have twice the number of edges than our combinatoric limit for simple graphs, and prevent the more elegant definition of graph types through the set \\(\\mathbf{S}\\). Plus, it would necessitate averaging of weights over different edge ID’s to arrive at a single undirected “edge weight”, and many other implementation details that make keeping track of specifics difficult for practitioners.\nInstead, we would like a canonical oriented distance matrix, which can be derived from the vectorized incidences in the undirected range of \\(B\\) (the standard basis vectors). Without loss of generality, let \\(u_e,v_e\\in V_e\\) be nodes such that \\(u&lt;v\\).7 Using this, we can unambiguously define a partition \\(B(e,\\cdot)=B(e,u_e) + B(e,v_e)\\) between incidences on \\(e\\), along with a new derived incidence, \\(B_o\\), which has oriented rows like: \\[B_o(e,\\cdot)=\\mathbf{b}^o_e = \\delta_e(u)-\\delta_e(v)\\] In other words, while the unoriented incidence matrix is the “foundational” representation for graphs in our formalism, the (canonical) oriented one can be derived, even if negative incidence values are not in \\(\\mathbb{S}\\).8\n7 the inequality is strict because self-loops are not allowed.8  This works as long as we are in at least a ring, since semirings in general do not need to define additive inverse operations. In this case we would limit ourselves to the oriented incidence.\n[3] M. Angeletti, J.-M. Bonny, and J. Koko, “Parallel Euclidean distance matrix computation on big datasets,” 2019. Available: https://hal.science/hal-02047514\nBut, now that we have removed the information on “which nodes an edge connects” from our definition of edges (since every edge is a scalar ID), how do we construct \\(V_e\\) without a circular dependency on \\(B\\) to find non-zero entries? Because of our unique identification of edges up to the combinatoric limit, we can still actually provide a unique ordering of the nodes in \\(V_e\\), without searching over the entirety of \\(B\\)’s domain. Using an identity from [3], we have a closed-form equation both to retrieve the IDs of nodes \\(u,v\\) (given an edge \\(e\\)), and an edge \\(e\\) (given two nodes \\(u,v\\)), for any simple graph with \\(n\\) vertices. \\[\n\\begin{gathered}\n    u_n(e) = n - 2 - \\left\\lfloor\\frac{\\sqrt{-8e + 4n(n - 1) - 7}-1}{2}\\right\\rfloor\\\\\n    v_n(e) = e + u_n(e) + 1 -\\frac{1}{2} \\left(n (n - 1) + (n - u_n(e))^2 - n+u_n(e)\\right)\\\\\n    e_n(u,v) = \\frac{1}{2}\\left(n(n - 1) - ((n - u)^2 - n+u)\\right) + v - u - 1\n\\end{gathered}\n\\tag{3.6}\\] Our ease-of-calculation lets us drop some of the excess notation and refer to our (un)oriented incidence matrices in terms of the incidences of each edge on their \\(u\\) or \\(v\\), directly. \\[\nB = B_u + B_e \\qquad B_o \\equiv B_u - B_v\n\\]\n\n\n\nInner products on \\(B\\)\nWith all of this background, the other representations of graphs can seen as derivations from the canonical incidence matrices. The Laplacian, which is usually introduced either in terms of ajacency/degree, or as the gram matrix for oriented edge vectors, is also related to the gram matrix between all pairs of incidences on \\((u,v)\\). The other identities are simply consequences of the polarization identity: \\[\n\\begin{split}\nL & = B_o^TB_o\\\\\n  & = \\|B_u - B_v \\|^2 \\\\\n  & = 2\\|B_u\\|^2 +2\\|B_v\\|^2 - \\|B_u + B_v \\|^2 \\\\\n  & = 2D - B^TB = D-A\n\\end{split}\n\\tag{3.7}\\]\n\n\n\n\n\n\n\n\n\n\n\nThe Laplacian is often used in defining random-walks and markov chains, such that the degree of each node should be normalized to 1, which can be accomplished either by row- or column-normalizing it: \\(L^{\\textrm{rw}}=D^{-1}L\\) or \\(LD^{-1}\\), respectively. If this degree-normalization is desired without de-symmetrizing \\(L\\), we can still perform an operation similar to normed kernels in Equation 2.6, called the symmetric normalized Laplacian. \\[\n\\tilde{L} = D^{-\\tfrac{1}{2}}LD^{-\\tfrac{1}{2}} = \\frac{L(u,v)}{\\sqrt{L(u,u)^2L(v,v)^2}}\n\\tag{3.8}\\]\n\n\nEdge Metrology, Edge Vectors\nImplicitly in the use of \\(B\\) with design matrix mechanics from the previous chapter is a treatment of edges as “observations” (the data space), and nodes as features. If an edge is an observation, then unfortunately we cannot really quantify uncertainty over repeated measurements of edges with the simple mechanics from Additive Smoothing (because that edge is that corresponding observation, and IDs are not duplicated).\nSo far we have seen two ways of representing the entire graph in matrix form: Incidence matrix \\(B\\) (or \\(B_o\\)), and the inner-product matrices derived from it (\\(L\\), \\(A\\)). Since we can recover node IDs from edge IDs by Equation 3.6, we can use a single vector to represent an entire graph structure by it’s edges alone. Then a data dimension for vectors in “edge space” can once again represent observations, with nodes implied by Equation 3.6. This is either done by contracting \\(B\\) along the nodes (columns) dimension, or by unrolling the upper triangle of \\(L\\) or \\(A\\) according to Equation 3.6.9 If each vector represents a value of \\(B\\) associated with a corresponding edge, then \\(m\\) of these vectors would be equivalent to \\(m\\) observations of \\({n \\choose 2}\\) edges on the same set of \\(n\\) nodes. Formally, for the \\(i\\)th observed structure on \\(n\\) nodes: \\[\n\\begin{gathered}\nR(i,e) = \\min(\\{B_i(e, u_n(e)),B_i(e,v_n(e))\\}) \\\\\n\\quad \\textrm{s.t.} \\quad R:I\\times E \\rightarrow \\mathbb{S}\\\\\ni\\in I = \\{1,\\cdots,m\\} \\qquad e \\in E=\\left\\{1,\\cdots,\\omega\\right\\}\\\\\nn=\\tfrac{1}{2}(1+\\sqrt{8\\omega+1})\n\\end{gathered}\n\\tag{3.9}\\] This representation formalizes what practitioners call “edgelists” into a data structure that can unambiguously distinguish directed, undirected, and weighted graphs. In addition, it allows for repeated measurements of edges over the same set of nodes, while flexibly growing when new nodes arrive.10\n9  Equation 3.9 uses an averaging operation to accomplish the contraction, but any reduction over the two nodes shared by an edge would accomplish the same, especially since we rarely see separate values for edge weight per-node, the way incidences can.10  For instance, say observations are stored as sparse entries via \\(R\\), and a new node arrives. First, the participating nodes can be recovered in a vectorized manner through Equation 3.6. Then, a new node id increases \\(n\\), followed by reassignment of the edge IDs with \\(e_n(u,v)\\).",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Incidence through vector representation</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html#generalized-node-occurrence-vectors",
    "href": "content/part1/1-02-graph-obs.html#generalized-node-occurrence-vectors",
    "title": "Incidence through vector representation",
    "section": "Generalized node occurrence vectors",
    "text": "Generalized node occurrence vectors\nTODO\n\n\n\nHyperedge Relation Observational Model\n\n\n\nHyperedges as vectors of node occurrence\n\n\n\n\n\n\n\n\n\\[\n%X(\\{1,2,3,4,\\cdots\\})=\\\\\n\\begin{array}{c c}\n& \\begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\\\ \\end{array} \\\\\n\\begin{array}{c c } x_1\\\\x_2\\\\x_3\\\\x_4\\\\ \\vdots \\end{array} &\n\\left[\n\\begin{array}{c c c c c c c c c c}\n  0  &  0  &  1  &  0  &  1  &  0  &  1  &  1  &  0  &  0 \\\\\n  1  &  0  &  0  &  0  &  1  &  1  &  0  &  0  &  0  &  0 \\\\\n  0  &  1  &  0  &  0  &  0  &  1  &  0  &  0  &  1  &  1 \\\\\n  0  &  0  &  0  &  1  &  1  &  0  &  0  &  1  &  0  &  0 \\\\\n  &&&& \\vdots &&&&&\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Bipartite representation of node “activation” data\n\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\nInner product on Hyperedges\nRoundabout way of describing binary/occurrence data. Inner product is co-occurrences.\nLeads to correlation/covariance, etc.\n\n\nCombining Occurrence & Dependence\n\nsoft cosine\nkernels on graphs (incl. coscia euclidean)\nLinear operator incidences\nRetrieving one from the other is hard.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Incidence through vector representation</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html",
    "href": "content/part1/1-03-recovery-road.html",
    "title": "Roads to Network Recovery",
    "section": "",
    "text": "Organizing Recovery Methods\ni.e. Network Recovery as an Inverse Problem, and what information is had at each point.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#organizing-recovery-methods",
    "href": "content/part1/1-03-recovery-road.html#organizing-recovery-methods",
    "title": "Roads to Network Recovery",
    "section": "",
    "text": "Observing Nodes vs Edges\nEmbeddings and Inner Products\nPreprocessing",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#counting-projection-and-co-occurrence",
    "href": "content/part1/1-03-recovery-road.html#counting-projection-and-co-occurrence",
    "title": "Roads to Network Recovery",
    "section": "Counting, Projection, and Co-Occurrence",
    "text": "Counting, Projection, and Co-Occurrence\nBasic Co-occurrence, and friends. Gram is uncentered covariance.\nAll estimate some kind of rate\n\nCosine Similarity\n\nOchiai and uncentered correlation\nangles don’t care about length\ngeometric mean on cond prob, actually\nthat means pseudocounts can make sense\n\nwe can actually interpret ochiai/cosine as an approximate bayes update.\nSay we observe binary variables A and B, along with others.\nNow, we want a probability of conditional dependence between A and B (does information flow directly between a and b when a and b happen together?). So we want the probability ofand edge \\(E_{ab}\\) being used, given that such an edge had the opportunity to be used. E.g. did two people have a causal interraction to make each other sick, given a time when we know they were both exposed/became sick. \\[P(E|O) = P(O|E)P(E)/P(O)\\]\nThe denominator is hard, because while we can estimate the frequency of each node as the occurrences/opportunities (i.e. events/exposure), so, \\(n_i/N\\), we can’t use that for an exposure for “number of times an edge between A,B could have been used”. If we multiply \\(N(a)N(b)\\), then we have the number of ways a or b could be related over all chances, but this won’t be a fraction of the number of samples, and could possible be much bigger. So dividing the number of times both did happen together by that number won’t get us a probability. So instead, we fib a bit.\nThe number of chances (out of all samples) that a pair had to happen together is somewhere between the chances each had separately. We make a pseudo-variable that uses this fact, but averaging the rates. but we are dealing with probabilities, which are based around “areas” and their ratios. So we want one count, such that watching it with a copy of itself has the same exposure as watching A and B separately. This is exaclty what geometric means are for:\nThen, a point estimate for the probability of an edge occurring is its actual co-ocurrence \\(n_{a,b}/N\\). It’s the ratio of these that give us the Ochiai as a probability: divided by the estimate for the co-ocurrence opportunities\n\n\nHyperbolic Projection\n\nreweights by degree in original BP\nmention other ways\n\n\n\nTransformations of Counts\nCome from contingency tables, like CS.\n\nodds-ratio\nYule’s Q,Y\nMutual Info",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#resource-and-information-flow",
    "href": "content/part1/1-03-recovery-road.html#resource-and-information-flow",
    "title": "Roads to Network Recovery",
    "section": "Resource and Information Flow",
    "text": "Resource and Information Flow\nBackboning\nIntuition that the most important edges are on geodesics for many paths\n\nResource Projection\n\n\nDoubly-stochastic and eOT\n\n\nHSS",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#inverse-problems-pgm-structure",
    "href": "content/part1/1-03-recovery-road.html#inverse-problems-pgm-structure",
    "title": "Roads to Network Recovery",
    "section": "Inverse Problems & PGM Structure",
    "text": "Inverse Problems & PGM Structure\n\nChow Liu\nGlobal Tree structure from MI\n\n\nShrinkage & GLASSO\n\n\nstability selection\n\nTakeaway: a way to organize existing algorithms, AND highlight unique set of problems we set out to solve",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#a-path-forward",
    "href": "content/part1/1-03-recovery-road.html#a-path-forward",
    "title": "Roads to Network Recovery",
    "section": "A Path Forward",
    "text": "A Path Forward\n\nTracing Information Loss Paths\n\n\n\nRelating Graphs and Hypergraph/bipartite structures as adjoint operators\n\n\nTable of Existing Approaches\n\nObservation-level loss (starting with the inner product or kernel)\nNon-generative model loss (no projection of data into model space)\nno uncertainty quantification\n\n\n\nSomething New\nSorting algorithms… none address all three!\ni.e. MOTIVATES FOREST PURSUIT",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html",
    "href": "content/part2/2-01-rand-sf.html",
    "title": "Graph Reduce & Desire Paths",
    "section": "",
    "text": "The Gambit of the Inner Product\nAddressing gaps discussed in the previous section to reach a generative model for network recovery requires careful attention to the generation mechanism for node activations. While there are many ways we might imagine bipartite data to be be generated, presuming the existence of a dependency graph that causes activation patterns will give us useful ways to narrow down the generative specification.\nFirst, we will investigate the common assumption that pairwise co-occurrences can serve as proxies for measuring relatedness, and how this “gambit of the group” is, in fact, a strong bias toward dense, clique-filled recovered networks. Because we wish to model our node activations as being caused by other nodes (that they depend on), we draw a connection to a class of models for spreading, or, diffusive processes. We outline how random-walks are related to these diffusive models of graph traversal, enabled by an investigation of the graph’s “regularized laplacian” from [1]. Then we use the implicit causal dependency tree structure of each observation, together with the Matrix Forest Theorem [2], [3] to more generally define our generative node activation model. This leads to a generative model for binary activation data as rooted random spanning forests on the underlying dependency graph.\nAs we saw repeatedly in Roads to Network Recovery, networks are regularly assumed to arise from co-occurrences, whether directly as counts or weighted in some way. This assumption can be a significant a source of bias in the measurement of edges. Why a flat count of co-occurrence leads to “hairballs” and bias for dense clusters can be related to the use of inner products on node activation vectors.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Graph Reduce & Desire Paths</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html#the-gambit-of-the-inner-product",
    "href": "content/part2/2-01-rand-sf.html#the-gambit-of-the-inner-product",
    "title": "Graph Reduce & Desire Paths",
    "section": "",
    "text": "Gambit of the Group\nIt seems reasonable, when interactions are unobserved, to assume some evidence for all possible interactions is implied by co-occurrence. However, similar to the use of uniform priors in other types of inference, if we don’t have a good reason to employ a fully-connected co-occurrence prior on interaction dependencies, we are adding systematic bias to our inference. Whether co-occurrence observations can be used to infer interaction networks directly was discussed at length in [4], where Whitehead and Dufault call this the gambit of the group.\n\n[4] H. Whitehead and S. Dufault, “Techniques for analyzing vertebrate social structure using identified individuals: Review and recommendations,” Advances in the Study of Behavior, vol. 28, no. 28, pp. 33–74, 1999.\n\nSo, consiously or unconsciously, many ethnologists studying social organization makr what might be called the “gambit of the group”: the assume that animals which are clustered […] are interacting with one another and then use membership of the same cluster […] to define association.\n\nThis work was rediscovered in the context of measuring assortativity for social networks,1 where the author of [5] advises that “group-based methods” can introduce sampling bias to the calculation of assortativity, namely, systematic overestimation when the sample count is low.\n1 Assortativity is, roughly, the correlation between node degree and the degrees of its neighbors.\n[5] D. N. Fisher, M. J. Silk, and D. W. Franks, “The perceived assortativity of social networks: Methodological problems and solutions,” in Trends in social network analysis, Springer International Publishing, 2017, pp. 1–19. doi: 10.1007/978-3-319-53420-6_1.\n\n[6] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\nThe general problems with failing to specify a model of what “edges” actually are get analyzed in more depth in [6]. They include a warning not to naively use correlational measures with a threshold, since even simple 3-node systems will easily yield false positives edges. Still, it would be helpful for practitioners to have a more explicit mental model of why co-occurence-based models yield systematic bias,\n\n\nInner-Product Projections and “Clique Bias”\nUnderlying correlation and co-occurrence models for edge strength is a reliance on inner products between node occurrence vectors. They all use gram matrices (or centered/scaled versions of them), which were brought up in Distance vs. Incidence. The matrix multiplication performed represents inner products between all pairs of feature vectors. For \\(X(i,j)\\in\\mathbb{B}\\), these inner products sum together the times in each observation that two nodes were activated together.\n\n\n\n\n\n\n\n\n\nFigure 5.1: Gram matrix as sum of observation outer products\n\n\n\n\n\n\nHowever, another (equivalent) way to view matrix multiplication is as a sum of outer products \\[\nG(j,j') = X^T X = \\sum_{i=1}^m X(i,j)X(i,j')= \\sum_{i=1}^m \\mathbf{x}_i\\mathbf{x}_i^T\n\\] Those outer products of binary vectors create \\(m\\times m\\) matrices that have a 1 in every \\(i,j\\) entry where nodes \\(i,j\\) both occurred, shown in Figure 5.1. Each outer product is effectively operating as a \\(D_i+A_i\\) with degrees normalized to 1. If the off-diagonals can be seen as adjacency matrices, they would strictly represent a clique on nodes activated in the \\(i\\)th observation In this sense, any method that involves transforming or re-weighting a gram matrix, is implicitly believing that the \\(k\\)th observation was a complete graph. This is illustrated in Figure 5.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Edge Measurements with Group Gambit (BoW) assumption\n\n\n\n\n\n\n\n\n\nFigure 5.2: Inner-product projections as sums of cliques illustrating “clique bias”.\n\n\n\nFor many modeling scenarios, this paradigm allows practitioners to make a more straight-forward intuition-check: do clique observations make sense here? When a list of authors for a paper is finished, does that imply that all authors mutually interacted with all others directly to equally arrive at the decision to publish? This would be similar to assuming the authors might simultaneously enter the same room, look at a number of others (who all look exclusively at each other, as well), and all at once decide to publish together. In our introduction, we described a more likely scenario we could expect from an observer on the ground: a researcher asks a colleague or two to collaborate, who might know a couple more with relevant expertise, and so on.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Graph Reduce & Desire Paths</span>"
    ]
  },
  {
    "objectID": "content/part2/2-01-rand-sf.html#networks-as-desire-path-density-estimates",
    "href": "content/part2/2-01-rand-sf.html#networks-as-desire-path-density-estimates",
    "title": "Graph Reduce & Desire Paths",
    "section": "Networks as Desire Path Density Estimates",
    "text": "Networks as Desire Path Density Estimates\nUnfortunately, methods based on inner-product thresholding are still incredibly common, in no small part due to how easy it is to create them from occurrence data, regardless of this “clique-bias”. The ability to map an operation onto every observation, e.g. in parallel, and then reduce all the observations into an aggregate edge estimate is a highly desirable algorithmic trait. This may be a reason so many projection and backboning techniques attempt two re-weight (but retain) the same basic structure, time and again.\nWhat we need is a way to maintain the ease-of-use of inner-product network creation:\n\nmap an operation onto each observation\nreduce to an aggregate edge guess over all observations\n\nbut with a more domain-appropriate operator at the observation level.\nLet’s start with replacements for the clique assumption. There are many non-clique classes of graphs we might believe local interactions occur on: path-graphs, trees, or any number of graphs that reflect the topolgy or mechanism of local interactions in our domain of interest. Authors have proposed classes of graphs that mirror human perception of set shapes [RELATIVE NEIGHBORHOOD]2, or graphs whose modeled dependencies are strictly planar [planar maximally filtered graps]3. Alternatively, the interactions might be scale free, small-world, trees, or samples from stochastic block models.[CITE] In any case, these assumptions provide an explicit way to describe the set of possible interaction graphs we believe our individual observations are sampled from.\n2  e.g. for dependencies based on perception, such as human decision making tendencies, or causes based on color names.3  e.g. when interactions are limited to planar dependencies, like inferring ancient geographic borders.\nSubgraph Distributions\nLet’s use the notation from Equation 3.9, such that each observation of nodes \\(\\mathbf{x}_i\\) is implicitly derived from a set of activated edges \\(\\mathbf{r_i}\\). To start, our current belief about what overall structure the whole network can take is \\(G^*=(V,E,B^*)\\), where \\(B^*\\) might always return \\(1\\) to start out (the complete graph). To further constrain the problem, let us assume that node activation is noiseless: any activated nodes were truly activated, and unactivated nodes were truly inactive (no false negative or false positive activations).4 So, each \\(\\mathbf{x}_i\\) will induce a subgraph \\(g_i = G^*[V_i]\\), where \\(V_i = \\{v\\in \\mathcal{V} | X(i,\\mathcal{V})=1\\}\\). Then, our domain knowledge takes the form of a constraint on edges within that subgraph, which will induce a family of subgraphs on \\(g_i\\). This family \\(\\mathcal{C}_i\\) belongs to the relevant class of graphs \\(\\mathcal{C}\\), limited to nodes \\(V_i\\), i.e.\n4  Hidden nodes (unobserved nodes beyond the \\(n\\)) are outside the scope of this work, though could potentially be implied for certain structures e.g. when the metric is known to be tree-like. [7] use a greedy embedding that minimizes distortion to estimate the need for added Steiner nodes. \n[7] R. Sonthalia and A. C. Gilbert, “Tree! I am no tree! I am a low dimensional hyperbolic embedding,” arXiv; arXiv, arXiv:2005.03847, Oct. 2020. doi: 10.48550/arXiv.2005.03847.\n5 \\(\\mathcal{P}(A)\\) is the powerset of \\(A\\), i.e. the set of all subsets of \\(A\\). \\[\n\\begin{gathered}\n\\mathcal{C}_i = \\{(V,E,B_i) \\in\\mathcal{C}|B_i(e,v)=B^*(e,v)\\mathbf{1}_{V_i}(v)\\mathbf{1}_{E_i}(e)\\}\\\\\nE_i\\in\\{\\mathcal{E}\\in\\mathcal{P}(E)| g_i[\\mathcal{E}]\\in\\mathcal{C}\\}\nV_i = \\{v\\in \\mathcal{V} | X(i,\\mathcal{V})=1\\}\n\\end{gathered}\n\\tag{5.1}\\]5\nIn other words, the edges that “caused” to the node activations in a given observation must together belong to a graph that, in turn, belongs to our desired class, which must occur on the nodes that were activated.\nAssuming we can define an associated measure \\(\\mu_i(c)\\) onfor each \\(c\\in\\mathcal{C}_i\\), we are able to define a probability distribution over subgraphs in the class.6 Using notation similar to distributions over spanning trees in [9]:\n6  This is certainly not a trivial assumption, and might either be ill-defined or require techniques like the Gumbel trick[8] to approximate, unless the partition function \\(\\mathbb{Z}\\) has a closed form, or \\(\\mu\\) is already a probability measure on some \\(\\sigma\\)-algebra over \\(\\mathcal{C}\\). Closed-form \\(\\mathcal{Z}\\) values on \\(\\mathcal{C}\\) are known for a handful of graph classes, such as the space of spanning trees, \\(\\mathcal{C}=\\mathcal{T}\\). However, another way this might be accomplished is through random geometric graphs[CITE], or geometric spanners like Urqhart[CITE] and Relative Neighborhood [CITE] graphs on a “sprinkling” of points that preserves their observed pairwise distances.\nThis is further discussed in Generalizing Inner Products on Incidences.\n[8] M. B. Paulus, D. Choi, D. Tarlow, A. Krause, and C. J. Maddison, “Gradient estimation with stochastic softmax tricks.” arXiv, 2020. doi: 10.48550/ARXIV.2006.08063.\n\n[9] R. Zmigrod, T. Vieira, and R. Cotterell, “Efficient computation of expectations under spanning tree distributions,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 675–690, Jul. 2021, doi: 10.1162/tacl_a_00391.\n\\[\n\\begin{gathered}\np_i(c) = \\frac{\\mu_i(c)}{\\mathbb{Z}_i}\\\\\n\\mathbb{Z}_i = \\sum_{c\\in\\mathcal{C}_i} \\mu_i(c)\n\\end{gathered}\n\\tag{5.2}\\]\nThis can be represented using the vectorization in Equation 3.9, due to the one-to-one correspondence established, so that, with a slight abuse of notation treating \\(\\mathcal{C}_i\\) as the parameter of distribution \\(p_i\\): \\[\n\\mathbf{r}_{i}(e)|\\mathbf{x_i} \\sim p_i(\\mathcal{C},E,V)\n\\tag{5.3}\\]\nSo we may not have an exact vector, but we have established a way to specify a family of edge vectors that could be responsible. With \\(p_i(c)\\), we also have a mechanism to sample them when a partition function is available (or able to be approximated).\nWith these mechanics in place, we see the choice of a clique (implied by the inner product) is a trivial application of Equation 5.3, since that is equivalent to selecting the class of cliques on \\(V_i\\) nodes. This has only one element (\\(\\|\\mathcal{C}_{\\text{clique}}\\|=1\\)), there is only 1 possible selection, with probability \\(p_i(K^{V_i})=1\\).\n\n\n\n\nGraph Unions as Desire Paths\nWith a distribution over subgraphs each observation, we are potentially able to sambple from them for bootstrap or Monte Carlo estimation purposes, or simply find a maximum likelihood estimate for each distribution. Assuming this is true, we may now sample or approximate a matrix \\(R(i,e):I\\times E \\rightarrow \\mathbb{B}\\).\nMethods for doing this efficiently in certain cases are the focus of Forest Pursuit: Approximate Recovery in Near-linear Time and Bayesian Estimation by Gibbs Sampling. However, once \\(R(i,e)\\) is estimated, a reasonable mechanism for estimating the support of the set of edges is to use \\(\\frac{\\text{count}}{\\text{exposure}}\\), but with a few needed modifications.\nFirst, while the nodes counts in \\(\\sum_i B(i,\\cdot)\\) are by assumption not independent, or even pairwise independent, the edge traversal counts \\(\\sum_i R(i,\\cdot)\\) could more reasonably be considered so. A model certainly could be constructed where edge existence depends on other edges existing (or not). But nothing is inherently self-inconsistent with a model that assumes the traversability of individual edges will be independent of one another.\nlet P(e) be the probability that an edge is traversed (in any observation), and P(u,v) the probability that nodes \\(u,v\\) co-occur. To approximate the overall traversability of an edge, we can calculate an empirical estimate for the conditional probability \\(P(e|(u,v))=\\frac{P(e)\\cap P(u,v)}{P(u,v)}\\) that an edge is traversed, given that the two incident nodes are activated. This estimate can use the same beta-bernoulli distribution from Equation 2.2.\nStill, how do we ensure our estimate is approximating traversability, so that the probability that an edge probability converges toward 1 as long as it has to be possible for \\(e\\) to be traversed? Recall from the introduction that, from a measurement perspective, the underlying networks rarely “exist” in the sense that we never truly find the underlying network, but only samples from (or caused by) it. Imagine that the “real” network is a set of paved sidewalks: our procedure is similar to watching people walk along paths between locations, and wanting to estimate which of the paths would be tread “enough” to be paved. This is where we build on an intuition based on the popular idea of desire paths which is a colloquial name for paths that form when individuals walk over grass or dirt enough to carve a trail. In network analysis and recovery, we usually are only allowed to see the desire paths that might have formed from our data, never the actual “pavement”.\nIf presented with two equivalent paths, an individual is likely to choose the one that has been tread more often before i.e. the “more beaten” path. So, we don’t want a probability that an edge has been traversed, but a probability over fractions of the time we expect an edge to have been traversed more often than not: how likely it is to “be beaten”. This is accomplished by forcing \\(\\alpha, \\beta &lt; 1\\). For the case where \\(\\alpha=\\beta=0.5\\), we call this special case an arcsine distribution.\nIn a coin tossing game where each “heads” gives a point to player A and “tails” to player B, then the point differential is modeled as a random walk. The arcsine distribution \\(\\text{Beta}(\\tfrac{1}{2},\\tfrac{1}{2})\\) is exactly the probability distribution for the fraction of time we expect one player to be “ahead” of the other. [10]\n\n[10] E. Ackelsberg, “What is the arcsine law?” 2018, Available: https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf \n\n“Contrary to popular opinion, it is quite likely that in a long coin-tossing game one of the players remains practically the whole time on the winning side, the other on the losing side.”\nWilliam Feller[11, Ch. III]\n\n[11] W. Feller, An introduction to probability theory and its applications, volume 1. J. Wiley & Sons: New York, 1968.\n\nThis is desirable behavior for a distribution over edge traversability! We expect the vast majority of edges to have a 0 or 1 as the most likely values, with the expected fraction of observations that an edge being traversed was “ahead” of it being not traversed matching our empirical count. We generalize this with \\(\\alpha = 1-\\beta\\), with \\(\\alpha + \\beta = 1\\), such that the new beta posterior from Equation 2.2 with \\(s\\) successes over \\(k\\) trials is:\n\\[\n\\begin{gathered}\n\\pi \\sim \\text{Beta}(\\alpha + c, 1-\\alpha-c) \\\\\nc = \\frac{s-ak}{k+1}\\\\\n\\end{gathered}\n\\tag{5.4}\\]\nImportant to note is that \\(k\\) is measured over the co-occurrences \\((u,v)\\), and successes are the traversals \\(e\\) derived from our samples in \\(R\\). This lets us formulate a likelihood model for each edge’s traversibility in the global network \\(G\\) (i.e. whether \\(B(e,v)&gt;0\\) for any \\(v\\)), which is i.i.d. Bernoulli. \\[\n\\begin{gathered}\n\\pi_e \\sim \\text{Beta}(\\alpha, 1-\\alpha), e\\in E\\\\\nE \\sim \\text{Bernoulli}(\\pi_e), e \\in E\n\\end{gathered}\n\\]\nThis does not yet specify a likelihood for \\(\\mathcal{C}_i\\), because we have not included a mechanism for the down-selection to each \\(\\mathbf{x}_i\\). This will be addressed more completely for the special case of \\(\\mathcal{C}=\\mathcal{F}\\), the set of spanning forests on a graph, in Generative Model Specification. In general, however, failing to specify the prior distribution on \\(\\mathcal{C}_i\\) does not necessarily make Equation 5.3 unusable, but necessitates an “empirical bayes” approach. With the marginals and co-occurrence counts for nonzero values in \\(X\\), we can make a point estimate for each edge given a node subset, without needing to consider a prior distribution for each \\(\\mathbf{x_i}\\).\nThe nonparametric approach, in cases that merit the use of spanning trees, will be central to accurate, scalable estimation of \\(B\\) through our proposed method, Forest Pursuit.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Graph Reduce & Desire Paths</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html",
    "href": "content/part2/2-02-forest-pursuit.html",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "",
    "text": "Random Walks as Spanning Forests\nIn this chapter, we build on the notion of “Desire Path” estimation of a dependency network from node activations — sampling from a class of subgraphs constrained to active nodes, then merging them. We present Forest Pursuit (FP), a method that is scalable, trivially parallelizes, and offline-capable, while also outperforming commonly used algorithms across a battery of standardized tests and metrics. The key application for using FP is in domains where node activation can be reasonably modeled as arising due to random walks—or similar spreading process—on an underlying dependency graph.\nFirst, we build an intuition for the use of trees as an unbiased estimator for desire path estimation when spreading processes are at work on the latent network. Then, the groundwork for FP is laid by combining sparse approximation through matching pursuit with a loss function modeled after the Chow Liu representation for joint probability distributions. The approximate complexity for FP is linear in the spreading rate of the modeled random walks, and linear in dataset size, while running in \\(O(1)\\) time with respect to the network size. This departs dramatically from other methods in the space, all of which assume to scale in the number of nodes. We then test FP against an array of alternative methods (including GLASSO) with MENDR, a newly-developed standard reference dataset and testbench for network recovery. FP outperforms other tested algorithms in nearly every case, and empirically confirms its complexity scaling for sub-thousand network sizes.\nThe class of diffusive processes we focus on “spread” from one node to another. If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes “depend” on others to be activated. In this case, a node activates when another node it depends on spreads their state to it. These single-cause activations are often modeled as a random-walk on the dependency graph: visiting a node leads to its activation.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#random-walks-as-spanning-forests",
    "href": "content/part2/2-02-forest-pursuit.html#random-walks-as-spanning-forests",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "",
    "text": "Random Walk Activations\nRandom walks are regularly employed to model spreading and diffusive processes on networks. If a network consists of locations, states, agents, etc. as “nodes”, and relationships between nodes as “edges”, then random walks consist of a stochastic process that “visits” nodes by randomly “walking” between them along connecting edges. Epidemiological models, cognitive search in semantic networks, stock price influences, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. [CITE…lots?]\nWhen network structure is known, the dynamics of random-walks are used to capture the network structure via sampling [LITTLEBALLOFFUR, etc], estimate node importance’s[PAGERANK], or predict phase-changes in node states (e.g. infected vs. uninfected)[SIR I think]. In our case, Since we have been encoding the activations as binary activation vectors, the “jump” information is lost—activations are “emitted” for observation only upon the random walker’s initial visit.[1] Further, the ordering of emissions has been removed in our binary vector representation, leaving only co-occurrence groups in each \\(\\mathbf{x}_i\\).1 In many cases, however, the existence of relationships is not known already, and analysts might assume their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.\n\n[1] K.-S. Jun, J. Zhu, T. T. Rogers, Z. Yang, et al., “Human memory search as initial-visit emitting random walk,” Advances in neural information processing systems, vol. 28, no. 20, pp. 2389–2393, 2015, doi: 10.1016/j.physleta.2019.04.060.\n1 For a brief treatment of the case that INVITE emission order is preserved, see Recovery from Partial Orders\n\nFinite Activations & Spanning Forest\nAs a general setting, the number of node activations (e.g. for datasets like co-authorship) is much smaller than the set of nodes (\\(\\|\\mathbf{x}_i\\in\\mathbb{S}^n\\|_0 \\ll n\\))2\n[2] go to some length describing discrete- and continuous-time random walk models that can give rise to binary activation vectors like our \\(X(i,j):I\\times J\\rightarrow \\mathbb{B}\\). The regularized laplacian (or forest)kernel of a graph[3] plays a central role in their analysis, as it will in our discussion going forward.\n2  \\(\\|\\cdot\\|_0\\) is the \\(\\ell_0\\) “pseudonorm”, counting non-zero elements (the support) of its argument.\n[2] K. Avrachenkov, P. Chebotarev, and A. Mishenin, “Semi-supervised learning with regularized laplacian,” Optimization Methods and Software, vol. 32, no. 2, pp. 222–236, Mar. 2017, doi: 10.1080/10556788.2016.1193176.\n\n[3] K. Avrachenkov, P. Chebotarev, and D. Rubanov, “Similarities on graphs: Kernels versus proximity measures,” European Journal of Combinatorics, vol. 80, pp. 47–56, Aug. 2019, doi: 10.1016/j.ejc.2018.02.002.\n\\[\nQ_{\\beta} = (I+\\beta L)^{-1}\n\\tag{6.1}\\]\nIn that work, it is discussed as the optimal solution to the semi-supervised “node labeling” problem, having a regularization parameter \\(\\beta\\), though its uses go far beyond this.[4], [5], [6] \\(Q\\) generalizes the so-called “heat kernel” \\(\\exp{(-t\\tilde{L})}\\), in the sense that it solves a lagrangian relaxation of a loss function based on the heat equation. This can be related to the PageRank (\\(\\exp{(-tL^{\\text{rw}})}\\)) kernel as well, which is explicitly based on random walk transition probabilities.\n\n[4] J. Pang and G. Cheung, “Graph laplacian regularization for image denoising: Analysis in the continuous domain,” IEEE Transactions on Image Processing, vol. 26, no. 4, pp. 1770–1785, Apr. 2017, doi: 10.1109/TIP.2017.2651400.\n\n[5] O. Knill, “Counting rooted forests in a network,” arXiv, arXiv:1307.3810, Jul. 2013. doi: 10.48550/arXiv.1307.3810.\n\n[6] P. Chebotarev and E. Shamis, “The matrix-forest theorem and measuring relations in small social groups,” arXiv, arXiv:math/0602070, Feb. 2006. doi: 10.48550/arXiv.math/0602070.\n3 \\(Q\\) can also be interpreted as a continuous-time random walk location probability, after exponentially distributed time, if spending exponentially-distributed time in each node.4 edge weights scaled by In fact, \\(Q\\) can be viewed as a transition matrix for a random walk having a geometrically distributed number of steps, giving us a small expected support for \\(\\mathbf{x}_i\\), as needed.3 However we interpret \\(Q\\), a remarkable fact emerges due to a theorem by Chebotarev: each entry \\(q=Q(u,v)\\) is equal to the probability4 that nodes \\(u,v\\) are connected in a randomly sampled spanning rooted forest\nIn other words, co-occurring node activations due to a random walk or heat kernel are deeply tied to the chance that those nodes find themselves on the same tree in a forest.\n\n\nSpreading Dependencies as Trees\nThe whole graph isn’t a tree….Every data point is.\n\n\n\n\n\n\n\n\n\nFigure 6.1: Edge Measurements with true (tree) dependencies known",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#sparse-approximation",
    "href": "content/part2/2-02-forest-pursuit.html#sparse-approximation",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Sparse Approximation",
    "text": "Sparse Approximation\n\nProblem Specification\n\n\nMatching Pursuit\n\n\nSpace of Spanning Forests",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#sec-FP",
    "href": "content/part2/2-02-forest-pursuit.html#sec-FP",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "text": "Forest Pursuit: Approximate Recovery in Near-linear Time\nI.e. the PLOS paper (modified basis-pursuit via MSTs) ### Algorithm Summary\n\nUncertainty Estimation\n\n\nApproximate Complexity",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#sec-FP-experiments",
    "href": "content/part2/2-02-forest-pursuit.html#sec-FP-experiments",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Simulation Study",
    "text": "Simulation Study\n\nMethod\n\n\nResults - Scoring\n\n\n\n\n\n\n\n\n\nFigure 6.2: Comparison of MENDR recovery scores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: Comparison of MENDR Recovery Scores by Graph Type\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: Partial Residuals (regression on E[MCC])\n\n\n\n\n\n\n\n\nResults - Performance\n\n\n\n\n\n\n\n\n\nFigure 6.5: Runtime Scaling (Forest-Pursuit vs GLASSO)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.6: Partial Residuals (regression on computation time)",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-02-forest-pursuit.html#discussion",
    "href": "content/part2/2-02-forest-pursuit.html#discussion",
    "title": "Forest Pursuit: Approximate Recovery in Near-linear Time",
    "section": "Discussion",
    "text": "Discussion\n\nInteraction Probability",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Forest Pursuit: Approximate Recovery in Near-linear Time</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html",
    "href": "content/part2/2-03-latent-forest-alloc.html",
    "title": "LFA: Latent Forest Allocation",
    "section": "",
    "text": "LFA as Factorization",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#lfa-as-factorization",
    "href": "content/part2/2-03-latent-forest-alloc.html#lfa-as-factorization",
    "title": "LFA: Latent Forest Allocation",
    "section": "",
    "text": "Dictionary Learning\n\n\nInteraction Degree, not Binary\n\n“plan” from avrachenkov",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#sec-lfa-like",
    "href": "content/part2/2-03-latent-forest-alloc.html#sec-lfa-like",
    "title": "LFA: Latent Forest Allocation",
    "section": "Generative Model Specification",
    "text": "Generative Model Specification\n - hierarchical model - marginalize over the root node.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#sec-lfa-gibbs",
    "href": "content/part2/2-03-latent-forest-alloc.html#sec-lfa-gibbs",
    "title": "LFA: Latent Forest Allocation",
    "section": "Bayesian Estimation by Gibbs Sampling",
    "text": "Bayesian Estimation by Gibbs Sampling\n\nUniform Random Spanning Trees\n\nMethods for sampling i.e. wilson’s and Duan’s (other? Energy paper?)\nTree Likelihoods, other facts (edge expectations)\n\n\n\nApproximate Forest Samples\n\ncomparison with LDA\nSimplifying Assumptions (conditional prob IS prob for this)\nI.e. the unwritten paper, modifying technique by [1] for RSF instead of RSTs\n\n\n[1] L. L. Duan and D. B. Dunson, “Bayesian spanning tree: Estimating the backbone of the dependence graph,” arXiv, arXiv:2106.16120, Jun. 2021. doi: 10.48550/arXiv.2106.16120.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#sec-lfa-efm",
    "href": "content/part2/2-03-latent-forest-alloc.html#sec-lfa-efm",
    "title": "LFA: Latent Forest Allocation",
    "section": "Expected Forest Maximization",
    "text": "Expected Forest Maximization\n\nAlternating Directions\n\nestimate laplacian to get \\(Q_i\\) as shortest path distance\n\n\n\ndegree normalization\nsym laplacian before forest kernel\n\n\nalgorithm overview",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part2/2-03-latent-forest-alloc.html#simulation-study",
    "href": "content/part2/2-03-latent-forest-alloc.html#simulation-study",
    "title": "LFA: Latent Forest Allocation",
    "section": "Simulation Study",
    "text": "Simulation Study\n\nScore Improvement\n\n\n\n\n\n\n\n\n\nFigure 7.1: Change in Expected MCC (EFM vs FP)\n\n\n\n\n\n\n\n\nOdds of Individual Edge Improvement\n\n\n\n\n\n\n\n\n\nFigure 7.2: Logistic Regression Coef. (EFM - FP) vs. (Ground Truth)\n\n\n\n\n\n\n\n\nTime Difference\n?",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>LFA: Latent Forest Allocation</span>"
    ]
  },
  {
    "objectID": "content/part3/3-06-qualitative.html",
    "href": "content/part3/3-06-qualitative.html",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "",
    "text": "Network Science Collaboration Network",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-06-qualitative.html#network-science-collaboration-network",
    "href": "content/part3/3-06-qualitative.html#network-science-collaboration-network",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "",
    "text": "Figure 8.1: 134 Network scientists from [NEWMAN;BOCCALETTI;SNEPPEN], connected by co-authorship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Max. likelihood tree dependency structure to explain co-authorships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Forest Pursuit estimate of NetSci collaborator dependency relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-06-qualitative.html#les-miserables-character-network",
    "href": "content/part3/3-06-qualitative.html#les-miserables-character-network",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "Les Miserables Character Network",
    "text": "Les Miserables Character Network\n\nBackboning\n\n\n\n\n\n\n\n\n\nFigure 8.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.6\n\n\n\n\n\n\n\n\nCharacter Importance Estimation\n\n\n\n\n\n\n\n\n\nFigure 8.7",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-ordered.html",
    "href": "content/part3/3-07-ordered.html",
    "title": "Recovery from Partial Orders",
    "section": "",
    "text": "Technical Language Processing\nLike before, but with the added twist of knowing our nodes were activated with a particular partial order.",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-ordered.html#technical-language-processing",
    "href": "content/part3/3-07-ordered.html#technical-language-processing",
    "title": "Recovery from Partial Orders",
    "section": "",
    "text": "insert from [1], [2]\n[1] R. Sexton and M. Fuge, “Organizing tagged knowledge: Similarity measures and semantic fluency in structure mining,” Journal of Mechanical Design, vol. 142, no. 3, Jan. 2020, doi: 10.1115/1.4045686.\n\n[2] R. Sexton and M. Fuge, “Using semantic fluency models improves network reconstruction accuracy of tacit engineering knowledge,” in Volume 2A: 45th design automation conference, American Society of Mechanical Engineers, Aug. 2019. doi: 10.1115/detc2019-98429.",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-ordered.html#verbal-fluency-animal-network",
    "href": "content/part3/3-07-ordered.html#verbal-fluency-animal-network",
    "title": "Recovery from Partial Orders",
    "section": "Verbal Fluency Animal Network",
    "text": "Verbal Fluency Animal Network\n\nEdge Connective Effiency and Diversity\n\n\n\n\n\n\n\n\n\nFigure 9.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.4: Comparison of backboning/dependency recovery methods tested vs. Forest Pursuit\n\n\n\n\n\n\n\n\nThresholded Structure Preservation\n\n\nDifferences in structural preservation with increased thresholding.\n\n\n\n\n\n\n\n\n\n\n(a) co-occurrence methods will retain local communities at the cost of global structure\n\n\n\n\n\n\n\n\n\n\n\n(b) dependency network drops rarer nodes from the preserved central structure at higher uncetainty cutoffs\n\n\n\n\n\n\nFigure 9.5: When only retaining the top 2% of edge strengths, blah\n\n\n\n\n\n\n\n\nForest Pursuit as Preprocessing\n\n\nDifferences in structural preservation with increased thresholding.\nRetaining the top 2% of edges, co-occurrence retains local communities\nat the cost of global structure.\n\n\n\n\n\n\n\n\n\n\n(a) Islands of local structure remain (doubly-stochastic)\n\n\n\n\n\n\n\n\n\n\n\n(b) Intact global structure with isolates\n\n\n\n\n\n\nFigure 9.6: We might prefer to drop low-certainty/rare nodes from a preserved central structure.",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-08-conclusion.html",
    "href": "content/part3/3-08-conclusion.html",
    "title": "Conclusion & Future Work",
    "section": "",
    "text": "Limitations of Desire Path Densities",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion & Future Work</span>"
    ]
  },
  {
    "objectID": "content/part3/3-08-conclusion.html#sec-future-fp",
    "href": "content/part3/3-08-conclusion.html#sec-future-fp",
    "title": "Conclusion & Future Work",
    "section": "Modifications and Extensions to Forest Pursuit",
    "text": "Modifications and Extensions to Forest Pursuit",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion & Future Work</span>"
    ]
  },
  {
    "objectID": "content/part3/3-08-conclusion.html#sec-future-hyperbolic",
    "href": "content/part3/3-08-conclusion.html#sec-future-hyperbolic",
    "title": "Conclusion & Future Work",
    "section": "Generalizing Inner Products on Incidences",
    "text": "Generalizing Inner Products on Incidences",
    "crumbs": [
      "Applications & Extentions",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion & Future Work</span>"
    ]
  }
]