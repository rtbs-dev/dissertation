[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Measuring Network Dependencies from Node Activations",
    "section": "",
    "text": "Foreward\n\nDISCLAIMER FOR PRIOR WORK\n\nPortions of Latent Graphs with Desire Paths, Approximate Recovery in Near-linear Time by Forest Pursuit, Modifications & Extensions, along with network figures from Qualitative Application of Relationship Recovery and Recovery from Working Memory & Partial Orders, are planned for submission to PLOS Complex Systems later this year.\nApproximate Recovery in Near-linear Time by Forest Pursuit references MENDR, a standard reference dataset and test-bench for network recovery algorithms created in the course of completing this thesis. It is awaiting DOI assignment through NIST internal review processes. Likewise for affinis, a python library for dependency inference from binary activations containing our reference implementations of various algorithms (including Forest Pursuit).\nPortions of Recovery from Working Memory & Partial Orders involving INVITE and recovery from partially ordered node sets are taken from [1] in the Journal of Mechanical Design, and its predecessor [2] from the 45th ASME Design Automation Conference.\n\n\n\n[1] R. Sexton and M. Fuge, “Organizing tagged knowledge: Similarity measures and semantic fluency in structure mining,” Journal of Mechanical Design, vol. 142, no. 3, Jan. 2020, doi: 10.1115/1.4045686.\n\n[2] R. Sexton and M. Fuge, “Using semantic fluency models improves network reconstruction accuracy of tacit engineering knowledge,” in Volume 2A: 45th design automation conference, American Society of Mechanical Engineers, Aug. 2019. doi: 10.1115/detc2019-98429.",
    "crumbs": [
      "Measuring Network Dependencies from Node Activations"
    ]
  },
  {
    "objectID": "content/00-intro.html",
    "href": "content/00-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Ambiguous Metrology\nA wide variety of fields show consistent interest in inferring latent network structure from observed interactions, from human cognition and social infection networks, to marketing, traffic, finance, and many others. [1] However, an increasing number of authors are noting a lack of agreement in how to approach the metrology of this problem. This includes rampant disconnects between the theoretical and methodological network analysis sub-communities[2], treatment of error as purely aleatory, rather than epistemic [3], or simply ignoring measurement error in network reconstruction entirely[4]. This thesis builds on recent methodological recommendations for increased focus on how dependencies should play a central role in network analysis [5], and facilitating a paradigm shift toward network analysis as inference of an inverse problem [2].\nNetworks in the “wild” rarely exist of and by themselves. Rather, they are a model of interaction or relation between things that were observed. One of the most beloved examples of a network, the famed Zachary’s Karate Club[6], is in fact reported as a list of pairwise interactions: every time a club member interacted with another (outside of the club), Zachary recorded it as two integers (the IDs of the members). The final list of pairs can be interpreted as an “edge list”, which can be modeled with a network: a simple graph. This was famously used to show natural community structure (Figure 1.1) that nicely matches the group separation that eventually took place when the club split into two.[7]\nNote, however, that we could have just as easily taken note of the instigating student for each interaction (i.e., which student initiated conversation, or invited the other to socialize, etc.). If that relational asymmetry is available, our “edges” are now directed, and we might be able to ask questions about the rates that certain students are asked vs. do the asking, and what that implies about group cohesion. Additionally, the time span is assumed to be “for the duration of observation” (did the students ever interact), but if observation time was significantly longer, say, multiple years, we might question the credulity of treating a social interaction 2 years ago as equally important to an interaction immediately preceding the split. This is now a “dynamic” graph; or, if we only measure relative to the time of separation, at the very least a “weighted” one.\nThis observation raises an interesting metrology problem: We do not know if any of these are true. “Metrology” is not limited to physical units, like “meters” and “grams”, but more generally is concerned with systematic quantification with uncertainty. Units provide a natural framework to describe what a metrologist is usually after: not just “how much”, but “how accurately and precisely that much”, as well. When we use “metrology” in the context of network analysis, we are specifically referring to the need to:\nThe difference between trueness and precision is a crucial, often overlooked distinction: how close a set of measurements are to a reference value vs. how repeatable/reproducible a measurement is[8].\nThe metrological questions we posed above are ones of _trueness_we have no way to tell if Zachary’s network model is specified correctly, because the reference network “type” is under-defined and we have no networks to compare it with. We simply have to take the network as a reference unto itself; it is a calibration artefact, much like a physical “meter rod”. However, even with an assumed perfect “trueness”, precision is often an issue as well! In fact, as illustrated in Figure 1.1, we do not know if the network being described from the original edge data even has 77 or 78 edges, due to ambiguous reporting in the original work. Lacking a precise definition of what the graph’s components (i.e., it’s edges) are, as measurable entities, means we cannot estimate the accuracy of the graph, whether for trueness or precision.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/00-intro.html#ambiguous-metrology",
    "href": "content/00-intro.html#ambiguous-metrology",
    "title": "Introduction",
    "section": "",
    "text": "[6] W. W. Zachary, “An information flow model for conflict and fission in small groups,” Journal of Anthropological Research, vol. 33, no. 4, pp. 452–473, Dec. 1977, doi: 10.1086/jar.33.4.3629752.\n\n[7] M. Girvan and M. E. J. Newman, “Community structure in social and biological networks,” Proceedings of the National Academy of Sciences, vol. 99, no. 12, pp. 7821–7826, Jun. 2002, doi: 10.1073/pnas.122653799.\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Zachary’s Karate Club, with ambiguously extant edge 78 highlighted.\n\n\n\n\n\n\n\n\nQuantify a network\nConsider the trueness of that quantification\nConsider the precision of that quantification.\n\n\n\n[8] ISO5725, “Accuracy (trueness and precision) of measurement methods and results.” International Organization for Standardization Geneva, 1994.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/00-intro.html#indirect-network-measurement",
    "href": "content/00-intro.html#indirect-network-measurement",
    "title": "Introduction",
    "section": "Indirect Network Measurement",
    "text": "Indirect Network Measurement\nWhile the karate club graph has unquantified edge uncertainty derived from ambiguous edge measurements, we are fortunate that we have edge measurements. Regardless of how the data was collected, it is de facto reported as a list of pairs, which lends itself to treatment as a reference artefact. In many cases, we simply do not have such luxury.\nInstead, edges are often measured indirectly, and instead we are given lists of node co-occurrences. Networks connecting movies as being “similar” might be derived from data that lists sets of movies watched by each user; networks of disease spread pathways might be implied from patient infection records; famously, we might build a network of collaboration strength between academic authors by mining datasets of the papers they co-author together.\nSuch networks are derived from what we will call node activation data, i.e., records of what entities happened “together”, whether contemporaneously, or in some other context or artifact. For this class, precision might be easy to assess, having oft-repeated activations.\n\n\n\n\n\n\n\n\n\nFigure 1.2: Observations as activation sets\n\n\n\n\n\n\nThese networkx are naturally represented as “bipartite” networks, having separate entites for, say, “papers” and “authors”, and connecting them with edges (paper 1 is “connected” to its authors E,H,C, etc.). But analysts are typically seeking the collaboration network connecting authors (or papers) themselves! Networks of relationships in this situation are not directly observed, but which if recovered could provide estimates for community structure, importances of individual authors (e.g. as controlling flow of information), and the “distances” that separate authors from each other, in their respective domains. [9] Common practice assumes that co-authorship in any paper is sufficient evidence of at least some level of social “acquaintance”, where more papers shared means more “connected”.\n\n[9] M. E. J. Newman, “Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality,” Physical Review E, vol. 64, no. 1, p. 016132, Jun. 2001, doi: 10.1103/physreve.64.016132.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Network based solely on co-authorship observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) graph of mutual collaboration relationships i.e. the “ground truth” social network\n\n\n\n\n\n\n\n\n\nFigure 1.3: Co-authorship vs. collaborator network\n\n\n\nThus our social collaboration network in Figure 1.3 (a) is borne out of indirect measurements: author connection is implied through “occasions when co-authorship occurred”. However, authors of papers may recall times that others were added, not by their choice, but by someone else already involved. In fact, the final author list of most papers is reasonably a result of individuals choosing to invite others, not a unanimous, simultaneous decision by all members. Let’s imagine we wished to study the social network of collaboration more directly: if we had the luxury of being in situ as, say, a sociologist performing an academic ethnography, we might have been more strict with our definition of “connection”. If the goal is a meaningful social network reflecting the strength of interraction between colleagues, perhaps we prefer that our edges represent “mutual willingness to collaborate”. Edge “measurement”, then, could involve records of events that show willingness to seek or participate in collaboration event, such as:\n\nAuthor (g) asked (e), (h), and (c) to co-author a paper, all of whom agreed\n(i) asked (f) and (j), but (j) wanted to add (b)’s expertise before writing one of the sections\netc.\n\nEach time two colleagues had an opportunity to work together and it was seized upon we might conclude that evidence of their relationship strengthened. With data like this, we could be more confident in claiming our collaboration network can serve as “ground truth,” as far as empirically confirmed collaborations go. However, even if the underlying “activations” are identical, our new, directly measured graph looks very different.\nFundamentally, the network in Figure 1.3 (b) shows which relationships the authors depend on to accomplish their publishing activity. When causal relations between nodes are being modeled as edges, we call such a graph a dependency network. We will investigate this idea further later on, but ultimately, if a network of dependencies is desired (or implied, based on analysis needs), then the critical problem remaining is how do we recover dependency networks from node activations? What is missing, once again, is any sense of reference value to base our assessment of trueness on. This thesis is primarily concerned with a metrological need within the network analysis community to have terms and techniques for describing and dealing with this problem. What goes wrong when we use co-occurrence/activation data to estimate the dependency network? What goes wrong when we wish to use co-occurrences for metrics like centrality and assortativity, or for exploratory analyses like building relationship type inventories?\n\n\n\n\n\n\n\n\n\nFigure 1.4: Recovering underlying dependency networks from node-cooccurrences.\n\n\n\n\n\n\nEven more practically, networks created directly from bipartite-style data are notorious for quickly becoming far too dense for useful analysis, earning them the (not-so-)loving moniker “hairballs”. Network “backboning,” as it has come to be called tries to find a subset of edges in this hairball that still captures its core topology in a way that’s easier to visualize.[10], [11] Meanwhile, underlying networks of dependencies that cause node activation patterns can provide this: they are almost always more sparse than their hairballs. Accessing the dependency backbone in a principled way is difficult, but doing so in a rapid, scalable manner is critical for practitioners to be able to make use of it to trim their hairballs.\n\n[10] P. B. Slater, “A two-stage algorithm for extracting the multiscale backbone of complex weighted networks,” Proceedings of the National Academy of Sciences, vol. 106, no. 26, pp. E66–E66, Jun. 2009, doi: 10.1073/pnas.0904725106.\n\n[11] Z. Neal, “The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors,” Social Networks, vol. 39, pp. 84–97, Oct. 2014, doi: 10.1016/j.socnet.2014.06.001.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/00-intro.html#scope-of-this-work",
    "href": "content/00-intro.html#scope-of-this-work",
    "title": "Introduction",
    "section": "Scope of this work",
    "text": "Scope of this work\nThe purpose of this thesis is to provide a solid foundation for edge metrology when our data consists of binary node activations, by framing network analysis as a problem of inference, as suggested by [2]. We give special focus to binary activations that occur due to spreading processes, such as random walks or cascades on an underlying carrier graph. Recovering the carrier, or, “dependency” network from node activations is of great interest to the network backboning and causal modeling communities, but often involves either unspoken sources of epistemic and aleatory error, or high computation costs (or both). To begin addressing these issues, Part I of this thesis presents a guide and review of current practices, some of their pitfalls, and how common statistical tools apply to the network recovery problem: a Practitioner’s Guide to Network Recovery. We will cover what “measurement” means in our context, and specifically the ways we encode observations, operations, and uncertainties numerically. Clarifying what different versions of what “relation” means (whether proximity or incidence) is critical, since network structure is intended to encode such relations as mathematical objects (despite common ambiguities and confusion around what practitioners intend on communicating through them). Then we organize a literature review to present a cohesive framework for assessing network recovery techniques, based on the assumptions and compromises being made to make the network inference problem tractable.\n\n[2] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\nNext, building on a gap found in the first part, Part II presents a novel method, Forest Pursuit, to extract dependency networks when we know a spreading process causes node activation (e.g. paper co-authorship caused by collaboration requests). We formalize a generic foundation for representing inferred networks as unions of observed subgraphs (Latent Graphs with Desire Paths), which we term Desire Path Density estimates. A closed form for Desire Path tree densities leads to the Forest Pursuit algorithm, (Approximate Recovery in Near-linear Time by Forest Pursuit) which scales linearly with the size of active-node sets, is trivially parallelizable and streamable over dataset size, and is agnostic to network size. We create a new reference dataset to enable community benchmarking of network recovery techniques, and use it show greatly improved accuracy over many other widely-used methods. We then extend Forest Pursuit (Modifications & Extensions) as a Bayesian probabilistic model, for which we present an Expectation Maximization scheme for posterior estimation. This improves upon the accuracy results of Forest Pursuit, at the cost of some speed and scalability, giving analysts multiple options to adapt to their needs.\nLast, in Part III we apply Forest Pursuit to several qualitative case-studies. We reconstruct scientific collaboration dataset to re-assess properties of the inferred network, and again with the classic Les Miserables character co-occurrences (Qualitative Application of Relationship Recovery). Then, we investigate network dependency recovery when partial-order information for co-occurrences are available, such as with text analysis (Recovery from Working Memory & Partial Orders), and test Forest Pursuit on a classic verbal fluency “animals” network recovery problem. Finally, we discuss more broadly the future needs of network recovery with Forest Pursuit (Conclusion & Future Work), specifically in the context of human-in-the-loop relationship annotation, hyperbolic graph embeddings, and gradient-based machine learning toolkits.\n\nOverview\nIn summary, the remainder of this thesis provides the following:\n\nMetrology with matrices and Vector representations of incidence defines common operations on binary observations, and builds a unified representation of both dependencies and co-occurrences as incidence structures and vectors.\nRoads to Network Recovery reviews current literature, and organizes it into a useful framework for needs assessment and future work.\nLatent Graphs with Desire Paths generalizes co-occurrence estimation to incorporate a priori domain information as Desire Path Density estimates of networks.\n\nApproximate Recovery in Near-linear Time by Forest Pursuit presents a new scalable algorithm for dependency recovery and validates it on a new testbench for network recovery problems\nModifications & Extensions builds on Forest Pursuit to improve its performance on certain metrics, and re-formulates it as a probabilistic model.\nQualitative Application of Relationship Recovery and Recovery from Working Memory & Partial Orders present case studies for applying forest pursuit to network analysis problems without available ground-truth networks.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html",
    "href": "content/part1/1-01-matrix-meas.html",
    "title": "Metrology with matrices",
    "section": "",
    "text": "Observation and feature “spaces”\nWhere metrology is concerned, the actual unit of observation and how it is encoded for us is critical to how analysts may proceed with quantifying, modeling, and measuring uncertainty around observed phenomena. Experiment and observation tends to be organized as inputs and outputs, or, independent variables and dependent variables, specifically. Independent variables are observed, multiple times (“observations”), and changes in outcome for each can be compared to the varying values associated with the independent variable input (“features”). For generality, say a practitioner records their measurements as scalar values, i.e., \\(x\\in\\mathbb{S}\\in\\{\\mathbb{R,Z,N},\\cdots\\}\\). The structure most often used to record scalar values of \\(n\\) independent/input variable features over the course of \\(m\\) observations is called a design matrix \\(X:\\mathbb{S}^{m\\times n}\\).1\nIf we index a set of observations and features, respectively, as \\[ i\\in I=\\{1,\\cdots,m\\}, \\quad j\\in J=\\{1,\\cdots,n\\},\\qquad I,J:\\mathbb{N}\\] then the design matrix can map the index of an observation and a feature to the corresponding measurement. \\[\nx=X(i,j)\\qquad X : I\\times J \\rightarrow \\mathbb{S}\n\\tag{2.1}\\] i.e., the measured value of the \\(j\\)th independent variable from the \\(i\\)th observation.2 In this scheme, an “observation” is a single row vector of features in \\(\\mathbb{S}^{n\\times 1}\\) (or simply \\(\\mathbb{S}^{n}\\)), such that each observation encodes a position in the space defined by the features, i.e., the feature space, and extracting a specific observation vector \\(i\\) from the entire matrix can be denoted as \\[\\mathbf{x}_i=X(i,\\cdot),\\quad \\mathbf{x}:J\\rightarrow\\mathbb{S}\\] Similarly, every “feature” is associated with a single column vector in \\(\\mathbb{S}^{1\\times m}\\), which can likewise be interpreted as a position in the space of observations (the data space): \\[\\mathbf{x}_j'=X(\\cdot,j),\\quad \\mathbf{x}':I\\rightarrow\\mathbb{S}\\] Note that this definition could be swapped without loss of generality. In other words, \\(\\mathbf{x}\\) and \\(\\mathbf{x}'\\) being in row and column spaces is somewhat arbitrary, having more to do with the logistics of experiment design and data collection. We could have measured our feature vectors one-at-a-time, measuring their values over an entire “population”, in effect treating that as the independent variable set.3\nTo illustrate this formalism in a relevant domain, let’s take another look at co-citation networks. For \\(m\\) papers we might be aware of \\(n\\) total authors. For a given paper, we are able to see which authors are involved, and we say those authors “activated” for that paper. It makes sense that our observations are individual papers, while the features might be the set of possible authors. However, we are not given information about which author was invited by which other one, or when each author signed on. In other words, the measured values are strictly boolean, and we can structure our dataset as a design matrix \\(X:\\mathbb{B}^{m\\times n}\\). We can then think of the \\(i^{\\mathrm{th}}\\) paper as being represented by a vector \\(\\mathbf{x}_i:\\mathbb{B}^n\\), and proceed using it in our various statistical models. If we desired to analyze the set of authors, say, in order to determine their relative neighborhoods or latent author communities, we could equally use the feature vectors for each paper, this time represented in a vector \\(\\mathbf{x}'_j:\\mathbb{B}^{1\\times m}\\).",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology with matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-matrix-notation",
    "href": "content/part1/1-01-matrix-meas.html#sec-matrix-notation",
    "title": "Metrology with matrices",
    "section": "",
    "text": "2  This notation is adapted from the sparse linear algebraic treatment of graphs in [1] and [2]. \n[1] J. Kepner and J. Gilbert, Graph algorithms in the language of linear algebra. Philadelphia: Society for Industrial; Applied Mathematics, 2011.\n\n[2] J. Kepner et al., “Mathematical foundations of the GraphBLAS,” in 2016 IEEE high performance extreme computing conference ( HPEC), Sep. 2016, pp. 1–9. doi: 10.1109/HPEC.2016.7761646.\n3  In fact, vectors are often said to be in the column-space of a matrix, especially when using them as transformations in physics or deep learning layers. We generally follow a one-observation-per-row rule, unless otherwise stated.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology with matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-lin-ops",
    "href": "content/part1/1-01-matrix-meas.html#sec-lin-ops",
    "title": "Metrology with matrices",
    "section": "Models & linear operators",
    "text": "Models & linear operators\nAnother powerful tool an analyst has is modeling the observation process. This is relevant when the observed data is hypothesized to be generated by a process we can represent mathematically, but we do not know the parameter values to best represent the observations (or the observations are “noisy” and we want to find a “best” parameters that account for this noise). This is applicable to much of scientific inquiry, though one common use-case is the de-blurring of observed images (or de-noising of signals), since we might have a model for how blurring “operated” on the original image to give us the blurred one. We call this “blurring” a linear operator if it can be represented as a matrix4, and applying it to a model with \\(l\\) parameters is called the forward map: \\[\\mathbf{x} = F\\mathbf{p}\\qquad F:\\mathbb{R}^{l}\\rightarrow \\mathbb{R}^n\\] where \\(P\\) is the space of possible parameter vectors, i.e., the model space. The forward map takes a modeled vector and predicts a location in data space.\n4 in the finite-dimensional caseOf critical importance, then, is our ability to recover some model parameters from our observed data, e.g., if our images were blurred through convolution with a blurring kernel, then we are interested in deconvolution. If \\(F\\) is invertible, the most direct solution might be to apply the operator to the data, as the adjoint map: \\[ \\mathbf{p} = F^H\\mathbf{x}\\qquad F^H:\\mathbb{R}^{n}\\rightarrow \\mathbb{R}^l\\] which removes the effect of \\(F\\) from the data \\(\\mathbf{x}\\) to recover the desired model \\(\\mathbf{p}\\).\nTrivially we might have an orthogonal matrix \\(F\\), so \\(F^H=F^{-1}\\) is available directly. In practice, other approaches are used to minimize the residual: \\(\\hat{\\mathbf{p}}^=\\min_{\\mathbf{p}} F\\mathbf{p}-\\mathbf{x}\\). Setting the gradient to 0 yields the normal equation, such that \\[ \\hat{\\mathbf{p}}=(F^TF)^{-1}F^T\\mathbf{x}\\] This should be familiar to readers as equivalent to solving ordinary least-squares (OLS). However, in that case it is more often shown as having the design matrix \\(X\\) in place of the operator \\(F\\).\nThis is a critical distinction to make: OLS as a “supervised” learning method treats some of the observed data we represented as a design matrix previously as a target to be modeled, \\(y=X(\\cdot,j)\\), and the rest maps parameters into data space, \\(F=X(\\cdot,J/j)\\). With this paradigm, only the target is being “modeled” and the rest of the data is used to create the operator. In the citation network example, it would be equivalent to trying to predict one variable, like citation count or a specific author’s participation in every paper, given every other author’s participation in them.\nFor simplicity, most work in the supervised setting treats the reduced data matrix as X, opting to treat \\(y\\) as a separate dependent variable. However, our setting will remain unsupervised, since no single target variable is of specific interest—all observations are “data”. In this, we more closely align with the deconvolution literature, such that we are seeking a model and an operation on it that will produce the observed behavior in an “optimal” way.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology with matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-smooth-err",
    "href": "content/part1/1-01-matrix-meas.html#sec-smooth-err",
    "title": "Metrology with matrices",
    "section": "Measurement quantification & error",
    "text": "Measurement quantification & error\nIn binary data, such as what we have been considering, it is common to model observables as so-called “Bernoulli trials”: events with two possible outcomes (on, off; yes, no; true, false), and one outcome has probability \\(p\\). These can be thought of as weighted coin-flips: “heads” with probability \\(p\\), and “tails” \\(1-p\\). If \\(k\\) trials are performed (the “exposure”), we say the number of successes \\(s\\) (the “count”) is distributed as a binomial distribution \\(s\\sim Bin(p,k)\\). The empirical estimate for the success probability is \\(\\hat{p}=\\tfrac{s}{k}\\).\nNote that this naturally resembles marginal sums on our design matrix \\(X\\), if we treat columns (or rows!) as an array of samples from independent Bernoulli trials: \\(\\hat{p}_j = \\frac{\\sum_{i\\in I} X(i,j)}{m}\\). Many probability estimates involving repeated measurements of binary variables (not simply the row/column variables) have this sort of \\(\\frac{\\textrm{count}}{\\textrm{exposure}}\\) structure, as will become useful in later sections.\nHowever, if we are “measuring” a probability, we run into issues when we need to quantify our uncertainty about it. For instance, an event might be quite rare, but if in our specific sample we never see it, we still do not generally accept a probability of zero.\n\nAdditive Smoothing\nOne approach to dealing with this involves adding pseudocounts that smooth out our estimates for count/exposure, from which we get the name “smoothing” [3, pp. 45–53]. \\[\\hat{p} = \\frac{s+\\alpha}{k+2\\alpha} \\] Adding 1 success and 1 failure (\\(\\alpha=1\\)) as pseudocounts to our observations is called Laplace’s Rule of Succession, or simply “Laplace smoothing,”5 while adding \\(\\alpha=0.5\\) successes and failures is called using Jeffrey’s Prior. It’s so-called because this pseudocount turns out to be a special case of selecting a Beta prior on the Bernoulli probability \\(p\\sim \\textrm{Beta}(\\alpha, \\beta)\\), such that the posterior distribution for \\(p\\) after our observations is \\(\\textrm{Beta}(s+\\alpha, k-s+\\beta)\\), which has the mean: \\[\nE[p|s,k]=\\frac{s+\\alpha}{k-\\alpha+\\beta}\n\\tag{2.2}\\]\n\n[3] D. Jurafsky and J. H. Martin, Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition with language models, 3rd ed. 2025. Available: https://web.stanford.edu/~jurafsky/slp3/\n5  derived when Laplace desired estimates of probability for unobserved phenomena, such as the sun (not) rising tomorrow.6  A useful comparison of the two priors (1, 0.5) is to ask, given all of the trials we have seen so far, whether we believe we are near the “end” or “middle” of an average run of trials. For \\(\\alpha=1\\), we believe nearly all evidence has been collected, but for \\(\\alpha=0.5\\), only half of expected evidence has been observed.\nThis exactly recovers additive smoothing with a Jeffrey’s prior for \\(\\alpha=\\beta=0.5\\).6 This generalization allows us to be more flexible, and specify our prior expectations on counts or exposure with more precision. Such models provide both an estimate of the aleatory uncertainty (via the posterior distribution), and a form of “shrinkage” that prevents sampling noise from unduly affecting parameter estimates (via the prior distribution). Despite being a simple foundation, this treatment of “counts” and “exposure” can be built upon in many ways.\n\n\nConditional Probabilities & Contingencies\nIn dependency/structure recovery, since our goal involves estimating (at least) pairwise relationships, the independence assumption required to estimate node occurrences as Beta-Binomial is clearly violated.7\n7  In fact, a recent method from [4] models probabilistic binary observations, with dependencies, by generalizing the mechanics overviewed here to a fully multivariate Bernoulli distribution, capable of including 3rd- and higher-order interactions (not just pairwise).\n\n[4] B. Dai, S. Ding, and G. Wahba, “Multivariate bernoulli distribution,” Bernoulli, vol. 19, no. 4, Sep. 2013, doi: 10.3150/12-bejsp10.\nHowever, it’s common to estimate how similar two random variables \\(A,B\\) are, e.g., if samples of each correspond to columns of binary \\(X\\). For instance, the joint probabilities \\(P(A\\cap B)\\) answer “how often does A happen with B, out of all data?” Conditional probabilities \\(P(A|B)=\\frac{P(A\\cap B)}{P(B)}\\) measure how often A occurs given B happened. Once again, we can estimate the base probabilities \\(P(A)\\) and \\(P(B)\\) with methods like Equation 2.2 for each marginal sums \\(X(\\cdot,A)\\) or \\(X(\\cdot,B)\\), but the joint and conditional probabilities can instead be estimated using matrix multiplication using the Gram matrix, discussed below. It encodes pair-wise co-occurrence counts, such that \\(G(i,i'):\\mathbb{Z}^{n\\times n}\\) has the co-occurrence count for node \\(i\\) with \\(i'\\).\nFor binary data, we typically create association meatures from values on a \\(2\\times2\\) contingency table, with counts and marginals. A shorthand notation for these values is:\n\\[\n\\begin{array}{c|cc|c}\n      & B=1         & B=0         & \\sum_B \\\\\n\\hline\nA=1   & p_{11}      & p_{10}      & p_{1\\bullet} \\\\\nA=0   & p_{01}      & p_{00}      & p_{0\\bullet} \\\\\n\\hline\n\\sum_A   & p_{\\bullet 1} & p_{\\bullet 0} & p_\\bullet \\\\\n\\end{array}\n\\tag{2.3}\\]\nThe co-occurrence probability \\(p_{11}=P(A\\cap B)\\) for each pair can also be approximated with the beta-binomial scheme mentioned above, but care must be taken not to confuse this with the edge strength connecting two nodes. First, nodes that rarely activate (low node probability) may nonetheless reliably connect to others when they do occur (high edge probability). In fact, without direct observation of edges, we are not able to estimate their count, or their exposure, which can be a source of systemic error from epistemic uncertainty. We don’t know when edges are used, directly, and we also don’t have a reliable way to estimate the opportunities each edge had to activate (their exposure), either. This is especially true when we wish to know whether an edge even can be traversed, i.e., the edge support. Support, as used in this sense, is the set of inputs for which we expect a non-zero output. Intuitively, this idea captures the sense that we might care more about whether an edge/dependency exists, not how important it is. For that, we have to re-assess our simple model: even if we could count the number of times an edge might have been traversed, how do we estimate the opportunities it had to be available for traversal (it’s “exposure”)?\nAssuming this kind of epistemic uncertainty can be adequately addressed through modeling—attempts at which will be discussed in more detail in Roads to Network Recovery—conditional probability/contingency tables will again be useful for validation. When comparing estimated edge probability to some known “true” edge existence (if we have that), we can count the number of correct predictions, as well as type I (false positive) and type II (false negative) errors. We can do this at every probability/weight threshold value, as well, and we will return to ways to aggregate all of these values into useful scoring metrics in Simulation Study.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology with matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-01-matrix-meas.html#sec-products",
    "href": "content/part1/1-01-matrix-meas.html#sec-products",
    "title": "Metrology with matrices",
    "section": "Distance vs. Incidence",
    "text": "Distance vs. Incidence\nAs we have already seen, operations from linear algebra make many counting and combinatoric tasks easier, while unifying disparate concepts to a common set of mechanics. In addition to having a map from integer indices to sets of interest, these design matrices/vectors are implicitly assumed to have entries that exist in a field \\(F=(\\mathbb{S},\\oplus,\\otimes)\\). equipped with operators analogous to addition (\\(\\oplus\\)) and multiplication (\\(\\otimes\\)).8 With this, we are able to define generalized inner products that take pairs vectors in a vector space \\(\\mathbf{x}\\in V\\), such that \\(\\langle\\cdot,\\cdot\\rangle_F:\\mathbb{S}^n\\times \\mathbb{S}^n\\rightarrow \\mathbb{S}\\). \\[\n\\langle\\mathbf{x}_a,\\mathbf{x}_b\\rangle_{F} = \\bigoplus_{j=1}^n \\mathbf{x}_a(j)\\otimes\\mathbf{x}_b(j)\n\\]\n8  Or, more generally, a semiring if inverse operations for \\(\\oplus,\\otimes\\) don’t exist.We can use this to perform “contractions” along any matching dimensions of matrices as well, since the sum index is well-defined. \\[\n\\begin{aligned}\nX\\in\\mathbb{S}^{m\\times n}\\quad Y\\in\\mathbb{S}^{n\\times m} \\\\\nZ(i,j)=X\\oplus,\\otimes Y = \\bigoplus_{j=1}^{n} X(i,j) \\otimes Y(j,k) = XY\n\\end{aligned}\n\\] For ease-of-use, we will assume the standard field for any given set \\((\\mathbb{S},+,\\times)\\) if not specified otherwise, which recovers standard inner products \\(\\langle\\cdot,\\cdot\\rangle\\). Still, the idea that matrix products can be generalized will allow us to “stay” in the correct (binary) domain under contraction, which will be an issue dealt with again in Problem Specification. It might be of interest to a reader to refer to an entirely linear-algebraic formulation of graph theory in [2], which also illustrates the usefulness of various fields (or semirings). They allow linear-algebraic representation of many graph operations, such as shortest paths through inner products over \\((\\mathbb{R}\\cup -\\infty,\\textrm{min}, +)\\). This works because discrete/boolean edge weights will not accumulate extra strength beyond 1 under contraction over observations.\n\nKernels & distances\nAs alluded to in the previous section, co-occurrence have a deep connection to a Gram matrix, which is a matrix of all pairwise inner products over a set of vectors.\n\\[\nX^TX=G(j,j')=\\langle\\mathbf{x'}_j,\\mathbf{x}_{j'}'\\rangle = \\sum_{i=1}^{m} X(i,j)X(i,j')\n\\tag{2.4}\\]\nMatrices that can be decomposed into another matrix and its transpose are symmetric, and positive semidefinite (PSD), making every gram matrix PSD. They are directly related to (squared) euclidean distances through the polarization identity:9 \\[\nd^2(j,j') = \\|\\mathbf{x}_{j'}'-\\mathbf{x}_{j'}'\\|^2 = G(j,j) - 2G(j,j') + G(j',j')\n\\tag{2.5}\\]\n9 Important: these definitions are all using the \\(\\mathbf{x}'\\) notation to indicate that these measurements are almost exclusively being done in the data space, i.e., on column vectors. While most definitions work on distances in terms of the measurements/objects/data, for inverse problems (like network recovery, structure learning, etc.) they are more often applied in terms of the features (here, the nodes. This can be seen in statistics as well, where covariance and correlation matrices (which are related to the gram and distance matrix definitions above), are defined as relationships between features/dimensions, not individual samples.\n[5] T. Zhou, J. Ren, M. s. Medo, and Y.-C. Zhang, “Bipartite network projection and personal recommendation,” Phys. Rev. E, vol. 76, no. 4, 4, p. 046115, Oct. 2007, doi: 10.1103/PhysRevE.76.046115.\n\n[6] M. Coscia, The atlas for the aspiring network scientist. Michele Coscia, 2021. Available: https://arxiv.org/abs/2101.00863\nIn our example from before, the gram matrix will have entries showing the number of papers shared by two authors (or total papers by each, on the diagonal). This is because an inner product between two author (column) vectors will add 1 for each paper in the sum only if it has both authors in common. This is called a bipartite projection[5], [6] into the authors “mode”, and is illustrated visually in Figure 1.3 (a).\nDue to [7], we can generalize Equation 2.5 such that any function “kernel” function \\(\\kappa(x,y)\\) that creates PSD matrix \\(K(j,j')\\in\\mathbb{S}^{n\\times n}\\). It says that such a PSD matrix can always be decomposed into a form \\(K=R^TR\\) for any matrix \\(R(i,j)\\in \\mathbb{S}^{m\\times n}\\), thus letting us use the polarization identity to create arbitrary distance metrics. on \\(\\mathbb{S}^n\\) [8]10 \\[\nd_K(j,j') = \\tfrac{1}{2}\\left(K(j,j)+K(j',j'))\\right)-K(j,j')\n\\tag{2.6}\\]\n\n[7] I. J. Schoenberg, “Metric spaces and positive definite functions,” Transactions of the American Mathematical Society, vol. 44, no. 3, pp. 522–536, 1938, doi: 10.1090/s0002-9947-1938-1501980-0.\n\n[8] K. Avrachenkov, P. Chebotarev, and D. Rubanov, “Similarities on graphs: Kernels versus proximity measures,” European Journal of Combinatorics, vol. 80, pp. 47–56, Aug. 2019, doi: 10.1016/j.ejc.2018.02.002.\n10  Distance metric, here, means that \\(d(x,y)\\) satisfies the triangle inequality for all \\(x,y\\).\n[9] S. Theodoridis and K. Koutroumbas, Eds., Pattern recognition, 4th ed. Burlington, MA: Academic Press, 2010.\nThis ability to create valid distance measures from arbitrary kernel functions is the core of a vast area of machine learning and statistics that employs the so-called kernel trick. [9] Different kernels yield different properties useful for distinguishing points having specific properties. One class of kernels are normalized to the range \\([0,1]\\), such that we ensure that equality along any one dimension is given a weight of \\(\\tilde{K}(j,j)=1\\). Such a kernel matrix can be derived from any other kernel, and is often combined with a logarithmic similarity measure \\(\\tilde{\\kappa}(x,y)=\\log{s(x,y)}\\). \\[\n\\begin{aligned}\n\\tilde{K}(j,j') &= \\frac{K(j,j')}{\\sqrt{K(j,j)^2K(j',j')^2}}\\\\\nd_{\\tilde{K}}(j,j') &= -\\log{\\tilde{K}(j,j')}\n\\end{aligned}\n\\tag{2.7}\\] Since this is equivalent to applying Equation 2.6 to \\(s\\) directly. This normalization should be familiar as the way cosine similarities and correlation matrices are made as well (also having 1s along their diagonal), and illustrates how non-metric similarities can be potentially made into (pseudo)metrics.\n\n\nIncidence structures & dependency\nRather than how “close” or “far” to points are in vector space, which is described with the kernels and distances above, whether something “touches”—or, is incident to—something else is usually described abstractly as an incidence structure. This is an abstract way to describe how things “touch”, such as when a set of points lie on a line/plane, or nodes touch an edge. We say an incidence structure is a triple of sets called (for historical reasons) points \\(\\mathfrak{P}\\), lines \\(\\mathfrak{L}\\), and flags \\(\\mathfrak{I}\\).[10] \\[(\\mathfrak{P,L,I})\\quad \\mathfrak{I}\\subseteq \\mathfrak{P}\\times\\mathfrak{L}\\]\n\n[10] G. E. Moorhouse, “Incidence geometry,” University of Wyoming, pp. 3–5, 2007.\nRepresenting these as matrices will be further explored in Graphs as incidence structures. But, the discrete nature of these incidence sets makes it clear that estimating the size and elements of \\(\\mathfrak{I}\\), is a very different question from estimating the similarity/distance between two entities in a vector space.\nIn statistics, such discrete structures usually arise when we wish to distinguish direct dependence from indirect. Take as an example a set of masses connected together by springs. If we shake one mass, all masses will also shake some amount, depending on the spring constants of the springs each mass is “transmitted” force through, and the losses due to friction or air resistance. While the amount of movement over time depends on how “close” in this spring network two masses are, the movement itself can only be transmitted through springs that the masses are incident to. Movement could be modeled through similarity/distance measurements like correlation, since none of the masses are independent (all move when any do), but incidence in terms of spring force transmission is modeled in terms of conditional (in)dependence. If we hold all but two masses still, and moving one doesn’t move the other, then we know they are conditionally independent: no spring connects them!\n\n\n\n\n\n\n\n\n\nFigure 2.1: Spring system as a network of conditional dependencies\n\n\n\n\n\n\nThis idea gets formalized as probabilistic graphical models, which are networks that define “incidence” between two variables as conditional dependence. Letting the random variables in the column-space of \\(X\\) be \\(A,B\\) and the remaining columns be \\(C=X(\\cdot,J\\setminus \\{A,B\\})\\), then \\[\nP(A\\cap B |C ) = P(A|C)P(B|C)\\implies (A\\perp B|C)\\implies (A,B)\\notin \\mathfrak{I}\n\\tag{2.8}\\] for a set of incidences \\(\\mathfrak{I}\\) defining a PGM that was sampled as \\(X\\).",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metrology with matrices</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html",
    "href": "content/part1/1-02-graph-obs.html",
    "title": "Vector representations of incidence",
    "section": "",
    "text": "Graphs as incidence structures\nTo provide a sufficiently rigorous foundation for network recovery from binary occurrence, we will need a rigorous way to represent networks and occurrences that lends itself to building structured ways both connect to each other. We build on the incidence structure and matrix product formalism from the previous chapter, introducing various ways to build graphs as incidence structures that have direct representations as matrices. This can be extended to representing occurrences as matrices of hyperedge vectors. This view allows us to interpret different representations of graphs (or hypergraphs) as connected by simple matrix operations.\nTraditionally[1], [2], we might say a graph on nodes (or, “vertices”) \\(v\\in V=\\{1,\\cdots,n\\}\\) and “edges” \\(E\\) is a tuple: \\[\nG=(V,E) \\quad \\textrm{s.t.} \\quad E\\subseteq V \\times V\n\\]\nIn incidence geometry terms, this would be similar to making two duplicate sets of the same nodes, and defining a graph as the set of incidences between nodes. The adjacency matrix \\(A\\) of \\(G\\), degree matrix \\(D\\), and graph/discrete Laplacian \\(L\\) are then defined as:1 \\[\n\\begin{aligned}\nA(u,v) & =\\mathbf{1}_E((u,v)) \\quad &A : V\\times V\\rightarrow \\mathbb{B} \\\\\nD(u,v) & =\\mathrm{diag}({\\textstyle\\sum}_V A(u,v))\\quad &D : V\\times V\\rightarrow \\mathbb{N} \\\\\nL(u,v) & = D(u,v) - A(u,v) \\quad &L : V\\times V\\rightarrow \\mathbb{Z}\n\\end{aligned}\n\\]\nHowever, if edges and their recovery is so important to us, defining them explicitly as nodes-node incidences can be problematic when we wish to estimate edge existence (or not), given noisy pairs of node co-occurrences. Additionally, we have to be very careful to distinguish whether our graph is (un)directed, weighted, simple, etc., and hope that the edge set has been filtered to a subset of \\(N\\times N\\) for each case. Instead, we propose a less ambiguous framework for vectorizing graphs, based on their underlying incidence structure.\nInstead, we give edges their own set of identifiers, \\(e\\in E=\\{1,\\cdots \\omega\\}\\). Now, define graphs as incidence structures between nodes and edges such that every edge is incident to either two nodes, or none:\n\\[\nG = (V,E,\\mathcal{I}) \\quad s.t. \\quad \\mathcal{I} \\subseteq E\\times V\n\\tag{3.1}\\]\nVariations on graphs can often be conveniently defined as constraints on \\(\\mathcal{I}\\):\nTogether, these constraints define “simple” graphs. Similarly, we can equip Equation 3.1 with a function \\(B\\) that allows \\(\\mathcal{I}\\) to encode information about the specific kinds of incidence relations under discussion. We give \\(B\\) a range of possible flag values \\(S\\):\n\\[\nG = (V,E,\\mathcal{I},B) \\quad s.t. \\quad \\mathcal{I} \\subseteq E\\times V\\quad B:\\mathcal{I}\\rightarrow S\n\\tag{3.2}\\]\nIf the “absence” of incidence needs to be modeled explicitly, a “null” stand-in (0,False) can be added to each \\(S\\), which is useful for representing each structure as arrays for use with linear algebra (i.e., \\(\\{0,1\\}\\),\\(\\{-1,0,-1\\}\\),\\(\\mathbb{R}^+_0\\), and \\(\\mathbb{R}\\), respectively). By doing so, we can also place an exact limit on the maximum possible size of \\(\\omega=\\|E\\|\\) in the simple graph case, and indicate edges by their unique ID, such that \\(\\mathcal{I}= E\\times V\\) is no longer a subset relation for \\(E=\\{1,\\cdots,{n\\choose2} \\}\\). Instead of existence in \\(\\mathcal{I}\\), we explicitly use incidence relation \\(S\\) to tell us whether each possible edge “exists” or not, simplifying our graph definition further4:\n\\[\n\\begin{gathered}\nG  = (V,E,B) \\quad s.t. \\quad B : E\\times V \\rightarrow S\\\\\nv \\in V = \\{1,\\cdots, n\\}\\quad \\\\\ne \\in E = \\left\\{1,\\cdots, {n\\choose 2} \\right\\}\n\\end{gathered}\n\\tag{3.3}\\]\nThe representation of \\(B\\) in Equation 3.3 bears a remarkable similarity to our original description of design matrices in Equation 2.1. In fact, as a matrix, \\(B(e,v)\\) is called the incidence matrix: every row has two non-zero entries, with every column containing a number of non-zero entries equal to that corresponding node’s degree in \\(G\\). Traditionally, we use an oriented incidence matrix, such that each row has exactly one positive (non-zero) value, and one negative (non-zero) value.5 Even for undirected graphs, the selection of which entry is positive or negative is left to be ambiguous, since much of the math used later is symmetric w.r.t. direction6.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector representations of incidence</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html#sec-incidence-vec",
    "href": "content/part1/1-02-graph-obs.html#sec-incidence-vec",
    "title": "Vector representations of incidence",
    "section": "",
    "text": "Self loops can be prohibited by only allowing unique flags for a given relation2\nMultigraphs are similarly described by whether we allow pairs of vertices to appear with more than one edge3\n\n2  never two flags with the same pair, i.e., \\(\\mathcal{I}\\) is a set, not a multiset.3  in the set of flags containing nodes \\(u\\) or \\(v\\), only one \\(e\\) may be incident to both of them.\n\n\nUndirected, unweighted graphs only need single elements: “incidence exists” i.e., \\(S=\\{1\\}\\)\nDirected graphs can use two elements e.g., a “spin” for \\(S=\\{-1,1\\}\\)\nWeighted, undirected graphs are supported on positive scalars e.g., \\(S=\\mathbb{R}^+\\)\nWeighted, directed graphs are supported on any scalar e.g., \\(S=\\mathbb{R}_{\\neq0}\\)\n\n\n4  if we allomulti-edges , then \n\n5  In fact, this would make B^*(v,e) equivalent to a graphical matroid, another common formalism that generalizes graph-like structures to vector space representations.6 though not always!\nEmbedding incidences in vector space\nA formalism for graphs that starts with incidence matrices would benefit from a canonical oriented incidence matrix, rather than the family that is ambiguous w.r.t. edge orientation. To start, we can be more precise by selecting each row(edge) vector, and partitioning it into two: one for each non-zero column (node) that edge is incident to. Every incidence can be represented individually as standard basis vector \\(\\mathbf{e}\\) in the feature space of \\(B\\), scaled by the corresponding value of \\(S\\).\nLet \\(V_e\\) be the set of nodes with (non-zero) incidence to edge \\(e\\). Then the incidence vectors are:\n\\[\n\\delta_e(v) = B(e,v)\\mathbf{e}_v \\quad \\forall v\\in V_e\n\\tag{3.4}\\] And the (unoriented) incidence matrix vectors are recovered as sums over the incidence vectors for each edge: \\[\n\\mathbf{b}_e = \\sum_{v\\in V_e} \\delta_e(v)\n\\tag{3.5}\\]\nA traditional approach might then define undirected graphs as equivalent, in some sense, to multidigraphs, where each edge is really two directed edges, in opposing directions. This does allow the matrix \\(B\\) to have the correct range for its entries in this formalism (the directed graph range, \\(S=\\{-1,0,1\\}\\)), and the edge identity based on sums would hold. However, the resulting set of incidences would have twice the number of edges than our combinatoric limit for simple graphs, and prevent the more elegant definition of graph types through the set \\(\\mathbf{S}\\). Plus, it would necessitate averaging of weights over different edge ID’s to arrive at a single undirected “edge weight”, and many other implementation details that make keeping track of specifics difficult for practitioners.\nInstead, we would like a canonical oriented distance matrix, which can be derived from the vectorized incidences in the undirected range of \\(B\\) (the standard basis vectors). Without loss of generality, let \\(u_e,v_e\\in V_e\\) be nodes such that \\(u&lt;v\\).7 Using this, we can unambiguously define a partition \\(B(e,\\cdot)=B(e,u_e) + B(e,v_e)\\) between incidences on \\(e\\), along with a new derived incidence, \\(B_o\\), which has oriented rows like: \\[B_o(e,\\cdot)=\\mathbf{b}^o_e = \\delta_e(u)-\\delta_e(v)\\] In other words, while the unoriented incidence matrix is the “foundational” representation for graphs in our formalism, the (canonical) oriented one can be derived, even if negative incidence values are not in \\(\\mathbb{S}\\).8\n7 the inequality is strict because self-loops are not allowed.8  This works as long as we are in at least a ring, since semirings in general do not need to define additive inverse operations. In this case we would limit ourselves to the oriented incidence.\n[3] M. Angeletti, J.-M. Bonny, and J. Koko, “Parallel Euclidean distance matrix computation on big datasets,” 2019. Available: https://hal.science/hal-02047514\nBut, now that we have removed the information on “which nodes an edge connects” from our definition of edges (since every edge is a scalar ID), how do we construct \\(V_e\\) without a circular dependency on \\(B\\) to find non-zero entries? Because of our unique identification of edges up to the combinatoric limit, we can still actually provide a unique ordering of the nodes in \\(V_e\\), without searching over the entirety of \\(B\\)’s domain. Using an identity from [3], we have a closed-form equation both to retrieve the IDs of nodes \\(u,v\\) (given an edge \\(e\\)), and an edge \\(e\\) (given two nodes \\(u,v\\)), for any simple graph with \\(n\\) vertices. \\[\n\\begin{gathered}\n    u_n(e) = n - 2 - \\left\\lfloor\\frac{\\sqrt{-8e + 4n(n - 1) - 7}-1}{2}\\right\\rfloor\\\\\n    v_n(e) = e + u_n(e) + 1 -\\frac{1}{2} \\left(n (n - 1) + (n - u_n(e))^2 - n+u_n(e)\\right)\\\\\n    e_n(u,v) = \\frac{1}{2}\\left(n(n - 1) - ((n - u)^2 - n+u)\\right) + v - u - 1\n\\end{gathered}\n\\tag{3.6}\\] Our ease-of-calculation lets us drop some of the excess notation and refer to our (un)oriented incidence matrices in terms of the incidences of each edge on their \\(u\\) or \\(v\\), directly. \\[\nB = B_u + B_e \\qquad B_o \\equiv B_u - B_v\n\\]\n\n\n\nInner products on \\(B\\)\nWith all of this background, the other representations of graphs can be seen as derivations from the canonical incidence matrices. The Laplacian, which is usually introduced either in terms of ajacency/degree, or as the gram matrix for oriented edge vectors, is also related to the gram matrix between all pairs of incidences on \\((u,v)\\). The other identities are simply consequences of the polarization identity: \\[\n\\begin{split}\nL & = B_o^TB_o\\\\\n  & = \\|B_u - B_v \\|^2 \\\\\n  & = 2\\|B_u\\|^2 +2\\|B_v\\|^2 - \\|B_u + B_v \\|^2 \\\\\n  & = 2D - B^TB = D-A\n\\end{split}\n\\tag{3.7}\\]\n\n\n\n\n\n\n\n\n\n\n\nThe Laplacian is often used in defining random-walks and markov chains, such that the degree of each node should be normalized to 1, which can be accomplished either by row- or column-normalizing it: \\(L^{\\textrm{rw}}=D^{-1}L\\) or \\(LD^{-1}\\), respectively. If this degree-normalization is desired without de-symmetrizing \\(L\\), we can still perform an operation similar to normed kernels in Equation 2.7, called the symmetric normalized Laplacian. \\[\n\\tilde{L} = D^{-\\tfrac{1}{2}}LD^{-\\tfrac{1}{2}} = \\frac{L(u,v)}{\\sqrt{L(u,u)^2L(v,v)^2}}\n\\tag{3.8}\\]\n\n\nEdge Metrology, Edge Vectors\nImplicitly in the use of \\(B\\) with design matrix mechanics from the previous chapter is a treatment of edges as “observations” (the data space), and nodes as features. If an edge is an observation, then unfortunately we cannot really quantify uncertainty over repeated measurements of edges with the simple mechanics from Additive Smoothing (because that edge is that corresponding observation, and IDs are not duplicated).\nSo far we have seen two ways of representing the entire graph in matrix form: Incidence matrix \\(B\\) (or \\(B_o\\)), and the inner-product matrices derived from it (\\(L\\), \\(A\\)). Since we can recover node IDs from edge IDs by Equation 3.6, we can use a single vector to represent an entire graph structure by it’s edges alone. Then a data dimension for vectors in “edge space” can once again represent observations, with nodes implied by Equation 3.6. This is either done by contracting \\(B\\) along the nodes (columns) dimension, or by unrolling the upper triangle of \\(L\\) or \\(A\\) according to Equation 3.6.9 If each vector represents a value of \\(B\\) associated with a corresponding edge, then \\(m\\) of these vectors would be equivalent to \\(m\\) observations of \\({n \\choose 2}\\) edges on the same set of \\(n\\) nodes. Formally, for the \\(i\\)th observed structure on \\(n\\) nodes: \\[\n\\begin{gathered}\nR(i,e) = \\min(\\{B_i(e, u_n(e)),B_i(e,v_n(e))\\}) \\\\\n\\quad \\textrm{s.t.} \\quad R:I\\times E \\rightarrow \\mathbb{S}\\\\\ni\\in I = \\{1,\\cdots,m\\} \\qquad e \\in E=\\left\\{1,\\cdots,\\omega\\right\\}\\\\\nn=\\tfrac{1}{2}(1+\\sqrt{8\\omega+1})\n\\end{gathered}\n\\tag{3.9}\\] This representation formalizes what practitioners call “edgelists” into a data structure that can unambiguously distinguish directed, undirected, and weighted graphs. In addition, it allows for repeated measurements of edges over the same set of nodes, while flexibly growing when new nodes arrive.10\n9  Equation 3.9 uses an averaging operation to accomplish the contraction, but any reduction over the two nodes shared by an edge would accomplish the same, especially since we rarely see separate values for edge weight per-node, the way incidences can.10  For instance, say observations are stored as sparse entries via \\(R\\), and a new node arrives. First, the participating nodes can be recovered in a vectorized manner through Equation 3.6. Then, a new node id increases \\(n\\), followed by reassignment of the edge IDs with \\(e_n(u,v)\\).",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector representations of incidence</span>"
    ]
  },
  {
    "objectID": "content/part1/1-02-graph-obs.html#node-activation-bipartite-graphs-and-hypergraphs",
    "href": "content/part1/1-02-graph-obs.html#node-activation-bipartite-graphs-and-hypergraphs",
    "title": "Vector representations of incidence",
    "section": "Node activation, bipartite graphs, and hypergraphs",
    "text": "Node activation, bipartite graphs, and hypergraphs\nWhat if an incidence structure allows for more than two incidences for the “line” set? In our binary design matrix, we might consider each observation its own “line”, such that it is incident to all the activated nodes. This is no longer a graph of edges and nodes, but rather a more general object.\n\nEvery incidence structure can be seen as an incidence matrix, which additionally can be thought of as a bipartite graph. In this sense, the incidence matrix is thought of as a bi-adjacency matrix, which is a subset of a larger adjacency having two off-diagonal non-zero blocks\n\\[\nA_{BP} =\n\\begin{pmatrix}\n0_{n,n} & X^T \\\\\nX & 0_{m,m}\n\\end{pmatrix}\n\\]\nThe graph having this adjacency structure has two sets of nodes that do not intraconnect (ergo, “bipartite”). The resulting structure for our toy example is shown next to the incidence matrix in Figure 3.1.\n\n\n\n\n\n\n\n\n\\[\n%X(\\{1,2,3,4,\\cdots\\})=\\\\\n\\begin{array}{c c}\n& \\begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\\\ \\end{array} \\\\\n\\begin{array}{c c } x_1\\\\x_2\\\\x_3\\\\x_4\\\\ \\vdots \\end{array} &\n\\left[\n\\begin{array}{c c c c c c c c c c}\n  0  &  0  &  1  &  0  &  1  &  0  &  1  &  1  &  0  &  0 \\\\\n  1  &  0  &  0  &  0  &  1  &  1  &  0  &  0  &  0  &  0 \\\\\n  0  &  1  &  0  &  0  &  0  &  1  &  0  &  0  &  1  &  1 \\\\\n  0  &  0  &  0  &  1  &  1  &  0  &  0  &  1  &  0  &  0 \\\\\n  &&&& \\vdots &&&&&\n\\end{array}\n\\right]\n\\end{array}\n\\]\n\n\n(a) : \\(X\\) as a (bipartite) incidence matrix.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Bipartite representation of node “activation” data\n\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\nWhen the set of lines is a family of subsets of points, incidences on points and lines form a hypergraph. Hypergraphs are usually thought of as graphs where edges can connect more than two nodes, which again can be made into an incidence structure in the same way our graphs are. This isomorphism lets many of the familiar ideas on graphs (e.g., walks, paths, laplacians, etc.) be reformulated in terms of hypergraphs. For a more in-depth elaboration on algebraic graph theory on graphs, see [4].\n\n[4] N. Reff and L. J. Rusnak, “An oriented hypergraphic approach to algebraic graph theory,” Linear Algebra and its Applications, vol. 437, no. 9, pp. 2262–2270, Nov. 2012, doi: 10.1016/j.laa.2012.06.011.\n\nInner product on Hyperedges\nRather than go into similar detail about hypergraphs, we want to focus on the implication of performing inner product calculations with vectors of the hypergraph. \\(\\mathbf{x}_i^T\\mathbf{x}_{i'}\\) will not necessarily result in a binary value, but a sum over all shared nodes in each edge. As mentioned previously, the Gram matrix for \\(X\\) will count co-occurrences in the off-diagonals, with marginal counts on the diagonal. This means that the relation between edge-vector entries and node-activations is not so straight forward: each entry has a “magnitude”, so forcing a binary edge activation from the adjacency or laplacian forms (using Equation 3.6) will necessarily lose information. This information loss is one of the key precautions practitioners are advised to consider in [2], since transformation between any of the representations for complex systems will change the information encoded by them.\n\n[2] L. Torres, A. S. Blevins, D. Bassett, and T. Eliassi-Rad, “The why, how, and when of representations for complex systems,” SIAM Rev., vol. 63, no. 3, pp. 435–485, Jan. 2021, doi: 10.1137/20M1355896.\n\n\nCombining Occurrence & Dependence\nAs an aside, there are several ways that known graph structures can be exploited to regularize distance or association measures for hypergraphic data. Graphs of dependency information can provide useful kernels on the graph.11 For an in-depth analysis of common kernels on graphs, including the now-famous PageRank, Heat, and Commute-time kernels, see [5]. In [6] the “commute time” kernel is said to generate a “generalized euclidean distance”, and using an adjacency matrix kernel before normalization by marginals gives what is known as “soft cosine” measures, which incorporate prior knowledge e.g., for semantic similarities between words.\n11  Note that a “kernel on a graph” works by defining distances between weighted vectors of nodes. A “graph kernel,” on the other hand, describes a kernel that defines distances between different graphs. \n[5] K. Avrachenkov, P. Chebotarev, and D. Rubanov, “Similarities on graphs: Kernels versus proximity measures,” European Journal of Combinatorics, vol. 80, pp. 47–56, Aug. 2019, doi: 10.1016/j.ejc.2018.02.002.\n\n[6] M. Coscia, “Generalized euclidean measure to estimate network distances,” in Proceedings of the international AAAI conference on web and social media, Association for the Advancement of Artificial Intelligence (AAAI), 2020, pp. 119–129. doi: 10.1609/icwsm.v14i1.7284.\nImportantly, each of these methods assume that a network structure is already available. What can often happen in practice, however, is the use of a filtered/thresholded version of the hypergraphic Gram matrix as the network, because no a priori network is available. This runs the risk of over-estimating the proximity of nodes in similar regions of node-space, reinforcing local correlations at the expense of long-distance path estimates. A more formalized analysis of this problem, which we term “clique bias,” can be found in The Gambit of the Inner Product.\nAs covered in the introduction, finding the underlying dependency network from the hypergraphic data is the core concern of network recovery. Reliably recovering this structure would then allow for less biased use of these graph kernels in real-life application.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Vector representations of incidence</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html",
    "href": "content/part1/1-03-recovery-road.html",
    "title": "Roads to Network Recovery",
    "section": "",
    "text": "Organizing Recovery Methods\nHere we give a brief overview of the key approaches to backboning and dependency recovery for networks through binary activations. We organize the literature into categories based on the kinds of constraints that have been applied to make the network reconstruction “inverse problem” tractable: local structure, information-flow/resource constraints, or global structure. Finally, we assess patterns in the assumptions made by the presented algorithms, and motivate the need for a new approach to fill a perceived gap in the network recovery space.\nAll recovery methods will require assumptions in addition to data to accomplish their task. As discussed in [1], there are fundamental difference in dependency capability between the network and hypergraphic/bipartite representations of complex systems. Necessarily, some information will be lost in translation between the two forms.\nAs a (hopefully) helpful way to organize the various kinds of assumptions that are taking bipartite observations to simple graphs, we present an organization of common modeling assupmtions into three loosely-defined groups:\nIn truth, this classification should be viewed as more of a sliding scale, with approaches falling somewhere within. Some approaches make very few assumptions about the shape a resulting network “should” take, but do so by making strong assumptions about how individual observations relate to a desired quantity, and especially how those observations are able to be combined to result in an “answer”, however that looks. Others instead provide clear normative constraints on the overall network topology, or emission mechanism, but this allows for flexibility in how data is individually handled.\nThis distinction could be thought of as a scale, which serves a role similar to pooling in bayesian inference. Do individuals (observations) all have fundamentally separate distributions, so that global behavior (and by extension, uncertainty) is an aggregate phenomena? Or do individuals (observations) inherit parameters from a global distribution shared by all, and anything outside that structural assumption must be “noise”? In between the extremes, some other assumption as to how the global and local scales mitigate information between them is required, i.e., partial pooling. In this domain, what we often see are attempts to perform noise corrections through the way information is thought to travel between nodes, generally.\nFor each of the above three groups we provide examples to illustrate modeling patterns and highlight common practice.1",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#organizing-recovery-methods",
    "href": "content/part1/1-03-recovery-road.html#organizing-recovery-methods",
    "title": "Roads to Network Recovery",
    "section": "",
    "text": "[1] L. Torres, A. S. Blevins, D. Bassett, and T. Eliassi-Rad, “The why, how, and when of representations for complex systems,” SIAM Rev., vol. 63, no. 3, pp. 435–485, Jan. 2021, doi: 10.1137/20M1355896.\n\n\nLocal Structure & Additivity\nInformation Flow & Resource Constraints\nGlobal Structure Models\n\n\n\n\n1  For a deeper assessment of the broad space of backboning and edge prediction in general a reader may be interested to see the overview in [2].\n[2] M. Coscia, The atlas for the aspiring network scientist. Michele Coscia, 2021. Available: https://arxiv.org/abs/2101.00863\n\nLocal Structure & Additivity Assumptions\nThese are, together, typically called “association” measures. While they are sometimes presented as functions of the entire dataset, they nearly always find a basis in the inner-product operation, and have definitions in terms of contractions along the data/observation dimension. By relying on the (Euclidean) inner product, even with various re-weighting or normalization schemes, an analyst is making strong assumptions about their ability to reliably take measurements from linear combinations of observed activation vectors.\nEssentially, if a measure relies on marginal counts or summation over the data axis (\\(\\mathbf{s}\\)), then the main assumptions are at the local level, about whether what we are adding together estimates our target correctly. The most basic would be to count co-occurrences, and consequently the co-occurrence probability \\(p_{11}=P(A,B)\\). However, for very rare co-occurrences, we need to correct for rate-imbalance of the nodes in much the same way correlation normalizes covariance. This idea leads to “cosine similarity”\n\n\n\n\n\n\nNote 4.1: Ochiai Coefficient (Cosine)\n\n\n\nEffectively an uncentered correlation, but for binary observations the “cosine similarity” is also called the Ochiai Coefficient between two sets \\(A,B\\), where binary “1” stands for an element belonging to the set.[3] In our use case, it is measured as \\[\n\\frac{|A \\cap B |}{\\sqrt{|A||B|}}=\\sqrt{p_{1\\bullet}p_{\\bullet 1}} \\rightarrow  \\frac{X^TX}{\\sqrt{\\mathbf{s}_i\\mathbf{s}_i^T}}\\quad \\mathbf{s}_i = \\sum_i \\mathbf{x}_i\n\\]\n\n\n\n[3] S. Janson and J. Vegelius, “Measures of ecological association,” Oecologia, vol. 49, no. 3, pp. 371–376, Jul. 1981, doi: 10.1007/bf00347601.\n\n[4] D. Allard, A. Comunian, and P. Renard, “Probability aggregation methods in geoscience,” Math Geosci, vol. 44, no. 5, pp. 545–581, Jul. 2012, doi: 10.1007/s11004-012-9396-3.\nThis interpretation of cosine similarity as the geometric mean of conditional probabilities is particularly useful when trying to approximate interaction rates. The geometric mean as a pooling operator is conserved through Bayesian updates [4], so the use of a prior with co-occurrences as base counts is possible for additive smoothing. To do this, the goemetric mean of marginal counts acts as a “psedovariable” for exposure somewhere between A and B. Empirically, this is a powerful approximation with good performance characteristics, for relatively little effort.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote 4.2: Odds Ratios\n\n\n\nAlong with others derived from it, the Odds ratio is based on the ratio of conditional probabilities, and takes the form \\[\\text{OR}=\\frac{p_{11}p_{00}}{p_{01}p_{10}}\\] Yule’s Q and Y [5] are mobius transforms of the (inverse) \\(\\text{OR}\\) and \\(\\sqrt{\\text{OR}}\\), respectively, that map association values to \\([-1,1]\\). \\[Q = \\frac{\\text{OR}-1}{\\text{OR}+1}\\quad Y=\\frac{\\sqrt{\\text{OR}}-1}{\\sqrt{\\text{OR}}+1}\\]\n\n\n\n[5] G. U. Yule, “On the methods of measuring association between two attributes,” Journal of the Royal Statistical Society, vol. 75, no. 6, p. 579, May 1912, doi: 10.2307/2340126.\nOdds ratio is important to logistic regression, where the coefficients are usually the log-odds ratios of occurrence vs. not (\\(\\log{\\text{OR}}\\)).\nYule’s Y, also called the “coefficient of colligation”, tends to scale with association strength in an intuitive way, so that proximity to 1 or -1 paints a more useful picture than the odds-ratio alone.\nAnother association measure, based in information theory, asks “how much can I learn about one variable by observing another?”\n\n\n\n\n\n\nNote 4.3: Mutual information\n\n\n\nAn estimate for the mutual information (i.e., between the sample distributions) can be derived from the marginals, as above, though it is more compactly represented as a pairwise sum over the domains of each distribution being compared: \\[\n\\text{MI}(A,B)\\approx \\sum_{i,j\\in[0,1]} p_{ij} \\log \\left( \\frac{p_{ij}}{p_{i\\bullet}p_{\\bullet j}} \\right)\n\\]\n\n\nIt is non-negative, with 0 occurring when A and B are independent. There are many other information-theoretic measures related to MI, but we specifically bring this up as it will be the basis for the Chow Liu method, later on.\nSometimes, especially in social networks, we might want to avoid overcounting relationships with very well-connected nodes. This was brought up with respect to the normalized Laplacian before, but we could also perform a normalization on the underlying bipartite adjacencies.\n\n\n\n\n\n\nNote 4.4: Hyperbolic Projection\n\n\n\nAttempts to account for the overcounting of co-occurrences on frequently occurring nodes, vs. rarer ones.[6] \\[ X^T\\text{diag}(1+\\mathbf{s}_j)^{-1}X \\quad \\mathbf{s}_j = \\sum_j{\\mathbf{x}_j'}\\] This re-weights each observation by its degree in the original bipartite graph.\n\n\n\n[6] M. E. J. Newman, “Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality,” Physical Review E, vol. 64, no. 1, p. 016132, Jun. 2001, doi: 10.1103/physreve.64.016132.\nSo far this is the first measure that re-weights observations before contraction, so that it depends on having the individual observations available (rather than only the gram matrix). In this case, each observation’s entries are all equally re-weighted by the number of activations in it (each nodes “activation fraction” in that observation).\n\n\nResource and Information Flow\nThese methods are somewhere between local and global constraint scales. This is accomplished by imagining nodes as having some amount of a resource (like information or energy) and correcting for observed noise in edge activation by reinforcing the geodesics that most likely transmitted that resource.\nFirst, closely related to hyperbolic re-weighting, we can imagine the bipartite connections as evenly dividing each nodes’ resources, before reallocating them to the nodes they touch, in turn. For instance, we might say each author splits their time among all of the papers they are on, and in turn every co-author “receives” an evenly divided proportion of everyone’s time they are co-authoring with.\n\n\n\n\n\n\nNote 4.5: Resource Allocation\n\n\n\nGoes one step further than hyperbolic projection, by viewing each node as having some “amount” of a resource to spend, which gets re-allocated by observational unit. [7] \\[ \\text{diag}(\\mathbf{s}_i)^{-1}X^T\\text{diag}(\\mathbf{s}_j)^{-1}X \\]\n\n\n\n[7] T. Zhou, J. Ren, M. s. Medo, and Y.-C. Zhang, “Bipartite network projection and personal recommendation,” Phys. Rev. E, vol. 76, no. 4, 4, p. 046115, Oct. 2007, doi: 10.1103/PhysRevE.76.046115.\nInterestingly, we could see this as a two-step random-walk normalization of the bipartite adjacency matrix. First \\(X\\) is row-normalized, then column-normalized. The final matrix is asymmetric, so a symmetric edge strength estimate is often retrieved by mean, max, or min reduction operations.\nRather than stop after two iterations, continuing to enforce unit marginals to convergence is known as the Sinkhorn-Knopp algorithm, which converges to a doubly-stochastic matrix (both marginal directions sum to 1).\n\n\n\n\n\n\nNote 4.6: Doubly Stochastic\n\n\n\nIf \\(A\\in \\mathbb{S}^{n\\times n}\\) is positive, then there exists \\(d_1,d_2\\) such that \\[W=\\text{diag}(d_1)A\\text{diag}(d_2)\\] is doubly-stochastic, and \\(W(u,v)\\) is the optimal transport plan between \\(u\\) and \\(v\\) with regularized Euclidean distance between them on a graph.[8], [9]\nThe doubly-stochastic filter [10] removes edges from \\(W\\) until just before the graph would become disconnected.\n\n\n\n[10] P. B. Slater, “A two-stage algorithm for extracting the multiscale backbone of complex weighted networks,” Proceedings of the National Academy of Sciences, vol. 106, no. 26, pp. E66–E66, Jun. 2009, doi: 10.1073/pnas.0904725106.\n\n[9] M. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal transport,” in Proceedings of the 27th international conference on neural information processing systems - volume 2, in NIPS’13. Red Hook, NY, USA: Curran Associates Inc., 2013, pp. 2292–2300.\n\n[8] B. Landa and X. Cheng, “Robust inference of manifold density and geometry by doubly stochastic scaling,” SIAM Journal on Mathematics of Data Science, vol. 5, no. 3, pp. 589–614, Sep. 2023, doi: 10.1137/22M1516968.\nAs the name implies, the optimal transport plan reflects the minimum cost to move some amount of resource from one node to another. By focusing on best-case cost, we enforce a kind of “principle of least action” to bias recovery toward edges along these geodesics.\nA more direct way to do this, perhaps, is to find the shortest paths from every node to each other node, and aggregate them.\n\n\n\n\n\n\nNote 4.7: High-Salience Skeleton\n\n\n\nCount the number of shortest-path trees an edge participates in, out of all the shortest-path-trees (one for every node). \\[ \\frac{1}{n}\\sum_{i=1}^n \\mathcal{T}_{\\text{sp}}(i) \\] where \\(\\mathcal{T}_{\\text{sp}}(n)\\) is the shortest-path tree rooted at \\(n\\) [11]\n\n\n\n[11] D. Grady, C. Thiemann, and D. Brockmann, “Robust classification of salient links in complex networks,” Nature Communications, vol. 3, no. 1, May 2012, doi: 10.1038/ncomms1847.\nUnfortunately, HSS is forced to scale with the number of nodes, and must calculate the entire spanning tree for each one.\n\n\nGlobal Structural Assumptions\nOften times these constraints are as simple as “the underlying dependency graph must belong to a family \\(\\mathcal{C}\\)” of graphs. Observations are thought of as emissions from a set of node distributions, where edges are representations of dependency relationship between them. To provide a foundation to formalize this notion, one framework is that of Markov Random Fields, which are undirected generalizations of bayes nets [12] that use edges to encode conditional dependence between node distributions.\n\n[12] R. Kindermann, Markov random fields and their applications. American Mathematical Society, 1980, p. 142.\nOne of the original structures for MRFs that we could recover from observed data was a tree.\n\n\n\n\n\n\nNote 4.8: Chow-Liu Spanning Tree\n\n\n\nEnforces tree structure globally. Approximates a joint probability \\[\nP\\left(\\bigcap_{i=1}^n v_i\\right) \\approx P' = \\prod_{e\\in T} P(u_n(e)|v_n(e)) \\quad T\\in \\mathcal{T}\n\\] where \\(\\mathcal{T}\\) is the set of spanning trees for the nodes. The Chow-Liu tree minimizes the Kullback-Leibler (KL) Divergence \\(\\text{KL}(P \\| P')\\) by finding the minimum spanning tree over pairwise mutual information weights.[13]\n\n\n\n[13] C. Chow and C. Liu, “Approximating discrete probability distributions with dependence trees,” IEEE Transactions on Information Theory, vol. 14, no. 3, pp. 462–467, May 1968, doi: 10.1109/tit.1968.1054142.\n\n[14] L. L. Duan and D. B. Dunson, “Bayesian spanning tree: Estimating the backbone of the dependence graph,” arXiv, arXiv:2106.16120, Jun. 2021. doi: 10.48550/arXiv.2106.16120.\nRecent work has made it possible to enforce spanning tree structure while efficiently performing monte-carlo-style bayesian inference, which estimates a distribution over spanning trees that explain observed behavior, and by extension the likelihood each edge is in one of these trees. [14]\nIf instead we imagine our MRF as being made up of individual Gaussian emissions, then the overall network will be a multivariate gaussian with pairwise dependencies along the edges. In fact, as a consequence of the Hammersley–Clifford theorem, the conditionally independent variables are precisely the set of zero entries in the the precision (inverse-covariance) matrix \\(\\Theta\\) of the multivariate model. Exploiting this fact leads to a semidifinite program to minimize the frobenius-norm of \\(\\Theta\\) with the sample covariance \\(\\| \\hat{\\Sigma}\\Theta \\|_F\\)2\n2  Since the sample covariance will not give an unbiased estimate for precision, these problems often require significant regularization. This class of problems is called “covariance shrinkage”, though we more specifically care about precision shrinkage as illustrated in Note 4.9.\n\n\n\n\n\nNote 4.9: GLASSO\n\n\n\nSemidefinite program to find (regularized) maximum likelihood precision of graph-structured multivariate Normal distribution using \\(\\ell_1\\) (“LASSO”) penalty [15]\n\\[\n\\operatorname*{argmax} \\mathcal{L}(\\Theta|\\hat{\\Sigma})\n  = \\operatorname*{argmin}_{\\Theta \\prec 0}\\ \\text{tr}(\\hat{\\Sigma} \\Theta) - \\log |\\Theta|+ \\rho\\|\\Theta\\|_1\n\\tag{4.1}\\]\nIn the binary case Equation 4.1 is still guaranteed to find the structure of the generating MRF, but only for graphs with singleton separators, as shown in [16].\n\n\n\n[16] P. Loh and M. J. Wainwright, “Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses,” in Advances in neural information processing systems, Curran Associates, Inc., 2012. doi: 10.1214/13-aos1162.\n\n[15] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3, pp. 432–441, Jul. 2008, doi: 10.1093/biostatistics/kxm045.\n3  something that this work attempts to begin addressing through standard reference datasets like MENDR, more on which is discussed in Simulation StudyThe “singleton separator” condition means that only MRFs structured like trees or block graphs will have graph-structured precision matrices returned by the GLASSO program. In effect (for the purpose of recovery from binary node activations) using GLASSO assumes either multivariate gaussian structure, or at the very least that all clique-separator sets are single-node. In practice, GLASSO is used for more than just this, but with theoretical misspecification we must rely more on empirical validation.3\nThere are many other models in this class, which provide strong global assumptions to make inference tractable. Not all look like structural assumptions on the graph structure itself, like Degree Sequence Models. [17], [18] They assume that the fundamental property of these datasets is their bipartite node degree distributions, leading to a generative model that can sample bipartite adjacency matrices with similar observation/node degree distributrions4\n\n[17] Z. Neal, “The backbone of bipartite projections: Inferring relationships from co-authorship, co-sponsorship, co-attendance and other co-behaviors,” Social Networks, vol. 39, pp. 84–97, Oct. 2014, doi: 10.1016/j.socnet.2014.06.001.\n\n[18] Z. P. Neal, R. Domagalski, and B. Sagan, “Comparing alternatives to the fixed degree sequence model for extracting the backbone of bipartite projections,” Sci Rep, vol. 11, no. 1, p. 23929, Dec. 2021, doi: 10.1038/s41598-021-03238-3.\n4 In that literature the “observation” partition’s nodes are typically called “artifacts” instead. \n[19] T. P. Peixoto, “Reconstructing networks with unknown and heterogeneous errors,” Phys. Rev. X, vol. 8, no. 4, p. 041011, Oct. 2018, doi: 10.1103/PhysRevX.8.041011.\n\n[20] T. P. Peixoto, “Network reconstruction and community detection from dynamics,” Phys. Rev. Lett., vol. 123, no. 12, p. 128301, Sep. 2019, doi: 10.1103/PhysRevLett.123.128301.\nStill others assume that the graphical structure can be described as generated by Stochastic Block Models, a meta-network of communities and their inter-community connection probabilities. They don’t constrain the graph class itself, but instead prescribe a generative process for graph creation (through community blocks). These have very nice properties for bayesian inference of structure, and can be modularly combined or nested for varying levels of specificity and hierarchical structure, sometimes with incredible computational efficiency. [19], [20]",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part1/1-03-recovery-road.html#a-path-forward",
    "href": "content/part1/1-03-recovery-road.html#a-path-forward",
    "title": "Roads to Network Recovery",
    "section": "A Path Forward",
    "text": "A Path Forward\nIn addition to the categories above, there is a second “axis” that practitioners should keep in mind when selecting their recovery algorithm of choice. Each of the above listed techniques can be mostly separated into two categories, based on whether hypergraphic/bipartite observations are assumed to be in data space, or in model space.\n\nObservation space assumptions\nRecall from Models & linear operators: an operator takes our model parameters and maps them to data space. The implication for inverse problems is a need to remove the effect of the operator, because we cannot directly observe phenomena in a way compatible with our model (e.g., which might model underlying causal effects).\nThis is a core point of view in [21], where those authors describe nearly all of network analysis as inferring hidden structure:\n\n“Here we argue that the most appropriate stance to take is to frame network analysis as a problem of inference, where the actual network abstraction is hidden from view, and needs to be reconstructed given indirect data.”\n– Peel et al., [21]\n\nWe note that this isn’t strictly true, assuming that the “network” is intended to represent something measured by the direct observation. For instance, if a network is intended to represent a discretization of distances (such as a k-nearest neighbors approximation) for computational efficiency. The co-occurrence measures can be thought of as estimators of node-node distances, especially with appropriate smoothing to remove zero-valued distances from undersampling.5 In otherwords, if an analyst wishes to discretize distances as incidences in a complex network, they are effectively using “high-pass” filter to remove low-similarity entries, which is an effective way to assess community structure—exactly like clustering for continuous data.6 In fact, for an example of this exact network-as-discretization idea being used for state-of-the-art clustering performance, see HDBSCAN in [22].\n5  See Maximum Spanning (Steiner) Trees for an elaboration of this connection via the “forest kernel”. 6  Or, at a slight risk of reductionism, drawing a world atlas with two colors for “above and below sea-level”: useful simplification for rapid assessment of shapes. \n[22] C. Malzer and M. Baum, “A hybrid approach to hierarchical density-based cluster selection,” in 2020 IEEE international conference on multisensor fusion and integration for intelligent systems (MFI), IEEE, Sep. 2020, pp. 223–228. doi: 10.1109/mfi49285.2020.9235263.\nBecause it is difficult to know a priori what a domain will require of network analysts, our main recommendation is for algorithm creators to transparently describe their technique’s data-space assumption:\n\nare observations already in model space, perhaps with with alleatoric noise to be removed?, or,\nare they in data space and require solving some form of inverse problem to recover a model specification?\n\nOnce again from the [21] review:\n\nSurprisingly, the development of theory and domain-specific applications often occur in isolation, risking an effective disconnect between theoretical and methodological advances and the way network science is employed in practice.\n– Peel et al. [21]\n\n[21] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\n\nIn a similar vein, we believe that a large amount of metrological inconsistency and struggle has at its heart a communication and technology transfer problem, which standardization and community toolkit support can hopefully work toward fixing.\n\n\nFilling the local+data “gap”\nWith this in mind, we show in Table 4.1 an overview of the covered approaches, and whether the method presumes operation on observations in the same space as the model, or if some inverse problem is needed.\n\n\n\n\n\nTable 4.1: Organizing recovery methods by representation space and level\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\nTo add to the point, the last column in Table 4.1 shows whether the full bipartite representation is even needed to perform the technique, or if it is possible with the gram matrix or marginal values alone.7\n\n7 note that any of these could make use of the bipartite “design matrix”, e.g., to estimate the edge support with stability selection[23] by subsampling it multiple times and repeating the algorithm accordingly. \n[23] N. Meinshausen and P. Bühlmann, “Stability selection,” Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 72, no. 4, pp. 417–473, Aug. 2010, doi: 10.1111/j.1467-9868.2010.00740.x.\n\n[24] H. J. Bierens, “The nadaraya–watson kernel regression function estimator: Estimation, testing, and specification of cross-section and time series models,” in Topics in advanced econometrics, 1. paperback ed., New York: Cambridge Univ. Press, 1996, pp. 212–247.\n\n[25] E. L. Kaplan and P. Meier, “Nonparametric estimation from incomplete observations,” Journal of the American Statistical Association, vol. 53, no. 282, pp. 457–481, Jun. 1958, doi: 10.1080/01621459.1958.10501452.\nIn the next sections, we focus on filling a gap for models that only make local assumptions and preserve additivity, but assumes that data is not represented directly in the network model-space. Much like the role that nonparametric estimators like KDE/Nadarya-Watson play in regression, or Kaplan-Meier estimators in survival analysis,[24], [25] additive models only make assumptions about local structure. But from these assumptions, they can provide critical insight into data and its structure, and push analysts to make regular “sanity checks” when results or assumptions conflict.",
    "crumbs": [
      "A Practitioner's Guide to Network Recovery",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Roads to Network Recovery</span>"
    ]
  },
  {
    "objectID": "content/part2/2-04-rand-sf.html",
    "href": "content/part2/2-04-rand-sf.html",
    "title": "Latent Graphs with Desire Paths",
    "section": "",
    "text": "The Gambit of the Inner Product\nAddressing gaps discussed in Roads to Network Recovery to reach a generative model for network recovery requires careful attention to the generation mechanism for node activations. While there are many ways we might imagine bipartite data to be be generated, presuming the existence of a dependency graph that causes activation patterns will give us useful ways to narrow down the generative specification.\nFirst, we will investigate the common assumption that pairwise co-occurrences can serve as proxies for measuring relatedness, and how this “gambit of the group” is, in fact, a strong bias toward dense, clique-filled recovered networks. Using the relationship between matrix products and sums of vector outer-products, we then motivate a generalization of co-occurrence estimation that can be flexibly adapted to domain knowledge, as appropriate, and avoid undue clique bias altogether. Finally, we propose a simple mechanism to frame network reconstruction as combinations of multiple overlaid subgraphs, by treating edges as i.i.d. Beta-Bernoulli random variables.\nInspired by the behavior of so-called “desire paths”, we constrain the beta prior to ensure desired behavior, which we call the Desire Path Density estimate of the global graph structure.\nAs we saw repeatedly in Roads to Network Recovery, networks are regularly assumed to arise from co-occurrences, whether directly as counts or weighted in some way. This assumption can be a significant a source of bias in the measurement of edges. In this section we provide an intuitive understanding for why a flat count of co-occurrence leads to “hairballs” (specifically, a bias for dense clusters and cliques), related to the use of matrix products on node activation design matrices.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latent Graphs with Desire Paths</span>"
    ]
  },
  {
    "objectID": "content/part2/2-04-rand-sf.html#sec-clique-bias",
    "href": "content/part2/2-04-rand-sf.html#sec-clique-bias",
    "title": "Latent Graphs with Desire Paths",
    "section": "",
    "text": "Gambit of the Group\nIt seems reasonable, when interactions are unobserved, to assume some evidence for all possible interactions is implied by co-occurrence. However, similar to the use of uniform priors in other types of inference, if we don’t have a good reason to employ a fully-connected co-occurrence prior on interaction dependencies, we are adding systematic bias to our inference. Whether co-occurrence observations can be used to infer interaction networks directly was discussed at length in [1], where they call this the gambit of the group.\n\n“So, consiously or unconsciously, many ethnologists studying social organization makr what might be called the ‘gambit of the group’: they assume that animals which are clustered […] are interacting with one another and then use membership of the same cluster […] to define association.”\n[1]\n\n[1] H. Whitehead and S. Dufault, “Techniques for analyzing vertebrate social structure using identified individuals: Review and recommendations,” Advances in the Study of Behavior, vol. 28, no. 28, pp. 33–74, 1999.\n\nThis work was rediscovered in the context of measuring assortativity for social networks,1 where the author of [2] advises that “group-based methods” can introduce sampling bias to the calculation of assortativity, namely, systematic overestimation when the sample count is low.\n1 Assortativity is, roughly, the correlation between node degree and the degrees of its neighbors.\n[2] D. N. Fisher, M. J. Silk, and D. W. Franks, “The perceived assortativity of social networks: Methodological problems and solutions,” in Trends in social network analysis, Springer International Publishing, 2017, pp. 1–19. doi: 10.1007/978-3-319-53420-6_1.\nA reader can analyze general problems with failing to specify a model of what “edges” actually are more in-depth in [3]. They also include a warning not to naively use correlational measures with a threshold, since even simple 3-node systems will easily yield false positives edges. Still, it would be helpful for practitioners to have a more explicit mental model of why co-occurence-based models yield systematic bias, and use that to build an alternative having some of the same benefits (speed, interpretability, uncertainty quantification, etc.)\n\n\nInner-Product projections and “clique bias”\nUnderlying correlation and co-occurrence models for edge strength is a reliance on matrices of inner products between node occurrence vectors. They all use gram matrices (or centered/scaled versions of them), which were brought up in Distance vs. Incidence. The matrix multiplication performed represents inner products between all pairs of feature vectors. For \\(X(i,j)\\in\\mathbb{B}\\), these inner products sum together the times in each observation that two nodes were activated together.\n\n\n\n\n\n\n\n\n\nFigure 5.1: Gram matrix as sum of observation outer products\n\n\n\n\n\n\nHowever, another (equivalent) way to view matrix multiplication is as a sum of outer products \\[\nG(j,j') = X^T X = \\sum_{i=1}^m X(i,j)X(i,j')= \\sum_{i=1}^m \\mathbf{x}_i\\mathbf{x}_i^T\n\\] Those outer products of binary vectors create \\(m\\times m\\) matrices that have a 1 in every \\(j,j'\\) entry where nodes \\(j,j'\\) both occurred, shown in Figure 5.1. Each outer product is effectively operating as a \\(D_i+A_i\\) with degrees normalized to 1. If the off-diagonals can be seen as adjacency matrices, they would strictly represent a clique on nodes activated in the \\(i\\)th observation In this sense, any method that involves transforming or re-weighting a gram matrix, is implicitly believing that the \\(i\\)th observation is a complete graph for all \\(i\\). This is illustrated in Figure 5.2.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Observations as activation sets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Edge Measurements with Group Gambit (BoW) assumption\n\n\n\n\n\n\n\n\n\nFigure 5.2: Inner-product projections as sums of cliques illustrating “clique bias”.\n\n\n\nIf every observation of node activations leads to an implied clique, we can reframe much of the “hairball” effect as a systematic bias (i.e. measurement error in the sense of_trueness_). We call this clique bias: the inferred graph will inherently include more and more cliques of node subsets as data arrives (assumed to themselves be cliques).\nFor many modeling scenarios, this paradigm allows practitioners to make a more straight-forward intuition-check: do clique observations make sense here? When a list of authors for a paper is finished, does that imply that all authors mutually interacted with all others directly to equally arrive at the decision to publish? This would be similar to assuming the authors might simultaneously enter the same room, look at a number of others (who all look exclusively at each other, as well), and all at once decide to publish together.\nOr, from the standpoint of scaling: does each extra node activation impart an amount of information that depends on the number of activated nodes? Put another way, if we knew our observations were on a planar graph, each node might require around 3 new edges.2 A tree or path adds one new edge for each new node. But a clique assumption means that each extra node activation adds edges quadratically in the number of already-activated nodes. Does this make sense? In our introduction, we described a more likely scenario we could expect from an observer on the ground: a researcher asks a colleague or two to collaborate, who might know a couple more with relevant expertise, and so on. From purely a logistical standpoint, it quickly becomes unfeasible for authors to mutually collaborate with all other co-authors equally: 10 coauthors already implies the 10th had to equally split interaction among 9 others to satisfy our model.\n2 Triangulations are worst-case, so \\(|E|\\leq 3|V|-6\\) due to Euler’s formula",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latent Graphs with Desire Paths</span>"
    ]
  },
  {
    "objectID": "content/part2/2-04-rand-sf.html#networks-as-desire-path-density-estimates",
    "href": "content/part2/2-04-rand-sf.html#networks-as-desire-path-density-estimates",
    "title": "Latent Graphs with Desire Paths",
    "section": "Networks as Desire Path Density Estimates",
    "text": "Networks as Desire Path Density Estimates\nUnfortunately, methods based on inner-product thresholding are still incredibly common, in no small part due to how easy it is to create them from occurrence data, regardless of this “clique-bias”. The ability to map an operation onto every observation, e.g., in parallel, and then reduce all the observations into an aggregate edge estimate is a highly desirable algorithmic trait. This may be a reason so many projection and backboning techniques attempt to re-weight (but retain) the same basic structure, time and again.\nWhat we need is a way to maintain the ease-of-use of inner-product network creation:\n\nMap an operation onto each observation\nReduce to an aggregate edge guess over all observations\n\nbut with a more domain-appropriate operator at the observation level.\nLet’s start with replacements for the clique assumption. There are many non-clique classes of graphs we might believe local interactions occur on: path-graphs, trees, or any number of graphs that reflect the topolgy or mechanism of local interactions in our domain of interest. Authors have proposed classes of graphs that mirror human perception of set shapes [4]3, or graphs whose modeled dependencies are strictly planar [5]4. Alternatively, the interactions might be scale free, small-world, trees, or samples from stochastic block models.[6] In any case, these assumptions provide an explicit way to describe the set of possible interaction graphs we believe our individual observations are sampled from.\n3  e.g., for dependencies based on perception, such as human decision making tendencies, or causes based on color names.\n[5] M. Tumminello, T. Aste, T. Di Matteo, and R. N. Mantegna, “A tool for filtering information in complex systems,” Proceedings of the National Academy of Sciences, vol. 102, no. 30, pp. 10421–10426, 2005.\n4  e.g., when interactions are limited to planar dependencies, like inferring ancient geographic borders.\n[6] P. W. Holland, K. B. Laskey, and S. Leinhardt, “Stochastic blockmodels: First steps,” Social Networks, vol. 5, no. 2, pp. 109–137, Jun. 1983, doi: 10.1016/0378-8733(83)90021-7.\n\nSubgraph Distributions\nLet’s use the notation from Equation 3.9, such that each observation of nodes \\(\\mathbf{x}_i\\) is implicitly derived from a set of activated edges \\(\\mathbf{r_i}\\). To start, our current belief about what overall structure the whole network can take is \\(G^*=(V,E,B^*)\\), where \\(B^*\\) might always return \\(1\\) to start out (the complete graph). To further constrain the problem, let us assume that node activation is noiseless: any activated nodes were truly activated, and unactivated nodes were truly inactive (no false negative or false positive activations).5 So, each \\(\\mathbf{x}_i\\) will induce a subgraph \\(g_i = G^*[V_i]\\), where \\(V_i = \\{v\\in \\mathcal{V} | X(i,\\mathcal{V})=1\\}\\). Then, our domain knowledge takes the form of a constraint on edges within that subgraph, which will induce a family of subgraphs on \\(g_i\\). This family \\(\\mathcal{C}_i\\) belongs to the relevant class of graphs \\(\\mathcal{C}\\), limited to nodes \\(V_i\\), i.e.,\n5  Hidden nodes (unobserved nodes beyond the \\(n\\)) are outside the scope of this work, though could potentially be implied for certain structures e.g., when the metric is known to be tree-like. [7] use a greedy embedding that minimizes distortion to estimate the need for added Steiner nodes. \n[7] R. Sonthalia and A. C. Gilbert, “Tree! I am no tree! I am a low dimensional hyperbolic embedding,” arXiv; arXiv, arXiv:2005.03847, Oct. 2020. doi: 10.48550/arXiv.2005.03847.\n6 \\(\\mathcal{P}(A)\\) is the powerset of \\(A\\), i.e., the set of all subsets of \\(A\\). \\[\n\\begin{gathered}\n\\mathcal{C}_i = \\{(V,E,B_i) \\in\\mathcal{C}|B_i(e,v)=B^*(e,v)\\mathbf{1}_{V_i}(v)\\mathbf{1}_{E_i}(e)\\}\\\\\nE_i\\in\\{\\mathcal{E}\\in\\mathcal{P}(E)| g_i[\\mathcal{E}]\\in\\mathcal{C}\\}\nV_i = \\{v\\in \\mathcal{V} | X(i,\\mathcal{V})=1\\}\n\\end{gathered}\n\\tag{5.1}\\]6\nIn other words, the edges that “caused” to the node activations in a given observation must together belong to a graph that, in turn, belongs to our desired class, which must occur on the nodes that were activated.\nAssuming we can define an associated measure \\(\\mu_i(c)\\) (one for each \\(c\\in\\mathcal{C}_i\\)) we are able to define a probability distribution over subgraphs in the class.7 Using notation similar to distributions over spanning trees in [10]:\n7  This is certainly not a trivial assumption, and might either be ill-defined or require techniques like the Gumbel trick[8] to approximate, unless the partition function \\(Z\\) has a closed form, or \\(\\mu\\) is already a probability measure on some \\(\\sigma\\)-algebra over \\(\\mathcal{C}\\). Closed-form \\(\\mathcal{Z}\\) values on \\(\\mathcal{C}\\) are known for a handful of graph classes, such as the space of spanning trees, \\(\\mathcal{C}=\\mathcal{T}\\). However, another way this might be accomplished is through random geometric graphs[9], or geometric spanners like the Relative Neighborhood [4] graphs on a “sprinkling” of points that preserves their observed pairwise distances.\nThis is further discussed in Generalizing inner products on incidences.\n[8] M. B. Paulus, D. Choi, D. Tarlow, A. Krause, and C. J. Maddison, “Gradient estimation with stochastic softmax tricks.” arXiv, 2020. doi: 10.48550/ARXIV.2006.08063.\n\n[9] E. N. Gilbert, “Random plane networks,” Journal of the Society for Industrial and Applied Mathematics, vol. 9, no. 4, pp. 533–543, Dec. 1961, doi: 10.1137/0109045.\n\n[4] J. W. Jaromczyk and G. T. Toussaint, “Relative neighborhood graphs and their relatives,” Proceedings of the IEEE, vol. 80, no. 9, pp. 1502–1517, 1992, doi: 10.1109/5.163414.\n\n[10] R. Zmigrod, T. Vieira, and R. Cotterell, “Efficient computation of expectations under spanning tree distributions,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 675–690, Jul. 2021, doi: 10.1162/tacl_a_00391.\n\\[\n\\begin{gathered}\np_i(c) = \\frac{\\mu_i(c)}{Z_i}\\\\\nZ_i = \\sum_{c\\in\\mathcal{C}_i} \\mu_i(c)\n\\end{gathered}\n\\tag{5.2}\\]\nThis can be represented using the vectorization in Equation 3.9, due to the one-to-one correspondence established, so that, with a slight abuse of notation treating \\(\\mathcal{C}_i\\) as the parameter of distribution \\(p_i\\): \\[\n\\mathbf{r}_{i}(e)|\\mathbf{x_i} \\sim p_i(\\mathcal{C},E,V)\n\\tag{5.3}\\]\nSo we may not have an exact vector, but we have established a way to specify a family of edge vectors that could be responsible. With \\(p_i(c)\\), we also have a mechanism to sample them when a partition function is available (or able to be approximated).\nWith these mechanics in place, we see that choosing “cliques” (implied by the inner product) is a trivial application of Equation 5.3, since that is equivalent to selecting the class of cliques on \\(V_i\\) nodes. This has only one element (\\(\\|\\mathcal{C}_{\\text{clique}}\\|=1\\)), there is only 1 possible selection, with probability \\(p_i(K^{V_i})=1\\).\n\n\n\n\nGraph Unions as Desire Paths\nWith a distribution over subgraphs each observation, we are potentially able to sample from them for bootstrap or Monte Carlo estimation purposes, or simply find a maximum likelihood estimate for each distribution. Assuming this is true, we may now sample or approximate a matrix \\(R(i,e):I\\times E \\rightarrow \\mathbb{B}\\).\nMethods for doing this efficiently in certain cases are the focus of Forest Pursuit and Model Specification. However, once \\(R(i,e)\\) is estimated, a reasonable mechanism for estimating the support of the set of edges is to use \\(\\frac{\\text{count}}{\\text{exposure}}\\), but with a few needed modifications.\nFirst, while the nodes counts in \\(\\sum_i B(i,\\cdot)\\) are by assumption not independent, or even pairwise independent, the edge traversal counts \\(\\sum_i R(i,\\cdot)\\) could more reasonably be considered so. A model certainly could be constructed where edge existence depends on other edges existing (or not). But nothing is inherently self-inconsistent with a model that assumes the traversability of individual edges will be independent of one another.\nLet P(e) be the probability that an edge is traversed (in any observation), and P(u,v) the probability that nodes \\(u,v\\) co-occur. To approximate the overall traversability of an edge, we can calculate an empirical estimate for the conditional probability \\(P(e|(u,v))=\\frac{P(e)\\cap P(u,v)}{P(u,v)}\\) that an edge is traversed, given that the two incident nodes are activated. This estimate can use the same Beta-Bernoulli distribution from Equation 2.2.\nStill, how do we ensure our estimate is approximating traversability, so that the probability that an edge probability converges toward 1 as long as it has to be possible for \\(e\\) to be traversed? Recall from the introduction that, from a measurement perspective, the underlying networks rarely “exist” in the sense that we never truly find the underlying network, but only observations sampled from it. Imagine that the “real” network is a set of paved sidewalks: our procedure is similar to watching people walk along paths between locations, and wanting to estimate which of the paths would be tread “enough” to be paved. This is where we build on an intuition based on the popular idea of desire paths which is a colloquial name for paths that form when individuals walk over grass or dirt enough to carve a trail. In network analysis and recovery from activations, then, we are only allowed to see lists of visited locations. If we can add a domain-informed assumption on what the trajectories of individuals could have been, based on those locations, then we can estimate the desire paths that might have formed from them. Importantly, we can use this framing to “reset” the trueness of our uncertainty: given a trajectory assumption, the desire path uncertainty becomes one of precision due to lack of knowledge about which path was taken. As [3] recommend, desire paths are inferred, and we never rely on needing to guess the actual “pavement”—only beliefs about “future paving”.\n\n[3] L. Peel, T. P. Peixoto, and M. De Domenico, “Statistical inference links data and theory in network science,” Nature Communications, vol. 13, no. 1, Nov. 2022, doi: 10.1038/s41467-022-34267-9.\nIf presented with two equal-length desire paths, an individual is likely to choose the one that has been tread more often before i.e., the “more beaten” path. So, we don’t want a probability that an edge has been traversed, but a probability over fractions of the time we expect an edge to have been traversed more often than not: how likely it is to “be beaten”. This is accomplished by forcing \\(\\alpha, \\beta &lt; 1\\). For the case where \\(\\alpha=\\beta=0.5\\), we call this special case an arcsine distribution.\nIn a coin tossing game where each “heads” gives a point to player A and “tails” to player B, then the point differential is modeled as a random walk. The arcsine distribution \\(\\text{Beta}(\\tfrac{1}{2},\\tfrac{1}{2})\\) is exactly the probability distribution for the fraction of time we expect one player to be “ahead” of the other. [11]\n\n[11] E. Ackelsberg, “What is the arcsine law?” 2018, Available: https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf \n\n“Contrary to popular opinion, it is quite likely that in a long coin-tossing game one of the players remains practically the whole time on the winning side, the other on the losing side.”\nWilliam Feller[12, Ch. III]\n\n[12] W. Feller, An introduction to probability theory and its applications, volume 1. J. Wiley & Sons: New York, 1968.\n\nThis is desirable behavior for a distribution over edge traversability! We expect the vast majority of edges to have a 0 or 1 as the most likely values, with the expected fraction of observations that an edge being traversed was “ahead” of it being not traversed matching our empirical count. We generalize this with \\(\\alpha = 1-\\beta\\), with \\(\\alpha + \\beta = 1\\), such that the new beta posterior from Equation 2.2 with \\(s\\) successes over \\(k\\) trials is:\n\\[\n\\begin{gathered}\n\\pi \\sim \\text{Beta}(\\alpha + c, 1-\\alpha-c) \\\\\nc = \\frac{s-ak}{k+1}\\\\\n\\end{gathered}\n\\tag{5.4}\\]\nImportant to note is that \\(k\\) is measured over the co-occurrences \\((u,v)\\), and successes are the traversals \\(e\\) derived from our samples in \\(R\\). This lets us formulate a likelihood model for each edge’s traversibility in the global network \\(G\\) (i.e., whether \\(B(e,v)&gt;0\\) for any \\(v\\)), which is i.i.d. Bernoulli. \\[\n\\begin{gathered}\n\\pi_e \\sim \\text{Beta}(\\alpha, 1-\\alpha), e\\in E\\\\\nE \\sim \\text{Bernoulli}(\\pi_e), e \\in E\n\\end{gathered}\n\\tag{5.5}\\]\nThis does not yet specify a likelihood for \\(\\mathcal{C}_i\\), because we have not included a mechanism for the down-selection to each \\(\\mathbf{x}_i\\). This will be addressed more completely for the special case of \\(\\mathcal{C}=\\mathcal{F}\\), the set of spanning forests on a graph, in Generative Model. In general, however, failing to specify the prior distribution on \\(\\mathcal{C}_i\\) does not necessarily make Equation 5.3 unusable, but necessitates an “empirical bayes” approach. With the marginals and co-occurrence counts for nonzero values in \\(X\\), we can make a point estimate for each edge given a node subset, without needing to consider a prior distribution for each \\(\\mathbf{x_i}\\).\nThe nonparametric approach, in cases that merit the use of spanning trees, will be central to accurate and scalable estimation of \\(B\\) through our proposed method covered in the next chapter, Forest Pursuit.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Latent Graphs with Desire Paths</span>"
    ]
  },
  {
    "objectID": "content/part2/2-05-forest-pursuit.html",
    "href": "content/part2/2-05-forest-pursuit.html",
    "title": "Approximate Recovery in Near-linear Time by Forest Pursuit",
    "section": "",
    "text": "Random Walks as Spanning Forests\nIn this chapter, we build on the notion of “Desire Path” estimation of a dependency network from node activations — sampling from a class of subgraphs constrained to active nodes, then merging them. We present Forest Pursuit (FP), a method that is scalable, trivially parallelizes, and offline-capable, while also outperforming commonly used algorithms across a battery of standardized tests and metrics. The key application for using FP is in domains where node activation can be reasonably modeled as arising due to random walks—or similar spreading process—on an underlying dependency graph.\nBecause we wish to model our node activations as being caused by other nodes (that they depend on), we draw a connection to a class of models for spreading, or, diffusive processes. We outline how random-walks are related to these diffusive models of graph traversal, enabled by an investigation of the graph’s “regularized laplacian” from [1].\nFirst, we build an intuition for the use of trees as an unbiased estimator for desire path estimation when spreading processes are at work on the latent network. Then, the groundwork for FP is laid by combining sparse approximation through matching pursuit with a loss function modeled after the Chow Liu representation for joint probability distributions. The approximate complexity for FP is linear in the spreading rate of the modeled random walks, and linear in dataset size, while running in \\(O(1)\\) time with respect to the network size. This departs dramatically from other methods in the space, all of scale in the number of nodes in the entire network. We then test FP against an array of alternative methods (including GLASSO) with MENDR, our proposed standard reference dataset and testbench for network recovery. FP outperforms other tested algorithms in nearly every case, and empirically confirms its complexity scaling for sub-thousand network sizes.\nThe class of diffusive processes we focus on “spread” from one node to another. If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes “depend” on others to be activated. In this case, a node activates when another node it depends on spreads their state to it. These single-cause activations are often modeled as a random-walk on the dependency graph: visiting a node leads to its activation.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Approximate Recovery in Near-linear Time by _Forest Pursuit_</span>"
    ]
  },
  {
    "objectID": "content/part2/2-05-forest-pursuit.html#random-walks-as-spanning-forests",
    "href": "content/part2/2-05-forest-pursuit.html#random-walks-as-spanning-forests",
    "title": "Approximate Recovery in Near-linear Time by Forest Pursuit",
    "section": "",
    "text": "Random Walk Activations\nRandom walks are regularly employed to model spreading and diffusive processes on networks. If a network consists of locations, states, agents, etc. as “nodes”, and relationships between nodes as “edges”, then random walks consist of a stochastic process that “visits” nodes by randomly “walking” between them along connecting edges. Epidemiological models, cognitive search in semantic networks, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. [2], [3], [4], [5], [6]\n\n[2] S. Kim, J. Breen, E. Dudkina, F. Poloni, and E. Crisostomi, “On the effectiveness of random walks for modeling epidemics on networks,” PLOS ONE, vol. 18, no. 1, p. e0280277, Jan. 2023, doi: 10.1371/journal.pone.0280277.\n\n[4] K.-S. Jun, J. Zhu, T. T. Rogers, Z. Yang, et al., “Human memory search as initial-visit emitting random walk,” Advances in neural information processing systems, vol. 28, no. 20, pp. 2389–2393, 2015, doi: 10.1016/j.physleta.2019.04.060.\n\n[5] P. G. Doyle and J. L. Snell, “Random walks and electric networks.” arXiv, 2000. doi: 10.48550/ARXIV.MATH/0001057.\n\n[6] M. Mane, D. DeLaurentis, and A. Frazho, “A markov perspective on development interdependencies in networks of systems,” Journal of Mechanical Design, vol. 133, no. 10, Oct. 2011, doi: 10.1115/1.4004975.\n\n[7] B. Rozemberczki, O. Kiss, and R. Sarkar, “Little ball of fur: A python library for graph sampling,” 2020, doi: 10.48550/ARXIV.2006.04311.\n\n[8] M. E. J. Newman, “The structure and function of complex networks,” SIAM Review, vol. 45, no. 2, pp. 167–256, Jan. 2003, doi: 10.1137/s003614450342480.\n1 For a brief treatment of the case that INVITE emission order is preserved, see Recovery from Working Memory & Partial OrdersWhen network structure is known, the dynamics of random-walks are used to capture the network structure via sampling [7], estimate node importance’s[Mathematicsnetworks_Newman2018], or predict phase-changes in node states (e.g., infected vs. uninfected)[8]. In our case, since we have been encoding the activations as binary activation vectors, the “jump” information is lost—activations are “emitted” for observation only upon the random walker’s initial visit.[4] Further, the ordering of emissions has been removed in our binary vector representation, leaving only co-occurrence groups in each \\(\\mathbf{x}_i\\).1 In many cases, however, the existence of relationships is not known already, and analysts might assume their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.\n\n\nActivations in a Forest\nAs a general setting, the number of node activations (e.g., for datasets like co-authorship) is much smaller than the set of nodes (\\(\\|\\mathbf{x}_i\\in\\mathbb{S}^n\\|_0 \\ll n\\))2\n[1] go to some length describing discrete- and continuous-time random walk models that can give rise to binary activation vectors like our \\(X(i,j):I\\times J\\rightarrow \\mathbb{B}\\). The regularized laplacian (or forest) kernel of a graph[9] plays a central role in their analysis, as it will in our discussion going forward.\n2  \\(\\|\\cdot\\|_0\\) is the \\(\\ell_0\\) “pseudonorm”, counting non-zero elements (the support) of its argument.\n[9] K. Avrachenkov, P. Chebotarev, and D. Rubanov, “Similarities on graphs: Kernels versus proximity measures,” European Journal of Combinatorics, vol. 80, pp. 47–56, Aug. 2019, doi: 10.1016/j.ejc.2018.02.002.\n\\[\nQ_{\\beta} = (I+\\beta L)^{-1}\n\\tag{6.1}\\]\nIn that work, it is discussed as the optimal solution to the semi-supervised “node labeling” problem, having a regularization parameter \\(\\beta\\), though its uses go far beyond this.[10], [11], [12] \\(Q\\) generalizes the so-called “heat kernel” \\(\\exp{(-t\\tilde{L})}\\), in the sense that it solves a lagrangian relaxation of a loss function based on the heat equation. This can be related to the PageRank (\\(\\exp{(-tL_{\\text{rw}})}\\)) kernel as well, which is explicitly based on random walk transition probabilities.\n\n[10] J. Pang and G. Cheung, “Graph laplacian regularization for image denoising: Analysis in the continuous domain,” IEEE Transactions on Image Processing, vol. 26, no. 4, pp. 1770–1785, Apr. 2017, doi: 10.1109/TIP.2017.2651400.\n\n[11] O. Knill, “Counting rooted forests in a network,” arXiv, arXiv:1307.3810, Jul. 2013. doi: 10.48550/arXiv.1307.3810.\n\n[12] P. Chebotarev and E. Shamis, “The matrix-forest theorem and measuring relations in small social groups,” arXiv, arXiv:math/0602070, Feb. 2006. doi: 10.48550/arXiv.math/0602070.\n3 \\(Q\\) can also be interpreted as a continuous-time random walk location probability, after exponentially distributed time, if spending exponentially-distributed time in each node.4 edge weights scaled by In fact, \\(Q\\) can be viewed as a transition matrix for a random walk having a geometrically distributed number of steps, giving us a small expected support for \\(\\mathbf{x}_i\\), as needed.3 However we interpret \\(Q\\), a remarkable fact emerges due to a theorem by Chebotarev[11], [12]: each entry \\(q=Q(u,v)\\) is equal to the probability4 that nodes \\(u,v\\) are connected in a randomly sampled spanning rooted forest\nIn other words, co-occurring node activations due to a random walk or heat kernel are deeply tied to the chance that those nodes find themselves on the same tree in a forest.\n\n\nSpreading Dependencies as Trees\nWith the overt link from spreading processes to counts of trees made, there’s room for a more intuitive bridge.\nFor single-cause, single source spreading process activations—on a graph—the activation dependency graph for each observation/spread/random walk must be a tree. With a single cause (the “root”), which is the starting position of a random walker, a node can only be reached (activated) by another currently activated node. If the random walk jumps from one visited node to another, previously visited one, that transition did not result in an activation, so the dependency count for that edge should not increase. This description of a random walk, where subsequent visits do not “store” the incoming transition, is roughly equivalent to one more commonly described as a Loop-Erased random walk. It is precisely used to uniformly sample the space of spanning trees on a graph.[13]\n\n[13] D. B. Wilson, “Generating random spanning trees more quickly than the cover time,” in Proceedings of the twenty-eighth annual ACM symposium on theory of computing - STOC ’96, in STOC ’96. ACM Press, 1996, pp. 296–303. doi: 10.1145/237814.237880.\nMuch like a reluctant co-author “worn down” by multiple collaboration requests, we can even include random walks that “receive” activation potential from more than one source. Say a node is activated when some fraction of its neighbors have all originated a random walk transition to it, or a node activates on its second visit, or similar. We simply count (as dependency evidence) the ultimate transition that precipitated activation. This could be justified from an empirical perspective as well: say we observe an author turn down requests for one paper from two individuals, but accept a third. We could actually infer a lowered dependency on the first two, despite the eventual coauthorship. Only the interaction that was observed as successful necessarily counts toward success-dependency, barring any contradicting information.\nIt’s important to add here that mutual convincing by multiple collaborators simultaneously (or over time) is expressly left out. In other words, only pairwise interactions are permitted. This is not an additional assumption, but a key limitation of our use of graphs in the first place! As Torres et al. go to great lengths elaborating in [14], it is critical to correctly model dependencies when selecting a structural representation of our problem to avoid data loss. The possibility for multi-way interactions would necessitate the use of either a simplicial complex or a hypergraph as the carrier structure, not a graph.\n\n[14] L. Torres, A. S. Blevins, D. Bassett, and T. Eliassi-Rad, “The why, how, and when of representations for complex systems,” SIAM Rev., vol. 63, no. 3, pp. 435–485, Jan. 2021, doi: 10.1137/20M1355896.\n\n\n\n\n\n\n\n\n\nFigure 6.1: Edge Measurements with true (tree) dependencies known\n\n\n\n\n\n\nFigure 6.1 demonstrates the use of trees as the distribution for subgraphs, instead of outer-products/cliques.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Approximate Recovery in Near-linear Time by _Forest Pursuit_</span>"
    ]
  },
  {
    "objectID": "content/part2/2-05-forest-pursuit.html#sparse-approximation",
    "href": "content/part2/2-05-forest-pursuit.html#sparse-approximation",
    "title": "Approximate Recovery in Near-linear Time by Forest Pursuit",
    "section": "Sparse Approximation",
    "text": "Sparse Approximation\nAs indicated previously, we desire a representation of each observation that takes the “node space” vectors (\\(\\mathbf{x}_i\\)) to “edge space” ones (\\(\\mathbf{r}_i\\)). We have separated each observation with the intention of finding a point-estimate for the “best” edges, such that the edge vector induces a subgraph belonging to a desired class. If we assume that each edge vector is in \\(\\mathbb{B}^{\\omega}\\), so that the interactions are unweighted, undirected, simple graphs, then for any family of subgraphs we will be selecting from at most \\(\\omega\\leq {n\\choose 2}\\) edges.\nRepresenting a vector as a sparse combination of a known set of vectors (also known as “atoms”) is called sparse approximation.\n\nProblem Specification\nSparse approximation of a vector \\(\\mathbf{x}\\) as a representation \\(\\mathbf{r}\\) using a dictionary of atoms (columns of \\(D\\)) is specified more concretely as [15]: \\[\\mathbf{\\hat{r}} = \\operatorname*{argmin}_{\\mathbf{r}}{\\|\\mathbf{x}-D\\mathbf{r} \\|_2^2} \\quad \\text{s.t.} \\|\\mathbf{r}\\|_0\\leq N  \\tag{6.2}\\] where \\(N\\) serves as a sparsity constraint (at most \\(N\\) non-zero entries). This is known to be NP-hard, though a number of efficient methods to approximate a solution are well-studies and widely used. Solving the Lagrangian form of Equation 6.2, with an \\(\\ell_1\\)-norm in place of \\(\\ell_0\\), is known as Basis Pursuit[16], while greedily solving for the non-zeros of \\(\\mathbf{r}\\) one-at-a-time is called matching pursuit[17]. In that work, each iteration selects the atom with the largest inner product \\(\\langle \\mathbf{d}_{i'},\\mathbf{x}\\rangle\\).\n\n[15] R. Rubinstein, M. Zibulevsky, and M. Elad, “Efficient implementation of the k-SVD algorithm using batch orthogonal matching pursuit,” Cs Technion, vol. 40, no. 8, pp. 1–15, 2008.\n\n[16] B. K. Natarajan, “Sparse approximate solutions to linear systems,” SIAM Journal on Computing, vol. 24, no. 2, pp. 227–234, Apr. 1995, doi: 10.1137/s0097539792240406.\n\n[17] S. G. Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3397–3415, 1993, doi: 10.1109/78.258082.\nWe take an approach similar to this, but with the insight that the inner product will not result in desired sparsity (namely, a tree). Our dictionary in this case will be the set of edges given by \\(B\\) (see Subgraph Distributions), while our sparsity is given by the relationship of the numbers of nodes and edges in a tree: \\[\n\\mathbf{\\hat{r}} = \\operatorname*{argmin}_{\\mathbf{r}}{\\|\\mathbf{x}-B^T\\mathbf{r} \\|_2^2} \\quad \\text{s.t.}\\quad \\|\\mathbf{r}\\|_0 = \\|\\mathbf{x}\\|_0 - 1\n\\tag{6.3}\\]\nThere are some oddities to take into account here. As a linear operator (see Models & linear operators), \\(B^T\\) takes a vector of edges to node-space, counting the number of edges each node was incident to. This means that, even with a ground-truth set of interactions, \\(B^T\\) would take them to a new matrix \\(X_{\\text{deg}}(i,j):I\\times J \\rightarrow \\mathbb{N}\\), which has entries of the number of interactions each individual in observation \\(i\\) was involved in. While very useful for downstream analysis (see Forest Pursuit as Preprocessing), the MSE loss in Equation 6.4 will never be zero, since \\(X_{\\text{deg}}\\) entries are not boolean. Large-degree “hub” nodes in the true graph would give a large residual, and the adjoint would subsequently fail to remove the effect of \\(B^T\\) on the edge vectors.\nIt might be possible to utilize a specific semiring, such as \\((\\min,+)\\), to enforce inner products (see Distance vs. Incidence) that take us back to a binary vector. This would be more than a simple hack, and belies a great depth of possible connection to the problem at hand. It is known that “lines” (arising from equations of the inner product) in tropical projective space are trees.[18] In addition, the tropical equivalent to Kirchoff’s polynomial (which counts over all possible spanning trees), is the direct computation of the minimum spanning tree.[19] For treatment of sparse approximation using tropical matrix factorization, see [20]\n\n[18] “The tropical grassmannian,” advg, vol. 4, no. 3, pp. 389–411, Jul. 2004, doi: 10.1515/advg.2004.023.\n\n[19] S. Jukna and H. Seiwert, “Tropical kirchhoff’s formula and postoptimality in matroid optimization,” Discrete Applied Mathematics, vol. 289, pp. 12–21, Jan. 2021, doi: 10.1016/j.dam.2020.09.018.\n\n[20] A. Omanović, H. Kazan, P. Oblak, and T. Curk, “Sparse data embedding and prediction by tropical matrix factorization,” BMC Bioinformatics, vol. 22, no. 1, Feb. 2021, doi: 10.1186/s12859-021-04023-9.\n\n[21] D. P. Wipf and B. D. Rao, “An empirical bayesian strategy for solving the simultaneous sparse approximation problem,” IEEE Transactions on Signal Processing, vol. 55, no. 7, pp. 3704–3716, Jul. 2007, doi: 10.1109/tsp.2007.894265.\nInstead, we will take an empirical bayes approach to the estimation of sparse vectors.[21] As a probabilistic graphical model, we assume each observation is emitted from a (tree-structured) Markov Random Field only defined on the activated nodes. This is underdetermined (any spanning tree could equally emit the observed activations), so we use an empirical prior as a form of shrinkage: the co-occurrences of nodes across all observed activation patterns. This let’s us optimize a likelihood from Equation 5.3, for the distribution of spanning trees on the subgraph of \\(G^*\\) inducted by \\(\\mathbf{x}\\). \\[\n\\mathbf{\\hat{r}} = \\operatorname*{argmax}_{\\mathbf{r}}{\\mathcal{L}(\\mathbf{r}|\\mathbf{x})} \\quad \\text{s.t.}\\quad \\mathbf{r}\\sim \\mathcal{T}(G^*[\\mathbf{x}])\n\\tag{6.4}\\]\n\n\nMaximum Spanning (Steiner) Trees\nThe point estimate \\(\\hat{\\mathbf{r}}\\) is therefore the mode of a distribution over trees, which is precisely the maximum spanning tree.[22] If we allow the use of all observations \\(X\\) to find an empirical prior for \\(\\mathbf{r}\\), then we can calculate a value for the mutual information for the activated nodes, and use this to directly calculate the Chow-Liu estimate. One algorithm for finding a maximum spanning tree is Prim’s[23], which effectively performs the matching pursuit technique of greedily adding an edge (i.e., non-zero entry in our vector) one-by-one. In this way, we effectively do perform matching pursuit, but minimizing the KL-divergence between observed node activations and a tree-structured MRF limited to those nodes, alone (rather than the mean-square-error).\n\n[22] R. Zmigrod, T. Vieira, and R. Cotterell, “Efficient computation of expectations under spanning tree distributions,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 675–690, Jul. 2021, doi: 10.1162/tacl_a_00391.\n\n[24] D. Hunter, “An upper bound for the probability of a union,” Journal of Applied Probability, vol. 13, no. 3, pp. 597–603, Sep. 1976, doi: 10.2307/3212481.\nHowever, the mode of the tree distribution is not strictly the one that uses mutual information as edge weights. There is reason to believe that edge weights based on pairwise joint probabilities might also be appropriate. Namely, the Hunter-Worsley bound for unions of (dependent) variables says that the sum of marginal probabilities over-counts the true union of activations (including by dependence relations). This alone would be known as Boole’s inequality, but the amount it overcounts is at most the weight of the maximum spanning tree over pairwise joint probabilities.[24] Adding the tree of joint co-occurrence probabilities is the most conservative way to arrive at the observed marginals from the probability of at least one node occurring (which could then be the “root”).\nFinally, we realize that the problem statement (“find the maximum weight tree on the subgraph”) is not the same as an MST, per-se, but rather the so-called “Steiner Tree” problem. In other words, we would like our tree of interactions to be of minimum weight on a node-induced subgraph of the true graph. The distribution of trees that our interactions are sampled from should be over the available edges in the recovered graph, which we do not yet have. Thankfully, a well-known algorithm for approximating the (graph) Steiner tree problem instead finds the minimum spanning tree over the metric closure of the graph.[25]\n\n[1] K. Avrachenkov, P. Chebotarev, and A. Mishenin, “Semi-supervised learning with regularized laplacian,” Optimization Methods and Software, vol. 32, no. 2, pp. 222–236, Mar. 2017, doi: 10.1080/10556788.2016.1193176.\nThis metric closure is a complete graph having weights given by the shortest-path distances between nodes. While we don’t know those exact values either, we do have the fact that the distance metric implied by the forest kernel (in Equation 6.1) is something of a relaxation of shortest paths. In the limit \\(\\beta\\rightarrow 0\\), \\(Q\\) is proportional to shortest path distances, while \\(\\beta\\rightarrow\\infty\\) instead gives commute/resistance distances.[1] And that kernel is counting the probability of co-occurrence on trees in any random spanning forest!\nAll this is to say that node co-occurrence measures are more similar to node-node distances in the underlying graph, not estimators of edge existence. But we can use this as an empirical prior to approximate Steiner trees that are on the true graph. For another recent application of sampling steiner trees to reconstruct node dependencies (though not for global network reconstruction), see [3].\n\n\n[3] H. Xiao, C. Aslay, and A. Gionis, “Robust cascade reconstruction by steiner tree sampling,” in 2018 IEEE international conference on data mining (ICDM), Nov. 2018, pp. 637–646. doi: 10.1109/ICDM.2018.00079.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Approximate Recovery in Near-linear Time by _Forest Pursuit_</span>"
    ]
  },
  {
    "objectID": "content/part2/2-05-forest-pursuit.html#sec-FP",
    "href": "content/part2/2-05-forest-pursuit.html#sec-FP",
    "title": "Approximate Recovery in Near-linear Time by Forest Pursuit",
    "section": "Forest Pursuit",
    "text": "Forest Pursuit\nInstantiating the above, we propose Forest Pursuit, an relatively simple algorithm for correction of clique-bias under a spreading process assumption.\n\nAlgorithm Summary\nOnce again, we assume \\(m\\) observations of activations over \\(n\\) nodes, represented as the design matrix \\(X:I\\times J \\rightarrow \\mathbb{B}\\). Like GLASSO, we assume that a Gram matrix (or re-scaling of it) is precomputed, for the non-streaming case.\nBased on the discussion in Note 4.1 we will use the cosine similarity as a degree-corrected co-occurrence measure, with node-node distances estimated as \\(d_K=-\\log{\\text{Ochiai}(j,j')}\\).5\n5  Note that any kernel could be used, given other justification, though anecdotal evidence has the negative-log-Ochiai distance performing marginally better than MI distance or Yule’s \\(Q\\).\n[25] L. Kou, G. Markowsky, and L. Berman, “A fast algorithm for steiner trees,” Acta Informatica, vol. 15, no. 2, pp. 141–145, 1981, doi: 10.1007/bf00288961.\nFor each observation, the provided distances serve to approximate the metric closure of the underlying subgraph induced by \\(\\mathbf{x}\\). This is passed to an algorithm for finding the minimum spanning tree. Given a metric closure, the MST in turn would be an approximation of the desired Steiner tree that has a total weight that will be a factor of at most \\(2-\\tfrac{2}{\\|\\mathbf{x}\\|_0}\\) worse than the optimal tree.[25] For \\(\\|\\mathbf{x}\\|_0 \\ll n\\) (i.e., many fewer authors-per paper than total authors), this error bound will be close to 1 (perfect reconstruction). The error factor bound approaches double the weight (in the worst-case) of the optimal tree as the expected number of authors-per-paper grows to infinity.\nAfter the point estimates for \\(\\mathbf{r}\\) have been calculated as trees, we can use the desire-path beta-binomial model (Equation 5.4) to calculate the overall empirical Bayes estimate for \\(\\hat{G}\\). As a prior for \\(\\alpha\\), instead of a Jeffrey’s or Laplace prior, we bias the network toward maximal sparsity, while still retaining connectivity. In other words, we assume that \\(n\\) nodes only need about \\(n-1\\) edges to be fully connected, which implies a prior expected sparsity of \\[\n\\alpha^*=\\frac{n-1}{\\tfrac{1}{2}n(n-1)} = \\frac{2}{n}\n\\tag{6.5}\\] which we can use as a sparsity-promoting initial value for \\(\\text{Beta}(\\alpha^*,1-\\alpha^*)\\).\n\n\n\\begin{algorithm} \\caption{Forest Pursuit} \\begin{algorithmic} \\Require $X\\in \\mathbb{B}^{m\\times n}, d_K\\in \\mathbb{R}_{\\geq 0}^{n\\times n}, 0&lt;\\alpha&lt;1$ \\Ensure $R \\in \\mathbb{B}^{m \\times {n\\choose 2}}$ \\Procedure{ForestPursuitEdgeProb}{$X, d_K, \\alpha$} \\State $R \\gets $\\Call{ForestPursuit}{$X, d_K$} \\State $\\hat{\\alpha}_m\\gets$\\Call{DesirePathBeta}{$X,R, \\alpha$} \\State \\textbf{return} $\\hat{\\alpha}_m$ \\EndProcedure \\Procedure{ForestPursuit}{$X, d_K$} \\State $R(\\cdot,\\cdot) \\gets 0$ \\For{$i\\gets 1, m$}\\Comment{\\textit{each observation}} \\State $\\mathbf{x}_i \\gets X(i,\\cdot)$ \\State $R(i,\\cdot) \\gets $\\Call{PursueTree}{$\\mathbf{x}_i, d_K$} \\EndFor \\State \\textbf{return} $R$ \\EndProcedure \\Procedure{PursueTree}{$\\mathbf{x}, d$} \\Comment{\\textit{Approximate Steiner Tree}} \\State $V \\gets \\{v\\in \\mathcal{V} | \\mathbf{x}(\\mathcal{V})=1\\}$ \\Comment{\\textit{activated nodes}} \\State $T \\gets$\\Call{MST}{$d[V,V]$} \\Comment{\\textit{e.g., Prim's Algorithm}} \\State $\\mathbf{u,v}\\gets \\{(j,j')\\in J\\times J | T(j,j')\\neq 0\\}$ \\State $\\mathbf{r} \\gets e_n(\\mathbf{u,v})$ \\Comment{\\textit{unroll tree adjacency}} \\State \\textbf{return} $\\mathbf{r}$ \\EndProcedure \\Procedure{DesirePathBeta}{$X,R, \\alpha$} \\State $\\mathbf{s} \\gets \\sum_{i=1}^m R(i,e)$ \\State $\\sigma \\gets \\sum_{i=1}^m X(i,j)$ \\State $\\mathbf{k} \\gets e_n(\\sigma\\sigma^T)$ \\Comment{\\textit{co-occurrence counts}} \\State $\\hat{\\alpha}_m \\gets \\alpha + \\frac{\\mathbf{s}-\\alpha \\mathbf{k}}{\\mathbf{k}+1}$ \\State \\textbf{return} $\\hat{\\alpha}_m$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nAlgorithm 1 outlines the algorithm in pseudocode for reproducibility.\n\n\nApproximate Complexity\nThe Forest Pursuit calculation presented in Algorithm 1 assumes an initial value for the distance matrix, which is similar to the covariance estimate that is pre-computed (as an initial guess) for GLASSO. Therefore we do not include the matrix multiplication for the gram matrix in our analysis, at least in the non-streaming case. Because every observation is dealt with completely independently, the FP estimation of \\(R\\) is linear in observation count. It is also trivially parallelizeable,6 and the bayesian update for \\(\\hat{\\alpha}_m\\) can be performed in a streaming manner, as well.\n6 Although, no common python implementation of MST algorithms are as-yet vectorized or parallelized for simultaneous application over many observations. We see development of non-blocking MSTs in common analysis frameworks as important future work.\n[23] R. E. Tarjan, Data structures and network algorithms, vol. 44. in CBMS-NSF regional conference series in applied mathematics, vol. 44. Society for Industrial; Applied Mathematics, 1983, pp. 72–77.\nEach observation requires a call to “PursueTree”, which involves an MST call for the pre-computed subset of distances on nodes activated for that observation. Note the use of “MST” here requires any minimum spanning tree algorithm, e.g., Prim’s, Kruskal’s, or similar. It is recommended to utilize Prim’s in this case, however, since Prim’s on a sufficiently dense graph can be made to run in \\(O(n)\\) time for \\(n\\) activated nodes by using a d-tree for its heap queue.[23] Since we are always using the metric closure, Prim’s will always run on a complete graph.\nImportantly, this means that FP’s complexity does not scale with the size of the network, but only the worst-case activation count of a given observation, \\(O(s_{\\text{max}})\\), where \\(s_{\\text{max}}=\\max_i{(\\|X(i,\\cdot)\\|_0)}\\) We say this is approximately constant in node size:\n\nThe total number of nodes is typically a given for a single problem setting\nIn many domains, the basic spreading rate of diffusion model (e.g., \\(R_0\\), or heat conductivity), does not scale with the total size of an observation\n\nThat last point means that constant scaling with network size is generally down to the domain in question. For instance, a heat equation simulated over a small area, having a given conductivity, will not have a different conductivity over a larger area; conductivity is a material property. Similarly, a virus might have a particular basic reproduction rate, or a set of authors might have a static distribution over how many collaborators they wish to work with. The former is down to viral load generation, and the latter a sociological limit: a bigger department usually does not imply more authors-per-paper by itself.\nSimilar to Equation 6.5, we might reasonably assume that the expected degree of nodes is roughly constant with network size i.e., an inherent property of the domain. So, the density of activation vectors (as a fraction of all possible edges) is going go scale with the inverse of \\(n\\). This makes our process, which is linear in activation count, out to be constant \\(O(1)\\) in network size. Then, if \\(\\bar{s}\\) is the expected non-zero count of each row of \\(X\\), the final approximate complexity of FP is \\(O(m\\bar{s})\\).7\n7  In our reference implementation, which uses Kruskal’s algorithm, the theoretical complexity is likewise \\(O(m\\bar{s}^2\\log{\\bar{s}})\\), though in our experience the values of \\(\\bar{s}\\) are small enough to not impact the runtime significantly.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Approximate Recovery in Near-linear Time by _Forest Pursuit_</span>"
    ]
  },
  {
    "objectID": "content/part2/2-05-forest-pursuit.html#sec-FP-experiments",
    "href": "content/part2/2-05-forest-pursuit.html#sec-FP-experiments",
    "title": "Approximate Recovery in Near-linear Time by Forest Pursuit",
    "section": "Simulation Study",
    "text": "Simulation Study\nTo test the performance of FP against other backboning and recovery methods, we have developed a public repository affinis containing reference implementations for FP, along with many co-occurrence and backboning techniques. The library contains source code and examples for many of the presented methods, and more. [UPDATE w/ DOI?]\nIn addition, to support the community and provide for a standard set of benchmarks for network recovery from activations, the MENDR reference dataset and testbench was developed. To make reproducible comparison of recovery algorithms easier, MENDR includes hundreds of randomly generated networks in several classes, along with random walks sampled on those networks. It can also be extended through community contribution, using data versioning to allow consistent comparison between different reports and publications over time.\n\nExperimental Method\nFor each algorithm shown in Table 6.1, every combination of the parameters in Table 6.2 was tested. 30 random graphs for each of nodes were tested, which was repeated again for each of three separate kinds of global graph structure. Every algorithm that could be supplied a prior via additive smoothing is shown in Table 6.1 as “\\(\\alpha\\)?: Yes”, and a minimum-connected (tree) sparsity prior was supplied \\(\\alpha=\\tfrac{2}{n}\\). The others, esp. GLASSO, do not have a \\(\\tfrac{\\text{count}}{\\text{exposure}}\\) form, and could not be easily interpreted in a way that allowed for additive smoothing. However, since the regularizaiton parameter for GLASSO is often critical for finding good solutions, a 5-fold cross validation was performed for each experiment to select a “best” value, with the final result run using that value. While this does have a constant-time penalty for each experiment, the reconstruction accuracy is significantly improved with this technique, and would reflect common practice in using GLASSO for this reason.\n\n\n\nTable 6.1: Summary of algorithms compared\n\n\n\n\n\n\n\n\n\n\n\n\nAlgorithm\nabbrev.\n\\(\\alpha\\)?\nclass\nsource\n\n\n\n\nForest Pursuit\nFP\nYes\nlocal\n-\n\n\nGLASSO\nGL\nNo\nglobal\n[26], [27]\n\n\nOchiai Coef.\nCS\nYes\nlocal\n[28]\n\n\nHyperbolic Projection\nHYP\nNo\nlocal\n[29]\n\n\nDoubly-Stochastic\neOT\nYes\nresource\n[30], [31]\n\n\nHigh-Salience Skeleton\nHSS\nYes\nresource\n[32]\n\n\nResource Allocation\nRP\nYes\nresource\n[33]\n\n\n\n\n[26] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3, pp. 432–441, Jul. 2008, doi: 10.1093/biostatistics/kxm045.\n\n[28] S. Janson and J. Vegelius, “Measures of ecological association,” Oecologia, vol. 49, no. 3, pp. 371–376, Jul. 1981, doi: 10.1007/bf00347601.\n\n[29] M. E. J. Newman, “Scientific collaboration networks. II. Shortest paths, weighted networks, and centrality,” Physical Review E, vol. 64, no. 1, p. 016132, Jun. 2001, doi: 10.1103/physreve.64.016132.\n\n[30] P. B. Slater, “A two-stage algorithm for extracting the multiscale backbone of complex weighted networks,” Proceedings of the National Academy of Sciences, vol. 106, no. 26, pp. E66–E66, Jun. 2009, doi: 10.1073/pnas.0904725106.\n\n[31] M. Cuturi, “Sinkhorn distances: Lightspeed computation of optimal transport,” in Proceedings of the 27th international conference on neural information processing systems - volume 2, in NIPS’13. Red Hook, NY, USA: Curran Associates Inc., 2013, pp. 2292–2300.\n\n[32] D. Grady, C. Thiemann, and D. Brockmann, “Robust classification of salient links in complex networks,” Nature Communications, vol. 3, no. 1, May 2012, doi: 10.1038/ncomms1847.\n\n[33] T. Zhou, J. Ren, M. s. Medo, and Y.-C. Zhang, “Bipartite network projection and personal recommendation,” Phys. Rev. E, vol. 76, no. 4, 4, p. 046115, Oct. 2007, doi: 10.1103/PhysRevE.76.046115.\n\n\n\nThe three classes of random graphs represent common use cases in sparse graph recovery. In addition, the block and tree graphs are types we expect GLASSO to correctly recover in this binary setting.[27] The block graphs of size \\(n\\) were formed by taking the line-graph of randomly generated trees of size \\(n+1\\).\nTrees were randomly generated using Prüfer sequences as impelmented in NetworkX [34]. To simulate possible social networks and other complex systems that show evidence of preferential attachment, scale-free graphs were sampled through the Barabási–Albert (BA) model, which was randomly seeded with a re-attachment parameter \\(m\\in\\{1,2\\}\\)[35].\n\n[27] P. Loh and M. J. Wainwright, “Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses,” in Advances in neural information processing systems, Curran Associates, Inc., 2012. doi: 10.1214/13-aos1162.\n\n[34] A. A. Hagberg, D. A. Schult, and P. J. Swart, “Exploring network structure, dynamics, and function using NetworkX,” in Proceedings of the 7th python in science conference, G. Varoquaux, T. Vaught, and J. Millman, Eds., Pasadena, CA USA, 2008, pp. 11–15.\n\n[35] A.-L. Barabási and R. Albert, “Emergence of scaling in random networks,” Science, vol. 286, no. 5439, pp. 509–512, Oct. 1999, doi: 10.1126/science.286.5439.509.\n\n\n\nTable 6.2: Experiment Settings (MENDR Dataset)\n\n\n\n\n\n\n\n\n\nparameters\nvalues\n\n\n\n\nrandom graph kind\nTree, Block, BA\\((m\\in\\{1,2\\}\\))\n\n\nnetwork \\(n\\)-nodes\n10,30,100,300\n\n\nrandom walks\n1 sample \\(m\\sim\\text{NegBinomial}(2,\\tfrac{1}{n})+10\\)\n\n\nrandom walk jumps\n1 sample \\(j\\sim\\text{Geometric}(\\tfrac{1}{n})+5\\)\n\n\nrandom walk root\n1 sample \\(n_0 \\sim \\text{Multinomial}(\\textbf{n},1)\\)\n\n\nrandom seed\n1, 2, … , 30\n\n\n\n\n\n\nEvery graph has a static ID provided by MENDR, along with generation and retrieval code for public review. New graphs kinds and sizes are simple to add for future benchmarking capability.\n\n\nMetrics\nTo compare each algorithm consistently, several performance measures have been included in the MENDR testbench. They are all functions of the True Positive/Negative (TP/TN) and False Positive/Negative (FP/FN) values.\n\n\n\n\n\n\nNote 6.1: Precision (P)\n\n\n\nFraction of positive predictions that are true, also called “positive predictive value” (PPV) \\[P= \\frac{TP}{TP+FP}\\]\n\n\n\n\n\n\n\n\nNote 6.2: Recall (R)\n\n\n\nFraction of true values that were returned as positive. Also called the TP-rate (TPR), and has an inherent trade-off with precision. \\[R=\\frac{TP}{TP+FN} \\]\n\n\n\n\n\n\n\n\nNote 6.3: Matthews Correlation Coefficient (MCC)\n\n\n\nBalances all of TP,TN,FP,FN. Preferred for class-imbalanced problems (like sparse recovery) [36] \\[\\frac{TP\\cdot TN - FP\\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\\]\n\n\n\n[36] D. Chicco and G. Jurman, “A statistical comparison between matthews correlation coefficient ( MCC), prevalence threshold, and fowlkes–mallows index,” Journal of Biomedical Informatics, vol. 144, p. 104426, Aug. 2023, doi: 10.1016/j.jbi.2023.104426.\n\n\n\n\n\n\nNote 6.4: Fowlkes-Mallows (F-M)\n\n\n\nGeometric mean of Precision and Recall, as opposed to the F-Measure that returns the harmonic mean. Also known to be the limit of the MCC as TN approaches infinity[37], which is useful as TN grows with \\(n^2\\) but TP only with \\(n\\). \\[\\sqrt{P\\cdot R}\\]\n\n\n\n[37] J. Crall, “The MCC approaches the geometric mean of precision and recall as true negatives approach infinity,” Apr. 2023, doi: 10.48550/ARXIV.2305.00594.\nBecause this work is focused on unsupervised performance, specifically for the use of these algorithms by analysts investigating dependencies, we opt to calculate TP,TN,FP,FN at every unique edge probability/strength value returned by each algorithm. Then, because we do not know a priori which threshold level will be selected by an analyst in the unsupervised setting, we take a conservative approach and report the expected values E[MCC] and E[F-M] over all unique threshold values. To consistently compare the expected values, we transform the thresholds for every experiment to the range \\([\\epsilon, 1-\\epsilon]\\), to avoid division-by-0 at the extremes.\nAnother common approach is to report the Average Precision Score (APS). This is not the average precision over the thresholds however, but instead the expected precision over the possible recall values achievable by the algorithm. It is approximating the integral under the parametric P-R curve, instead of the thresholds themselves. \\[\\text{APS} = \\sum_{e=1}^{\\omega} P(e)(R(e)-R(e-1))\\] where \\(P(e)\\) and \\(R(e)\\) are the precision and recall at the threshold set by the edge \\(e\\), in rank-order. This is more commonly done for supervised settings, however, and will report a high value as long as any threshold is able to return a both a high precision and a high recall, simultaneously.\n\n\nResults - Scoring\nThe results over every experiment are shown in Figure 6.2. Only FP is able to report MCC and F-M values with medians over about 0.5, regularly reaching over 0.8. GLASSO is clearly the second-best at recovery in these experiments, though for scale-free networks the improvement over simply thresholding the Ochiai coefficient is negligible. For APS, both GLASSO and Ochiai are equally able to return high scores, indicating at least one threshold for each that performed well. A simple mechanism for FP to perform equally well at APS is discussed in Forest Pursuit Interaction Probability.\n\n\n\n\n\n\n\n\n\nFigure 6.2: Comparison of MENDR recovery scores\n\n\n\n\n\n\nBreaking down the results by graph kind in Figure 6.3, we see the remarkable ability of FP to dramatically outperform every other algorithm in MCC and F-M, showing remarkable accuracy together with stability over the set of threshold values. This is indicative of FP’s ability to more directly estimate the support of each edge, with lower values occurring only when co-occurrences aren’t being consistently explained with the same set of incidences.\n\n\n\n\n\n\n\n\n\nFigure 6.3: Comparison of MENDR Recovery Scores by Graph Type\n\n\n\n\n\n\nAnother important capability of any recovery algorithm is to improve its estimate when provided with more data. Of course, this also will depend on other factors, such as the dimensionality of the problem (network size), and specifically for us, whether longer random walks makes network inference better or worse.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Trend: MCC vs network size\n\n\n\n\n\n\n\n\n\n\n\n(b) Trend: MCC vs observation count\n\n\n\n\n\n\n\n\n\n\n\n(c) Trend: MCC vs random-walk length\n\n\n\n\n\n\nFigure 6.4: Score trends vs problem scaling\n\n\n\n\n\nAs Figure 6.4 shows, FP is positively correlated with all three. Most importantly, the trend for FP is strongest as the number of observations increases, which is not a phenomenon seen in the other methods. In fact, it appears that count-based methods’ scores are negatively correlated with added random walk length and added observations. Only HYP and CS scores are shown in Figure 6.4, but all other tested methods (other than FP and GLASSO) show the same trend.\nHowever, because the graph sampling protocol includes \\(n\\) in the distributions for the observation count and random-walk length, we additionally performed a linear regression on the (log) parameters. The partial residual plots are shown in Figure 6.5, which shows the trends of each variable after controlling for the others.\n\n\n\n\n\n\n\n\n\nFigure 6.5: Partial Residuals (regression on E[MCC])\n\n\n\n\n\n\nThis analysis indicates that all methods should likely increase in their performance when extra observations are added, though FP does this more efficiently than either CS or GLASSO. Interestingly, CS is largely unaffected by network size, compared to FP and GL, though GL performs the worst in this regard. However, it is in the random-walk length that we see the benefit of dependency-based algorithms. The Ochiai coefficient suffers dramatically as more nodes are activated by the spreading process, since this means the implied clique size grows by the square of the number of activations. FP remains unaffected by walk-length, while (impressively) GLASSO appears to have a marginal boost in performance when walk lengths are high.\n\n\nResults - Runtime Performance\nFor both Forest Pursuit and GLASSO, runtime efficiency is critical if these algorithms are going to be adopted by analysts for backboning and recovery. Figure 6.6 shows the (log-)seconds against the same parameters from before. For similar sized networks, FP is consistently taking 10-100x less time to reach a result than GLASSO does. Additionally, many of the experiments led to ill-conditioned matrices that failed to converge for GLASSO under any of the regularization parameters tests (the “x” markers in Figure 6.6). As expected, the number of observations plot shows a clear limit in terms of controlling the lower-bound of FPs runtime, since in this serial version every observation runs one more call to MST. On the other hand, GLASSO appears to have significant banding for walk length and observation counts, likely indicating dominance of network size for its runtime.\n\n\n\n\n\n\n\n\n\nFigure 6.6: Runtime Scaling (Forest-Pursuit vs GLASSO)\n\n\n\n\n\n\nTo control for each of the variables, and to empirically validate the theoretical analysis in Approximate Complexity}, a regression of the same three (log-)parametwers was performed against (log-)seconds. The slopes in Figure 6.7, which are plotted on a log-log scale, correspond roughly to polynomial powers in linear scale. In regression terms, we are fitting the log of \\[y_{\\text{sec}} = ax_{\\text{param}}^\\gamma\\] so that the slope in a log-log plot is \\(\\gamma\\).\n\n\n\n\n\n\n\n\n\nFigure 6.7: Partial Residuals (regression on computation time)\n\n\n\n\n\n\nIn a very close match to our previous analysis, the scaling of FP is almost entirely explained by the observation count and random-walk length, alone: the coefficient on network size shows constant-time scaling. Similarly, the scaling with observation count is very nearly linear time, as predicted. The residuals show non-linear behavior for the random-walk length parameter, which would make sense, due to the theoretical \\(\\|E\\|\\log\\|V\\|\\) scaling of Kruskal’s algorithm. At this scale, \\(n\\log n\\) and \\(n^2\\log n\\) complexity might appear smaller than linear time, due to the log factor. GLASSO hardly scales with random walk length, and only marginally with observation count. In typical GLASSO, the observation count has already been collapsed to calculate the empirical covariance matrix, so its effects here might be due instead to the cross-validation and the need to calculate empirical covariance for observation subsets. The big difference, however, is GLASSO scaling in significantly superlinear time—almost \\(O(n^2)\\). This is usually the limiting factor for analyst use of such an algorithm in network analysis more generally.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Approximate Recovery in Near-linear Time by _Forest Pursuit_</span>"
    ]
  },
  {
    "objectID": "content/part2/2-06-latent-forest-alloc.html",
    "href": "content/part2/2-06-latent-forest-alloc.html",
    "title": "Modifications & Extensions",
    "section": "",
    "text": "Forest Pursuit Interaction Probability\nForest Pursuit is a flexible base for adding or modifying behavior as needed. In this chapter we demonstrate two ways to extend it, along with a probabilistic model to serve as a foundation for future extensions. First, we address the perceived shortcoming of FP compared to GLASSO in terms of APS score, showing that a simple modification to the way FP weights edge probability recovers the same performance as GLASSO (at the cost of other scores). Then we use the implicit causal dependency tree structure of each observation, together with the Matrix Forest Theorem [1], [2] to more generally define our generative node activation model. This leads to a generative model for binary activation data as rooted random spanning forests on the underlying dependency graph. Finally, we use this model to continue where FP left off by alternating between estimation of \\(R\\) an \\(B\\), in a method we call Expected Forest Maximization (EFM). EFM shows small but consistent improvement in performance over FP, at the cost of computation (and unknown convergence) time.\nWithout using stability selection[3], GLASSO is not directly estimating the “support” of the edge matrix, but the strength of each edge. to do similar with FP, we could directly estimate the frequency of edge occurrence using \\(R(i,e)\\) marginal averages, rather than conditioning on co-occurrence. Simply multiplying each FP edge probability by the co-occurrence probability of each node-node pair gives this as well, which we call FPi: the direct “interaction probability” for each pair of nodes.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modifications & Extensions</span>"
    ]
  },
  {
    "objectID": "content/part2/2-06-latent-forest-alloc.html#sec-fpi",
    "href": "content/part2/2-06-latent-forest-alloc.html#sec-fpi",
    "title": "Modifications & Extensions",
    "section": "",
    "text": "[3] N. Meinshausen and P. Bühlmann, “Stability selection,” Journal of the Royal Statistical Society Series B: Statistical Methodology, vol. 72, no. 4, pp. 417–473, Aug. 2010, doi: 10.1111/j.1467-9868.2010.00740.x.\n\nSimulation Study Revisited\nBy doing this simple re-weighting, FPi actually beats GLASSO’s median APS for the dataset, but at the cost of MCC and F-M scores (which both drop to between FP and GLASSO), as Figure 7.1 demonstrates. Similarly, the individual breakdown by graph kind in Table 7.1 shows a similar pattern, with FPi coming close to GLASSO for scale-free networks, but exceeding it for trees and matching for block graphs. Still, the difference is small enough, and at such a significant penaly to MCC and F-M scores over a variety of thresholds, that it is hard to recommend the FPi re-weighting unless rate-based edge analysis is desired, e.g. if Poisson or Exponential occurrence models are desired.\n\n\n\n\n\n\n\n\n\n\nTable 7.1: Comparing scores for FP, FPi and GLASSO\n\n\n\n\n\n\n\n\n\n\nFP\nFPi\nGL\n\n\n\n\nBlock\n\n\nAPS\n0.78\n0.9\n0.9\n\n\nF-M\n0.74\n0.57\n0.51\n\n\nMCC\n0.7\n0.55\n0.46\n\n\nScaleFree\n\n\nAPS\n0.69\n0.76\n0.81\n\n\nF-M\n0.67\n0.46\n0.44\n\n\nMCC\n0.63\n0.43\n0.36\n\n\nTree\n\n\nAPS\n0.9\n0.92\n0.88\n\n\nF-M\n0.81\n0.66\n0.53\n\n\nMCC\n0.78\n0.65\n0.49\n\n\n\n\n\n\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: FPi shows best APS, lower MCC,F-M\n\n\n\n\n\n\n\n\n\n\nSimulation Case Study\nTo illustrate what is going on, we have selected two specific experiments as a case study, in Figure 7.2. In the first, BL-N030S01, a 30-node block graph with 53 random walk samples, has FP performing worse than GLASSO and Ochiai, in terms of APS (which is reported in the legends). We see that FP shows high precision, which drops off significantly to increase recall at all. Only a few edges had high probability (which is usually desirable for sparse approximation), and some of the true edges were missed this way. However, FPi rescaling makes rarer edges fall off earlier in the thresholding, letting the recall rise by dropping rare edges, rather than simply the low-confidence ones.\nIn the second, SC-N300S01 is a 300-node scale-free network with 281 walks. Both FP and FPi show significantly better recovery capability, since enough walks have visited a variety of nodes to give FP better edge coverage. In this graph, no algorithm comes within 0.25 of FP’s impressive 0.88 APS, especially with 300 nodes and fewer than that many walks.\n\n\n\n\n\n\nFigure 7.2: P-R curves for two experiments",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modifications & Extensions</span>"
    ]
  },
  {
    "objectID": "content/part2/2-06-latent-forest-alloc.html#sec-lfa-like",
    "href": "content/part2/2-06-latent-forest-alloc.html#sec-lfa-like",
    "title": "Modifications & Extensions",
    "section": "Generative Model",
    "text": "Generative Model\nSymmetry of marked directed and undirected trees (symmetry in \\(Q\\))[1]\n\nMarked Random Spanning Forest (RSFm) distribution\nEvery spanning forest on a graph described by \\(Q\\) can be thought of as an equivalent spanning tree over a graph augmented with an extra “source” node, which is connected to every other node with a weight \\(\\tfrac{1}{\\beta}\\). Sampling random spanning trees on the augmented graph is equivalent to random spanning forests on the original.[4] We can use this fact to create a distribution for node activation sets based on co-occurrence on a rooted tree.\n\n[4] K. Avrachenkov, P. Chebotarev, and A. Mishenin, “Semi-supervised learning with regularized laplacian,” Optimization Methods and Software, vol. 32, no. 2, pp. 222–236, Mar. 2017, doi: 10.1080/10556788.2016.1193176.\nA “rooted tree” is a tree with a marked node. In Figure 7.3 we see this illustrated, where a randomly sampled tree on the graph augmented with R leads to many subtrees in the original graph. Marking one node (d) at random selects the tree that contains (d,h,e), which corresponds to record \\(x_1\\) back in Figure 5.2 (a).\n\n\n\n\n\n\n\n\n\nFigure 7.3: Dissemination plan as rooted RST on augmented graph\n\n\n\n\n\n\nNote that the marked node does not necessarily need to be the one that the source “injected” to, since the observed activations set is equivalent for any of (d,h,e) being marked. This is an important symmetry when we will not know which node “actually” started each cascade, during inference.\nIt means that the node activation set and the graph structure are conditionally independent, given a sampled spanning tree on the augmented graph. Sampling efficiently from a spanning tree distribution is a well studied problem, and we can use that efficiency in combination with a node-marking (categorical) distribution to formulate an overall distribution for node activation.\nTherefore, RSFm distribution models the probability of emitting node \\(j\\) in the \\(i\\)-th observation as the probability of occurring in the same tree as a marked “root” node \\(\\phi_{ij}\\), given a graph of \\(z_E\\) edges and a source “distance” parameter \\(\\beta\\). Since the graph is assumed to be independent of the trees sampled on it, we can use the law of total (conditional) probability:\n\\[\nP(x_{ij}|z_E, \\phi_{ij}) = \\sum_{T\\in\\mathcal{T}_{+R}}P(x_{ij}| T,\\phi_{ij}) P(T|z_E)\n\\]\nIncredibly, the probabilities for \\(P(T|z_E)\\) and \\(P(x|T,\\phi)\\) all have closed form representations. The spanning tree distribution discussed elsewhere[5], [6] can be used to motivate a spanning forest distribution, which is based on the Laplacian of the augmented graph \\(L_+\\).1 This means that the probability of a spanning tree in the augmented graph is proportional to the probability of a forest in the original, or, \\(P(T\\in\\mathcal{T}_{+R}|z_E) \\propto P(F\\in\\mathcal{F}|z_E)\\).\n\n[6] R. Zmigrod, T. Vieira, and R. Cotterell, “Efficient computation of expectations under spanning tree distributions,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 675–690, Jul. 2021, doi: 10.1162/tacl_a_00391.\n1 Here, \\(L_+(G)\\) and \\(L_+(G/e)\\) represent the Laplacian of \\(G_{+R}\\) and \\(G_{+R}\\) without edge \\(e\\), respectively.\\[\nP(x_{ij}|z_E, \\phi_{ij}) = \\sum_{F\\in\\mathcal{F}}P(x_{ij}| F,\\phi_{ij}) P(F|z_E)\n\\]\nUsing this equivalence on Kirchoff’s matrix tree theorem gives the Chebotarev-Shamis Forest Theorem[1] for the partition function over spanning forests \\(Z_\\mathcal{F}\\), which gives a closed form for Equation 5.2 over all spanning forests (not just spanning trees on a subgraph). If \\(\\mu(F)\\) is a measure on forests, e.g. the product of weights of edges, then:\n\n[1] P. Chebotarev and E. Shamis, “The matrix-forest theorem and measuring relations in small social groups,” arXiv, arXiv:math/0602070, Feb. 2006. doi: 10.48550/arXiv.math/0602070.\n\\[\n\\begin{gathered}\nP(F|z_E) = \\frac{1}{Z_\\mathcal{F}} \\mu(F)\\\\\nZ_\\mathcal{F} = \\det{(I+\\beta L)}\n\\end{gathered}\n\\]\n        \n\nThere are a variety of techniques already cited for sampling from this distribution, because of the connection with spanning trees on an augmented graph. This means we can sample from RSFm with only one modification from normal spanning tree distributions:\n\nGenerate random spanning trees on \\(G_{+R}\\), according to \\(P(F|z_E)\\).\nUniformly sample a marked node \\(\\phi\\).\nActivate all nodes connected to \\(\\phi\\) in the induced spanning forest.\n\nTo do inference, however, we may again use the forest kernel \\(Q\\), which gives us probabilities for node co-occurrence on trees in a spanning forest. We only need to combine entries of Q for the observed nodes to determine the likelihood of an observation. \\(\\sum_\\mathcal{F} P(x_{ij}|F,\\phi_{ij})\\) would be the probability of a node occurring (or not) on the same subtree as the marked node, and as previously discussed, this is just \\(Q(u\\in V_i,\\phi)\\) for each node that is activated, and \\(1-Q(v\\notin V_i,\\phi)\\), for each that is not. Note that this assumes we marginalize over a uniformly distributed \\(\\phi\\).\n\n\nModel Specification\n\n\n\n\nThe overall model that fills out the gaps left in Equation 5.5 has the form:\n\\[\n\\begin{aligned}\n\\pi_{e\\in E} &\\sim \\operatorname{Beta}_E(\\alpha, 1-\\alpha)     \\\\\nz_{e\\in E} &\\sim \\operatorname{Bernoulli}_E(\\pi_e)             \\\\\n\\phi_{i\\in I, j\\in J} &\\sim \\operatorname{Categorical}_I(\\theta) \\\\\nx_{i\\in I,j\\in J} &\\sim \\operatorname{RSFm}_K(\\beta, z_e, \\phi_n) \\\\\n\\end{aligned}\n\\]\n\nThe above still has the problem of needing an estimate for \\(Q=(I+\\beta L_z)\\) based on the graph described by the active edges in \\(z\\). It could be done with a monte-carlo sampler, though the nested likelihood may prove difficult.\nThe conditional independence mentioned previously could lend itself to a collapsed Gibbs-sampling scheme. Each observation is a marked node and a sampled random spanning forest, which can equivalently be described as a sampled spanning tree over the activated nodes, like originally discussed in Equation 5.2. This works because we can marginalize out the equal marked-node probability \\(\\phi\\) if we assume a uniform probability of selection for each node, and because we can jointly use the desire-path model to derive an estimate for z_E from the internal edge samples of RSFm.\nBeginning with the FP point estimate, each edge in every spanning subtree can be efficiently resampled according to the Bayesian Spanning Tree distribution from [5]. Once every edge in a tree has been resampled, the overall estimate for the desire path network can be updated, and sampling can continue. This would be very similar to the way collapsed Gibbs sampling works for Latent Dirichlet Allocation[7], but with edges selected from a spanning forest distribution instead of “topics” from a multinomial. Derivations and implementation of such a scheme is left for future work.\n\n[5] L. L. Duan and D. B. Dunson, “Bayesian spanning tree: Estimating the backbone of the dependence graph,” arXiv, arXiv:2106.16120, Jun. 2021. doi: 10.48550/arXiv.2106.16120.\n\n[7] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal of machine Learning research, vol. 3, no. Jan, pp. 993–1022, 2003, doi: 10.7551/mitpress/1120.003.0082.",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modifications & Extensions</span>"
    ]
  },
  {
    "objectID": "content/part2/2-06-latent-forest-alloc.html#expected-forest-maximization",
    "href": "content/part2/2-06-latent-forest-alloc.html#expected-forest-maximization",
    "title": "Modifications & Extensions",
    "section": "Expected Forest Maximization",
    "text": "Expected Forest Maximization\nAnother possibility is to approach the problem as a kind of matrix factorization, jointly estimating \\(B\\) and \\(R\\) in an alternating manner. Where Forest Pursuit was an empirical Bayes estimate for \\(R\\), alternating from there between \\(B\\) and \\(R\\) leads to a simple Expectation Maximization scheme:\n\nAlternate embedding the node activations as edge activations and combining these under a desire path model to estimate the graph structure.\n\n\nFactorization & Dictionary Learning\nJointly finding a sparse representation of data using a linear combination of basis vectors (called a “dictionary”), and finding an optimal dictionary with which to do the embedding, is called “sparse dictionary learning”. One of the original proposed methods to solve this was the Method of Optimal Directions (MOD)[8], which uses a sparse “pursuit”-like method to find the representation vectors, and then uses that representation to find \\(\\hat{B}\\gets XR^+\\).2 However, our desire path estimation for independent (arcsine) Bernoulli edges gives an efficient workaround to needing the pseudo-inverse. Based on our interpretation of the forest kernel, counting co-occurrences on trees is equivalent to estimating the inverse of the regularized graph laplacian, anyway. Each Forest Pursuit estimate can yield a new distance \\(d_Q(u,v)\\), which can be used as a regularized shortest-path distance for small enough \\(\\beta\\). Then, new steiner tree estimates will approximate the modes of each spanning tree distribution, which can be used for a new desire-path estimate of the edges, and so-on.\n\n[8] K. Engan, S. O. Aase, and J. Hakon Husoy, “Method of optimal directions for frame design,” in 1999 IEEE international conference on acoustics, speech, and signal processing. Proceedings. ICASSP99 (cat. no.99CH36258), IEEE, 1999. doi: 10.1109/icassp.1999.760624.\n2  \\(R^+\\) is the pseudo-inverse of \\(R\\).\nProbabilistically, we are finding the expectation over spanning forests (collapsing over uniform \\(\\phi\\)) in the form of calculating \\(Q\\), and then maximizing the likelihood of each observation through Forest Pursuit. The proposed algorithm Expected Forest Maximization is outlined in Algorithm 1.\n\n\n\\begin{algorithm} \\caption{Expected Forest Maximization (EFM)} \\begin{algorithmic} \\Require $X\\in \\mathbb{B}^{m\\times n}, d_K\\in \\mathbb{R}_{\\geq 0}^{n\\times n}, 0&lt;\\alpha&lt;1$ \\Ensure $R \\in \\mathbb{B}^{m \\times {n\\choose 2}}$ \\Procedure{EFM}{$X, d_K, \\alpha, \\beta, \\epsilon$} \\State $R \\gets $\\Call{ForestPursuit}{$X, d_K$} \\State $\\hat{\\alpha}_m\\gets$\\Call{DesirePathBeta}{$X,R, \\alpha$} \\While {$\\|\\hat{\\alpha}-\\alpha\\|_{\\infty}&gt;\\epsilon$} \\State $\\alpha_m \\gets \\hat{\\alpha}_m$ \\State $Q \\gets (I+\\beta L_{\\text{sym}}(\\alpha_m))^{-1}$ \\State $d_K \\gets d_Q$ \\State $R \\gets $\\Call{ForestPursuit}{$X, d_K$} \\State $\\hat{\\alpha}_m\\gets$\\Call{DesirePathBeta}{$X,R, \\alpha_m$} \\EndWhile \\State \\textbf{return} $\\hat{\\alpha}_m$ \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\nIn practice, we limit iterations to less than 100, though that limit is not reached in our testing. We use a small \\(\\beta=10^{-3}\\) to approximate shortest path distances, and we can do so within the forest pursuit loop to avoid inverting large matrices. Instead of \\(d_K\\), the graph structure itself (once an initial estimate is made) can be passed to Forest Pursuit, where each subgraph can estimate \\(d_{Ki}\\) independently. After all, shortest paths in a subgraph when we assume no hidden nodes will be the same as in the global graph. Note as well the use of the symmetric normalized Laplacian \\(L_{\\text{sym}}\\). We use this to enable the splitting of inversions like this, because the global Laplacian has higher possible degrees than the subgraph, and we do not wish to bias Steiner tree estimation by node degree. Not doing so leads to rapid divergence of the loss function, as all MSTs will tend toward star-graphs around the highest-degree node.\n\n\nEFM Simulation Study\nOne-shot Forest Pursuit appears to perform quite well, so it’s useful to quantify the expected gain in performance by repeating it an unknown number of times. There are no generic guarantees for EM convergence, though anecdotally the number of iterations was limited in our experiments to under a thousand, and that limit was never hit while using a covergence parameter of \\(\\epsilon=1\\mathrm{e}-5\\).\nThe distribution of E[MCC] score change vs. FP is shown in Figure 7.4.\n\n\n\n\n\n\n\n\n\nFigure 7.4: Change in Expected MCC (EFM vs FP)\n\n\n\n\n\n\nWhile useful, it’s not clear whether individual edges are more likely to be “true” edges, given a bigger change in EFM score. To test this, a logistic regression was performed for every experiment in MENDR against the true edge values, using the change in scores on those edges between FP and EFM as training data. To avoid overfitting, a significant amount of regularization was applied, chosen using 5-fold crossvalidation. The coefficients for all experiments are shown as a histogram in Figure 7.5.\n\n\n\n\n\n\n\n\n\nFigure 7.5: Logistic Regression Coef. (EFM - FP) vs. (Ground Truth)\n\n\n\n\n\n\nThe graph kind did not make a significant difference to EFM improvement, but overall log-odds improvement is very low.Still, the value is positive accross the entire dataset, so EFM does have a very small-but-nonzero impact on improving edge prediction.\nThe runtime graphs can also be updated, with EFM shown in Figure 7.6 and Figure 7.7 against FP and Glasso. EFM still ran significantly faster than GLASSO in this region. However, the scaling with network size is no longer constant-time, especially since convergence used above is the max-abs error, which requires that every node reach a minimum level of convergence and might take much longer, overall.\n\n\n\n\n\n\n\n\n\nFigure 7.6: Runtime Scaling (Forest-Pursuit vs GLASSO)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.7: Partial Residuals (regression on computation time)",
    "crumbs": [
      "Nonparametric Network Recovery With Random Spanning Forests",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Modifications & Extensions</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-qualitative.html",
    "href": "content/part3/3-07-qualitative.html",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "",
    "text": "Network Science Collaboration Network\nA more realistic test of Forest Pursuit’s behavior is in unsupervised, qualitative analysis of complex systems and networks. In this chapter we will recreate two well-known bipartite datasets from network science, and compare several network recreation methods on them. As we have discussed previously[1], there is some interest in quantifying network assortativity for social networks, and whether the positive assortativity effect is a methodological artifact. Assuming an analyst is interested in social network dependencies—like which nodes are directly influencing which others—we provide a glimpse at how correcting for clique bias with Forest Pursuit (FP) affects assortativity.\nIn the second case study, we also investigate how centrality measures can be adversely affected when clique-bias is not accounted for, especially when appearance in cliques is known to be rare for certain node types.\nFollowing in the steps of [2], we recreate a network from co-authorships in works cited by two literature review papers, [3] and [4]. The list of all cited papers was reconstructed via web-of-science queries, and these were used to construct author “activation” observations.1 Nodes and papers were retained only if they had two or more papers and authors listed, respectively. A baseline co-authorship network was initially constructed using the largest connected component, and a modularity-maximizing community detection algorithm [[6];Findingevaluatingcommunity_Newman2004] was used to highlight community structure. As recommended in [2], we also show a reduced course-grained “quotient” graph of the communities and their interconnection to help visualize and understand the large-scale structure each network presents.\nThis baseline network can be seen in Figure 8.1, where (to keep with the treatment in [2]) we have added a quotient network of the communities themselves.\nThe detected assortativity \\(r=0.059\\) is small, but positive. As expected, each community tends to have a dominant “clique”, since many members of these groups tend to mutually author papers together (simultaneously).\nHowever, if we are after a “social network” that describes social influence of individuals with respect to others, then we are hoping to recover collaborative dependency relationships.\nWith some background knowledge of how university labs tend to work (with students, post-docs, advisors, colleagues, etc.), we might not believe that every member of a community has significant influence on the writing of every other member. So, to estimate a dependency network of collaboration relationships, we could turn to a Chow-Liu tree, which is shown in Figure 8.2.\nThis dependency is much more sparse, and indeed the assortativity coefficient has dropped to \\(r=-0.251\\). In academic writing, we might even expect a lowered assortativity, since students that may not go on to be prolific in their original field would still seek out prolific advisors, initially. Unfortunately, enforcing a tree structure has some negative side effects. Overlaying the original community structure from Figure 8.1 onto the tree and calculating a quotient graph shows that the community structure is impacted by the change. Many of the communities are “unrolled” into long chains of authors, since trees cannot allow small loops or cycles. This makes for a shortest-path distance between community hubs (in the same field) to be upwards of 9-10 jumps, which goes against the scale-free/small-world nature we expect from social systems.\nWith Forest Pursuit, we can attempt to correct for “clique bias” in the measurement of dependency relationships, while allowing for flexibility in the global network structure. The FP recovered collaborator graph is shown in Figure 8.3.\nThe communities from the co-occurrence network are entirely preserved, with a nearly identical community structure in the graph quotient compared to the co-occurrence graph. But now we see an assortativity that is close to zero (slightly negative at \\(r=-0.069\\)). This brings the assortativity more in line with the World Wide Web networks, or even close to results for random preferential attachment networks that are zero in the limit[7].\nRelative to the tree network, there are not so many long chains, as many authors have been allowed to “loop-back” and have relationships with nearby colleagues, reducing the path distance between communities in the process. Still, like the trees, FP tends to reduce the degree of each node, which is summarized in Figure 8.4.\nFrom a domain modeling perspective, it might be reasonable to assume that any given author has 2-3 influential collaboration relationships shown, especially considering students and post-docs are likely to constitute a good number of nodes. Even from a logistics perspective (during a given time window working with an advisor) the number of times a student asks/is asked to participate in a paper has to be limited, given average publishing rates.\nOnly rare individuals would have upwards of 10 influential relationships, with a mean closer to the 2-3 of the FP network, rather than a full quarter having 10 and the median being 4-5 (as in the co-occurrence network). In addition, recall that this is a social network derived from literature review paper citations, not exhaustive inventories of each author’s work, so every influential relationship should not be shown in this sample.\nSo, which network is preferable? It depends.\nThe premise of FP is to find a max. likelihood network such that the observed activations (authors on a paper) arise from a random walk along dependencies (author relationships of influence causing others to join). If influential (dependency) relationships are being measured, then the logistics of maintaining 2-3 of them seems more realistic than 5-10 (especially only from the sampled papers). Additionally, we expect students having few influential relationships would attach preferentially to advisors with more, so a positive assortativity might be problematic. FP agrees with these conclusions, making it consistent with that set of a priori domain beliefs vs. the co-occurrence network alone. These kinds of qualitative assessments are not rigorous, but the modeling questions required to choose FP (or not) go a long way to reducing error in trueness. They help modelers validate whether the recovered network was made in a consistent way with their domain knowledge.",
    "crumbs": [
      "Applications & Case Studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-qualitative.html#network-science-collaboration-network",
    "href": "content/part3/3-07-qualitative.html#network-science-collaboration-network",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "",
    "text": "[3] M. E. J. Newman, “The structure and function of complex networks,” SIAM Review, vol. 45, no. 2, pp. 167–256, Jan. 2003, doi: 10.1137/s003614450342480.\n\n[4] S. BOCCALETTI, V. LATORA, Y. MORENO, M. CHAVEZ, and D. HWANG, “Complex networks: Structure and dynamics,” Physics Reports, vol. 424, no. 4–5, pp. 175–308, Feb. 2006, doi: 10.1016/j.physrep.2005.10.009.\n1  An additional paper ([5]) was added to ensure the the largest connected component contained necessary communities that were originally missing compared to [2].\n[5] K. Sneppen and M. E. J. Newman, “Coherent noise, scale invariance and intermittency in large systems,” Physica D: Nonlinear Phenomena, vol. 110, no. 3–4, pp. 209–222, Dec. 1997, doi: 10.1016/s0167-2789(97)00128-0.\n\n[2] M. E. J. Newman and M. Girvan, “Finding and evaluating community structure in networks,” Physical Review E, vol. 69, no. 2, p. 026113, Feb. 2004, doi: 10.1103/physreve.69.026113.\n\n[6] A. Clauset, M. E. J. Newman, and C. Moore, “Finding community structure in very large networks,” Physical Review E, vol. 70, no. 6, p. 066111, Dec. 2004, doi: 10.1103/physreve.70.066111.\n\n\n\n\n\n\n\n\n\nFigure 8.1: 134 Network scientists connected by co-authorship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.2: Chow-Liu tree of NetSci collaborator dependency relationships\n\n\n\n\n\n\n\n\n\nWho is causing who to join papers?\nWho are the central “collaborators” that influence the paper writing of many others?\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.3: Forest Pursuit estimate of NetSci collaborator dependency relationships\n\n\n\n\n\n\n\n\n\n[7] M. E. J. Newman, “Assortative mixing in networks,” Physical Review Letters, vol. 89, no. 20, p. 208701, Oct. 2002, doi: 10.1103/physrevlett.89.208701.\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Degree distributions of FP vs co-occurrence social networks",
    "crumbs": [
      "Applications & Case Studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-07-qualitative.html#les-misérables-character-network",
    "href": "content/part3/3-07-qualitative.html#les-misérables-character-network",
    "title": "Qualitative Application of Relationship Recovery",
    "section": "Les Misérables Character Network",
    "text": "Les Misérables Character Network\nAnother famous network derives from [8] via mining character co-occurrences in chapters of Les Misérables (1862), Victor Hugo’s sprawling saga of [in]equality and [in]justice in 19th-century France. This dataset is often reported as being inherently a graph, but we have reconstructed the underlying bipartite observations for the purpose of removing clique bias from the network reconstruction. In the original network, an extra count is added to the weight of an edge every time a character occurs in a chapter with another. Once again, we have only retained characters/chapters that appeared with two or more chapters/characters, respectively. In Figure 8.5, we run the same community detection scheme to improve the visual understandability of the network, along with inset quotient graphs.\n\n[8] D. E. Knuth, The stanford GraphBase: A platform for combinatorial computing, vol. 1. AcM Press New York, 1993.\n\n\n\n\n\n\n\n\n\nFigure 8.5: Les Miserables character co-occurrence network\n\n\n\n\n\n\nNote that the communities demonstrate clear clique-like behavior. On one hand, this does make assessment of clusters easier, since characters in roughly the same scenes are densely grouped together. However, this network makes it difficult to parse relationships, such that backboning would become necessary.\nWe should also ask what we want out of this “social” network. Another way we might think about a social network of characters is “which characters influence the appearance (or not) of which other characters?” By extension, “which characters are significant, in the sense that they dictate the appearance of more characters than others?” From a domain modeling perspective, this is like asking how an author is deciding which characters to include in a chapter. By not correcting for clique-bias, we are implicitly assuming that Victor Hugo would be setting out to write a chapter and immediately writes down a list of every character, independently, should appear. Instead, it’s likely that the desired appearance of certain characters in a chapter leads to the inclusion of others, as the plot requires.\nBecause the inclusion of certain characters leads to the inclusion of others (by this model), we can reasonably model the authorial “character inclusion process” as spreading from character to character. With this in mind, we apply Forest Pursuit to correct for clique-bias, and show the resulting dependency network (with the original community partition) in Figure 8.6.\n\n\n\n\n\n\n\n\n\nFigure 8.6: Les Miserables character dependency network (Forest Pursuit)\n\n\n\n\n\n\nThe network edge probabilities have been thresholded in the same manner as the DS (minimum connectivity) filter, only including necessary edges to still retain overall connectivity. As before, our community structure shown in the quotient graph has been preserved, even with the significant edge density reduction.\nApplying these networks, we might wish to distinguish through them which characters are “main” and which are “supporting”, though more on a gradient than in a binary/classification sense. One way to do this is through centrality measures [9], [10], which estimate the “importance” of nodes in various ways. Eigenvector centrality, specifically, finds the importance of nodes relative to the importance of each node’s neighbors2 It does this by estimating the stationary distribution of a random walk on the network (what is the probability after many jumps of ending up in each node?)\n\n[9] M. Newman, Mathematics of networks. Oxford University Press, 2018. doi: 10.1093/oso/9780198805090.003.0006.\n\n[10] M. Coscia, The atlas for the aspiring network scientist. Michele Coscia, 2021. Available: https://arxiv.org/abs/2101.00863\n2  Regularized eigenvector centrality is equivalent to PageRank centrality, i.e. the seminal work from [11], of Google fame.\n[11] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation ranking: Bringing order to the web.” Stanford InfoLab; Stanford InfoLab, Technical Report 1999-66, Nov. 1999. Available: http://ilpubs.stanford.edu:8090/422/\n\n[12] S. Fortunato, M. Boguñá, A. Flammini, and F. Menczer, “Approximating PageRank from in-degree,” in Algorithms and models for the web-graph, Springer Berlin Heidelberg, pp. 59–71. doi: 10.1007/978-3-540-78808-9_6.\nThe topology of a network greatly impacts the centrality measure recovered. In fact, a well-known result shows that much of the variance in eigenvector/PageRank centrality values can be explained with node in-degree alone.[12] Of course, we do tend to see nodes with high-degree as hubs, and more important for it—that is, we would if we weren’t so concerned with having to deal with clique bias in the network. The tendency of eigenvector centrality to rely on degree (and therefore reward densely-connected cliques with high importance) prevents us from using it over more expensive methods like betweenness centrality. Instead, we might be able to differentiate between high-degree nodes in cliques and “actual” hubs by controlling for clique bias ahead of time with Forest Pursuit.\n\nIn Figure 8.7 we show a bump-plot of the ranks of the top 15 most central nodes for both the original co-occurrence network and the FP network estimate. In the original network, centrality rewards characters appearing in mutual cliques as being central, especially the “revolutionary” group (shown in orange in Figure 8.6 and Figure 8.5). Are these really the most important characters? Important characters like Cosette and Fantine are nowhere near the top. We wish to find important characters in the sense that they have influence over the appearance of many other characters. In that case, even Javert (though being an important recurring character) largely plays the role of generic law enforcement throughout, not necessarily pulling other characters in or out of scenes with him as much as others like Marius. More than Javert, Marius plays central motivating roles across several distinct social groups throughout the novel. All of these problems are corrected to some degree in the FP estimate.\n\n\n\n\n\n\n\n\n\nFigure 8.7: Changes in character centrality ranking for FP vs co-occurrence\n\n\n\n\n\n\nWhy are the centrality levels for Fantine, Eponine, and Cosette so much higher in FP? Why were they so low in the first place? This brings up another benefit of correcting clique-bias: estimating character importance despite systematic interaction-reduction bias from authors. As made famous by the so-called “Bechdel test” [13], authors tend to systematically reduce the agency and interaction rates of female characters, an effect that has been studied from many angles covering multiple centuries [14]. We highlight every female character in Figure 8.7 (bold/thicker paths), which shows that the co-occurrence network had one single female character in the top 15 most central (Eponine, ranked 14th). This is because their degrees are overshadowed by large cliques! By correcting for clique-bias, every female character significantly increases in centrality ranking This way, the occurrence of many characters can be explained through these female characters, rather than assuming each character separately connects with every other, biasing importances toward large groups of men.\n\n[13] C. van Raalte, “1. No small-talk in paradise: Why elysium fails the bechdel test, and why we should care,” in Media, margins and popular culture, 1st ed., H. Savigny, E. Thorsen, D. Jackson, and J. Alexander, Eds., London: Imprint: Palgrave Macmillan, 2015.\n\n[14] O. Stuhler, “The gender agency gap in fiction writing (1850 to 2010),” Proceedings of the National Academy of Sciences, vol. 121, no. 29, Jul. 2024, doi: 10.1073/pnas.2319514121.\nIn context, titles of entire books and volumes of Les Misérables are named after women we would consider crucial to the plot (e.g. Fantine, Cosette, and Eponine).\nAs [14] indicates, however, authors are likely to under-represent their agency and interactions with other characters throughout the story, so networks based primarily on co-occurrence will likewise systematically underestimate their importance.",
    "crumbs": [
      "Applications & Case Studies",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Qualitative Application of Relationship Recovery</span>"
    ]
  },
  {
    "objectID": "content/part3/3-08-ordered.html",
    "href": "content/part3/3-08-ordered.html",
    "title": "Recovery from Working Memory & Partial Orders",
    "section": "",
    "text": "Technical Language Processing with INVITE\nThis whole time we have assumed that the ordering of nodes was unknown, or at the very least unreliable. However, there are frequently cases, especially in text processing applications, where we have some sense of an ordering on activations. By partial order on a set (a “poset”), we mean that all elements are either comparable as greater (before) or less (after), or incomparable. The set of posets is therefore precisely isomorphic to the set of directed acyclic graphs, based on reachability.\nOur original example with authors might be thought of as a poset: (i) precedes/asks (f) and (j), (j) precedes/asks (b), but (b) and (f) are incomparable. We don’t know if (i) asked (f) before or after (j) asked (b).1 We have updated the original example with explicit partial orders in Figure 9.1. Note that nodes in some lists could be re-arranged while keeping the partial order the same (keeping all arrows pointing to the right).\nSadly our co-authorship example does not often include the (partial) order of author additions,2 but other common network recovery problems do have an inherent order to them. A very common need is to recover concept association networks, whether from lists of tags or directly from a corpus of written language. What’s needed is an assumption on how the observed partial order of concepts is generated. [2] proposes a “foraging” mechanism, so that concepts get sequentially recalled from “semantic patches” of nearby concepts in memory. The partial order comes from our ability to maintain more than one concept in working memory, so that the next concept can be “foraged” from any of the other recently recalled ones[3], [4].\nIn this section, we briefly cover a method for network inference by [5] that utilizes partial order information from ordered lists of concepts, called INVITE. We use it to demonstrate improvement over bag-of-words and markov assumptions for downstream technical language processing [6] tasks, as originally demonstrated in [7], and [8]\nFinally, we show that using Forest Pursuit for partially ordered data can still be quite useful for network backboning, and for a fraction of the computational cost. We investigate a network recovery task from verbal/semantic fluency data [9], which involves recovery of a network of animal relationships from memory and recall experiments. Even without directly using partial order information, proper data preparation along with previously-discussed (un-ordered) recovery methods can lead to significantly improved network backboning and analysis capability\nMaintenance work orders are often represented as categorized data, though increasingly quantitative use of a technician’s descriptions is being pursued by maintenance management operations [10], [11]. Tags are another way to flexibly structure this otherwise “unstructured” data, which Table 9.1 shows in comparison to more traditional categorization.\nWhether entered directly or generated from text by keyword extraction, the tags will tend to have ordering information readily available. A traditional way to model this kind of text is through either bag-of-words (the co-occurrence node activation data already discussed) or as a sequence of order-n markov model emissions. An order-n markov model \\(\\text{MC}n\\) estimates the probability of observing the \\(i\\)th item \\(t_i\\) in a sequence \\(T\\) as \\[\nP(t_i|T) \\approx P(t_i | t_{i-1}, \\cdots,t_{i-n})\n\\]\nUnlike the clique bias from before, assuming markov jumps for each observation leads to a different kind of bias, with higher precision but reduced recall as shown in Figure 9.2.\nWithout knowing the underlying dependency relationships, it’s difficult to estimate which edges were used by a random-walker, since subsequent visits in memory to a “tag” are not being reported once a technician first adds it. [5] call this an “Initial-visit-emitting” random walk, or INVITE for short. To more accurately recover the network, they suggest maximizing the absorption probability for each step of a partial order, individually, knowing which nodes have already been activated.",
    "crumbs": [
      "Applications & Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Working Memory & Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-08-ordered.html#technical-language-processing-with-invite",
    "href": "content/part3/3-08-ordered.html#technical-language-processing-with-invite",
    "title": "Recovery from Working Memory & Partial Orders",
    "section": "",
    "text": "[11] R. Sexton, M. Hodkiewicz, and M. P. Brundage, “Categorization errors for data entry in maintenance work-orders,” Annual Conference of the PHM Society, vol. 11, no. 1, Sep. 2019, doi: 10.36001/phmconf.2019.v11i1.790.\n\n\n\n\n\nTable 9.1: Maintenance Work Order as categorized vs. tagged data\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9.2: Partial-order edge measurements with Markov assumption\n\n\n\n\n\n\n\n\nOptimizing absorbing-state probabilities\nSay the set of components or concepts that have a corresponding tag in our system is denoted by the node-set \\(N\\). A user-given set of \\(T\\) 3 for a specific record can be denoted as a Random Walk (RW) trajectory \\(\\mathbf{t}=\\{t_1, t_2, t_3, \\cdots t_{T}\\}\\), where \\(T\\leq N\\). This limit on the size of \\(T\\) assumes tags are a set of unique entries. Any transitions between previously visited tags in \\(\\mathbf{t}\\) will not be directly observed, making the transitions observed in \\(\\mathbf{t}\\) strictly non-Markov, and allowing for a potentially infinite number of possible paths to arrive at the next tag through previously visited ones. Instead of directly computing over this intractable model for generating \\(\\mathbf{t}\\), the key insight from the original INVITE paper [5] comes from partitioning \\(\\mathbf{t}\\) into \\(T-1\\) Markov chains with absorbing states. Previously visited tags are “transient” states, and unseen tags are “absorbing”. It is then possible to calculate the absorption probability into the \\(k\\) transition (\\(t_k \\rightarrow t_{k+1}\\)) using the fundamental matrix of each partition. If the partitions at this jump consist of \\(q\\) transient states with transition matrix among themselves \\(\\mathbf{Q}^{(k)}_{q\\times q}\\), and \\(r\\) absorbing states with transitions into them from \\(q\\) as \\(\\mathbf{R}^{(k)}_{q\\times r}\\), the Markov transition matrix \\(\\mathbf{M}^{(k)}_{n\\times n}\\) has the form \\[\n\\mathbf{M}^{(k)} =\n    \\begin{pmatrix}\n        \\mathbf{Q}^{(k)}  & \\mathbf{R}^{(k)} \\\\\n        \\mathbf{0}        & \\mathbf{I}\n    \\end{pmatrix}\n\\tag{9.1}\\]\n3 While some sources use “tagging” as a proxy for a set of strictly un-ordered labels (as in multi-label classification), we preserve the mechanism by which the tags were generated in the first place, i.e., in a specific order.\n[5] K.-S. Jun, J. Zhu, T. T. Rogers, Z. Yang, et al., “Human memory search as initial-visit emitting random walk,” Advances in neural information processing systems, vol. 28, no. 20, pp. 2389–2393, 2015, doi: 10.1016/j.physleta.2019.04.060.\n\n[12] P. G. Doyle and J. L. Snell, “Random walks and electric networks.” arXiv, 2000. doi: 10.48550/ARXIV.MATH/0001057.\nHere \\(\\mathbf{0}\\), \\(\\mathbf{I}\\) represent lack of transition between/from absorbing states. It follows from [12] that the probability \\(P\\) of a chain starting at \\(t_k\\) being absorbed into state \\(k+1\\), is given as \\[\n\\begin{gathered}\n    P\\left(t_{k+1} \\middle| t_{1:k},\\mathbf{M}\\right) =\n        \\left.\\mathbf{N}^{(k)}R^{(k)}\\right|_{q,1}\\\\\n\\mathbf{N} = \\left( \\mathbf{I}-\\mathbf{Q} \\right) ^{-1}\n\\end{gathered}\n\\tag{9.2}\\]\nThe probability of being absorbed at \\(k+1\\) conditioned on jumps \\(1:k\\) is thus equivalent to the probability of observing the \\(k+1\\) INVITE tag. If we approximate an a priori distribution of tag probabilities to initialize our chain as \\(t_1\\sim\\text{Cat}(n,\\theta)\\) (which could be empirically derived or simulated), then the likelihood of our observed tag chain \\(\\mathbf{t}\\), given a transition matrix, is: \\[\n\\mathcal{L}\\left(\\mathbf{t}| \\theta, \\mathbf{M}\\right) =\n        \\theta(t_1)\\prod_{k=1}^{T-1} P\\left(t_{k+1}\\,\\middle|\\ t_{1:k},\\mathbf{M}\\right)\n\\]\nFinally, if we observe a set of tag lists \\(\\mathbf{C} = \\left\\{ \\mathbf{t}_1, \\mathbf{t}_2, \\cdots, \\mathbf{t}_{c} \\right\\}\\), and assume \\(\\theta\\) can be estimated independently of \\(\\mathbf{M}\\), then we can frame the problem of structure mining on observed INVITE data as a minimization of negative log-likelihood. A point estimate for our association network given \\(\\mathbf{M}\\) can found as: \\[\n    \\mathbf{M}^* \\leftarrow \\operatorname*{argmin}_{\\mathbf{M}} \\quad\n    \\sum_{i=1}^{C}\n    \\sum_{k=1}^{T_i-1}\n        -\\log \\mathcal{L} \\left(t^{(i)}_{k+1} \\middle| t^{(i)}_{1:k},\\mathbf{M}\\right)\n\\]\nThis (deeply nested) likelihood can now be optimized using standard solvers, and our reference implementation uses stochastic gradient descent via PyTorch [13].4\n\n[13] A. Paszke et al., “Automatic differentiation in PyTorch,” in NIPS-w, 2017.\n4  The n-gram markov transition models (MC1,MC2) trained for comparison vs INVITE were trained using pomegranate[14].\n[14] J. Schreiber, “Pomegranate: Fast and flexible probabilistic modeling in python,” Journal of Machine Learning Research, vol. 18, no. 164, pp. 1–6, 2018.\n\n\nApplication: Mining Excavator MWOs\nTo assess the applicability of the INVITE-based similarity measure to real-world scenarios, we apply our model to tags annotated for a mining dataset pertaining to 8 similarly-sized excavators at various sites across Australia [15], [16].\n\n[15] M. R. Hodkiewicz, Z. Batsioudis, T. Radomiljac, and M. T. Ho, “Why autonomous assets are good for reliability–the impact of ‘operator-related component’failures on heavy mobile equipment reliability,” in Annual conference of the prognostics and health management society 2017, 2017.\n\n[16] M. Hodkiewicz and M. T.-W. Ho, “Cleaning historical maintenance work order data for reliability analysis,” Journal of Quality in Maintenance Engineering, vol. 22, no. 2, pp. 146–163, 2016.\n\n[17] R. B. Sexton and M. B. Brundage, “Nestor: A tool for natural language annotation of short texts,” Journal of Research of the National Institute of Standards and Technology, vol. 124, Nov. 2019, doi: 10.6028/jres.124.029.\n\n[10] R. Sexton, M. Hodkiewicz, M. P. Brundage, and T. Smoker, “Benchmarking for keyword extraction methodologies in maintenance work orders,” Annual Conference of the PHM Society, vol. 10, no. 1, Sep. 2018, doi: 10.36001/phmconf.2018.v10i1.541.\nThe tags were created by a subject-matter expert spending 1 hour of time in the annotation assistance tool nestor [17], using a methodology outlined in a previous benchmarking study for that annotation method [10].\nThat work compared the ability of tags to estimate survival curves and mean time-to-failure, when compared with a custom-designed keyword extraction tool based on classifying the maintenance issues by subsystem. While certain sets of tags were able to predict time-to-failure with high accuracy for certain subsystems, a key problem identified in that work is knowing a priori which tags best indicate when a subsystem is failing?\n\nWhich tags best represent a given subsystem?\n\nSome tags are sufficient (albeit unnecessary) conditions to indicate a subsystem. That the “hydraulic” tag indicates a Hydraulic System MWO is obvious, but so might a “valve” tag—“hydraulic” is implied but not present. Consequently, we can treat the problem of assigning tags to represent a subsystem as a semi-supervised multi-class classification problem in the style of [18]. Like in that work, we need to know a selection tag\\(\\rightarrow\\)subsystem assignments, as well as network of weighted tag-tag edges.\n\n[18] K. Avrachenkov, P. Chebotarev, and A. Mishenin, “Semi-supervised learning with regularized laplacian,” Optimization Methods and Software, vol. 32, no. 2, pp. 222–236, Mar. 2017, doi: 10.1080/10556788.2016.1193176.\n\n[19] M. Ho, “A shared reliability database for mobile mining equipment,” PhD thesis, University of Western Australia, 2015.\nThen, if we can compare the semi-supervised tag classifications to a ground-truth classification by a human annotator (which are available for the excavator dataset thanks to [19]), we can assess the ability of each network to capture the human annotator’s internal/cognitive tag relationship structure.\n\n\nWhich network assigns tags to subsystems most like a domain expert?\nTo test the ability of the similarity measures to accomplish this, the top three most common subsystems in the data were used as classes, namely, Hydraulic System, Engine, and Bucket. The tags “hydraulic”, “engine”, and “bucket” were assigned to those subsystems as known labels, respectively. Tags were filtered to only include ones of high-importance and sufficient information: only work orders containing at least 3 unique tags, and only tags that occurred at least 10 unique times within the those work orders, were included for this analysis (\\(C=263\\) MWOs, \\(N=40\\) tags). Then the number of occurrences for every tag can be compared across subsystems, giving each tag a ground-truth multinomial (categorical) probability distribution for occurring within each subsystem. We determine ground-truth classification labels as subsystems that account for \\(\\geq60\\%\\) of each tag’s occurrences. Tags more balanced than that are considered “unknown subsystem”.\nTo perform semi-supervised classification on the recovered relationship graphs, we use a label-spreading algorithm described in [20], which itself was inspired by spreading activation networks in experimental psychology [21], [22]. The result of this algorithm is tags having a score for each class, with the classification being the maximally scored class for that tag. These class assignments can then be compared to the ground-truth labels, which we have done by weighted macro-averaging of the \\(F_1\\)-score (see Figure 9.3 (a)).\n\n[20] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Schölkopf, “Learning with local and global consistency,” in Advances in neural information processing systems, 2004, pp. 321–328.\n\n[21] J. R. Anderson, The architecture of cognition. Psychology Press, 2013.\n\n[22] J. Shrager, T. Hogg, and B. A. Huberman, “Observation of phase transitions in spreading activation networks,” Science, vol. 236, no. 4805, pp. 1092–1094, 1987.\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\nFigure 9.3: Semisupervised MWO Tag Classification with Network Label Propagation\n\n\n\nThe classification of the INVITE-based similarity measure far outperforms the other measures as a preprocessor for label-spreading, when measured by average \\(F_1\\)-score. However, since these “classifications” are actually thresholded multinomial distributions (with some tags regularly occurring across multiple subsystems), how do we know if an underlying structure has actually been recovered, rather than simply a black-box classifier that happens to perform well at this setting?\nTo begin answering this question, we might ask whether the relative scores returned by label-spreading are similar to the original multinomial distributions themselves, rather than the overall classification. To find out, we use softmax normalization5 to transform each tag’s scores into a “predicted multinomial”, before finally calculating the Kullback-Leibler divergence (KLD) between the true and predicted multinomials for every tag. The total KLD, summed over all tags, is also shown in Figure 9.3 (b), along with positions of each tag’s multinomial as projected onto the 2-simplex for the true and \\(F_1\\)-optimal predicted distributions. Once again, the INVITE performs much better at this task, over a wide range of \\(\\sigma\\) (lower is better).\n5 For visualization, a temperature parameter was added to softmax, and this was optimized for minimum KLD via Brent’s method [23] for each similarity measure independently to provide an equal footing for comparison.\n[23] R. P. Brent, “An algorithm with guaranteed convergence for finding a zero of a function,” The Computer Journal, vol. 14, no. 4, pp. 422–425, 1971.\nA reason for the performance disparity can be seen in the simplex projections: recovered topology via INVITE-similarity does a much better job of separating the three classes, while not letting any single tag overcompensate by dominating a subsystem’s area. Even the “unknown” tags are correctly placed roughly between Bucket and Hydraulic System regions, reflecting the true topology of the system.",
    "crumbs": [
      "Applications & Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Working Memory & Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/part3/3-08-ordered.html#sec-animal-fluency",
    "href": "content/part3/3-08-ordered.html#sec-animal-fluency",
    "title": "Recovery from Working Memory & Partial Orders",
    "section": "Forest Pursuit Animal Network",
    "text": "Forest Pursuit Animal Network\nFor our last case study, we return to Forest Pursuit as a useful tool for analysis even when that might mean ignoring partial ordering information. Note that Equation 9.2 is effectively the same as a (\\(\\beta=1\\)) forest matrix if the transition (normalized adjacency) matrix was replaced by a (sub-)graph laplacian. Intuitively, the INVITE loss function is summing up over absorption (log-)probabilities at each new node activation: i.e. each step of a tree’s creation. Because the probability of a sampled tree is precisely proportional to the product of its edge weights, then weighing a tree by its absorption probabilities and running INVITE should have a mathematically similar effect as the FP estimate. The similarity would be exact whenever the tree that set of nodes traveled along was the mode of the tree distribution: the MST.\nIn other words, whenever the random walks did use the minimum distance to reach each node, the two methods should be equivalent. While this isn’t happening much individually, the effect of many random walks will average out to this MST, precisely because it is the tree distribution mode from which we assume node activations sample under marked Random Spanning Forests.\nTo illustrate FP’s efficacy in network estimation despite ignoring partial order information, we turn to a classic network recovery problem in this space: semantic networks from fluency data[9], [24], [25].\n\n[24] T. J. Prescott, L. D. Newton, N. U. Mir, P. W. R. Woodruff, and R. W. Parks, “A new dissimilarity measure for finding semantic structure in category fluency data with implications for understanding memory organization in schizophrenia.” Neuropsychology, vol. 20, no. 6, pp. 685–699, Nov. 2006, doi: 10.1037/0894-4105.20.6.685.\n\nDomain-aware preprocessing\nVerbal (semantic) fluency tests involve asking participants to list as many items belonging to a prompted category in their available time. For instance, an “animals” prompt could lead to an answer like “dog, cat, lion, tiger, bear, wolf…”, etc. Like before, the general idea is that recall of each item derives from a random walk in a cognitive “memory space”, with emissions (usually) only when a new animal is encountered. A participant might have backtracked internally to dog from bear and jumped to wolf, for example.\nUsing a high-dimensional multilabel embedding space is possible, with one-column-per-animal, but the lists tend to be quite long and give networks with “hairball” tendencies. However, if our intention is to find dependencies between animals for a large number of participants, we might reasonably limit the co-occurrence to only a nearby subset of each list. Effectively, we can limit the set of possible co-occurring animals based on our domain knowledge, namely, the limits of working memory. Conservatively, with a well-cited work claiming 7 \\(\\pm\\) 2 semantic units at a time in working memory[3], we can limit the set of possible dependencies for a given item to the 5 items on either side. The 5 previous might have originated a jump to the current term, and the 5 after might be subsequent targets, for a rolling window of 10 terms at-a-time.6\n\n[3] G. A. Miller, “The magical number seven, plus or minus two: Some limits on our capacity for processing information.” Psychological Review, vol. 63, no. 2, pp. 81–97, Mar. 1956, doi: 10.1037/h0043158.\n6  This is precisely the logic that leads to the use of Skip-Gram and Continuous-Bag-of-Words transformations of text to weakly supervise word2vec and GloVe models[26], [27]. \n[26] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for word representation,” in Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014, pp. 1532–1543.\n\n[27] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013.\n\n[9] J. C. Zemla and J. L. Austerweil, “Estimating semantic networks of groups and individuals from fluency data,” Computational brain & behavior, vol. 1, pp. 36–58, 2018, doi: 10.31234/osf.io/kg45r.\nFor our case study, we use lists of animals submitted by participants as described in [9], [25]. We limit the animals under consideration to those that occurred in more than 30 lists for a good sample size, as well as lists with at least two animals. This resulted in 100 animals over 293 fluency lists. However, we ignore this filtering when creating the 10-animal rolling windows, to avoid inclusion of unrelated animals into prematurely filtered windows. After re-applying the filter, 100 animals appear across 8020 working-memory windows. Figure 9.4 shows the effects of this preprocessing on marginal distributions.\n\n\n\n\n\n\n\n\n\nFigure 9.4: Effects of rolling-window activations on observation data\n\n\n\n\n\n\nDoing this preprocessing (for rolling-windows of 10) shifts relative animal frequencies downward (since there are many more “observations” from the rolling window), while also shifting the number of animals-per-observation to be strictly less than 10. As desired, the pairwise cosine similarity of all vectors \\(\\mathbf{x}'_{j'},\n\\mathbf{x}'_{j}\\) is significantly reduced. While many participants might cover similar animals overall, we want to investigate animal dependencies locally, and we don’t expect individuals to always recall animals in the same memory “area” the whole time.\n\n\nEdge Connective Efficiency and Diversity\nTo compare the results of different backboning techniques, we introduce a new simple measure to quantify a network’s sparsity, in terms of how many edges more than \\(n-1\\) (the minimum number needed to connect all \\(n\\) nodes) are being used. \\[\n\\begin{gathered}\n\\psi_n(e) = \\frac{e_{\\text{min}}}{e}\\frac{e_{\\text{max}}-e}{e_{\\text{max}}-e_{\\text{min}}}\\\\\ne_{\\text{max}} = \\frac{n(n-1)}{2} \\quad e_{\\text{min}} = n-1\n\\end{gathered}\n\\tag{9.3}\\]\nWe call \\(\\psi\\) the graph’s “connective efficiency”, and it will range from 0 when the graph is fully connected, to 1 when it is a tree, to &gt;1 when it has insufficient edges to be connected. This measure is intended to compare graphs that have had edges removed until it is about to be disconnected, such as with the Doubly Stochastic Filter (DS) [28]. However, values greater than 1 also give insight to how much sparser than a tree some graph is.\n\n[28] P. B. Slater, “A two-stage algorithm for extracting the multiscale backbone of complex weighted networks,” Proceedings of the National Academy of Sciences, vol. 106, no. 26, pp. E66–E66, Jun. 2009, doi: 10.1073/pnas.0904725106.\n\n[25] J. Goñi et al., “The semantic organization of the animal category: Evidence from semantic verbal fluency and network theory,” Cognitive processing, vol. 12, no. 2, pp. 183–196, 2011.\nThe DS animal network is shown in Figure 9.5. With large, deeply connected clusters centered around contexts animals are found in Figure 9.5 looks very similar to the network recovered to make Fig. 4 in [25]. Clusters approximate animal types by their location relative to humans: farm/livestock, ocean/water creatures, “African” and jungle animals, small indoor mammals, small outdoor mammals, etc.\n\n\n\n\n\n\n\n\n\nFigure 9.5: Verbal Fluency (animals) Network Backbone (Doubly-Stochastic)\n\n\n\n\n\n\nThis is largely how the literature on semantic fluency leaves their network recovery solution, with clusters based on human proximity or physical location. However, there are other ways people might relate animals than simply by location. Additionally, clique bias is quite strong in this network: why must every farm animal be mutually connected if its possible to recall any of them through one or two “hub” animals? This is related to the incredible inefficiency of this backbone, with a \\(\\psi=0.35\\) being rather closer to fully connected than sparse. Additionally, the two animals that seem to appear regardless of others are cat and dog, which ironically makes DS penalize their proximity to any of the clusters. Both are ironically clustered with ocean animals due to their tendency to be listed near fish.\nThe Chow-Liu tree network is shown in Figure 9.6, and goes some way to alleviating these issues. Clusters are largely intact, instead represented by large branches/subtrees off the main group. However, some community relationships have been sacrificed to maintain strong individual edges, such as monkey+giraffe for location similarity at the expense of separating two halves of the pink cluster across a wide distance. More alternate paths between creatures (i.e. loops) are needed to better represent our perception of animal relationships.\n\n\n\n\n\n\n\n\n\nFigure 9.6: Verbal Fluency (animals) Dependency Network (Chow-Liu Tree)\n\n\n\n\n\n\nThe other dependency network recovery method is GLASSO, which we have similarly thresholded at the minimum-connected point. It only slightly improves on connective efficiency (\\(\\psi=0.44\\)), though the cliques are replaced with much more dispersed connections throughout the graph. We also see that reasonable inter-group connections are better represented, such as rabbit+squirrel, though cat and dog are still isolated due to overrepresentation throughout the dataset.\n\n\n\n\n\n\n\n\n\nFigure 9.7: Verbal Fluency (animals) Dependency Network (GLASSO)\n\n\n\n\n\n\nSubjectively, the GLASSO network is still difficult for an analyst to synthesize into useful knowledge, with so many edges, while the DS network only really managed to communicate one “kind” of knowledge (the context clusters). We would ideally prefer a backbone that provides a wider diversity of important edge “types”, for an analyst to better understand the kinds of animal relationships humans perceive.\nTo illustrate this, we show the Forest Pursuit(FP) network in Figure 9.8. It has also been filtered to minimum-connected, like DS and GLASSO, though in this case the connective efficiency to reach that threshold is a staggering \\(\\psi=0.88\\).\n\n\n\n\n\n\n\n\n\nFigure 9.8: Verbal Fluency (animals) Dependency Network (Forest Pursuit)\n\n\n\n\n\n\nUnlike the other networks that push “generalist” nodes like cat and dog onto long, distant chains, those chains are used in the FP network to hold rare subgroups of clusters, treating them as “gated” by the prominent “hubs” of those groups. For example, cat is correctly linked to mouse and lion (in addition to bird), while lemur is pushed down a longer chain of primates, “gated” by gorilla. Similarly with eel through lobster and crab.\nA much broader edge-type diversity is also made apprarent with many non-context-based relationships made obvious with the improved sparsity. An analyst has an easier job of creating “edge-type inventories”, making the FP backbone an excellent exploratory assistant: animals can be related because they are:\n\nCo-located\nTaxonomically similar (cheetah+leopard)\nFamous predator/prey pairs (cat+mouse)\nPop-culture references (lion\\(\\rightarrow\\)tiger\\(\\rightarrow\\)bear)7\nSimilar in ecological niche/role (koala+sloth)\nRelated through conservation or public awareness (panda+gorilla)\netc.\n\n7 Note that the dependency-based methods correctly interpred these three as not being mutually connected in a triad, but specifically with this ordering (tiger in the middle).This is further reflected in how FP alters the way centrality measures behave. Replicating Figure 8.7 for these graphs, Figure 9.9 shows the change in rank across the top 15 animals for the DS, GLASSO, and FP networks.\n\n\n\n\n\n\n\n\n\nFigure 9.9: Changes in animal centrality ranking for FP vs co-occurrence,GLASSO\n\n\n\n\n\n\nThe DS centrality finds the most densely connected clique and gives all of its members incredibly high values. Meanwhile, the none of the top 5 most common animals (dog,cat,lion,tiger,bear)8 have high centrality at all. Both GLASSO and DS have farm animals (chicken,goat,cow) as the most important, despite the idea that goat likely could be reached from e.g. cow quite often. FP adds more variety, giving hub-animals from different communities high centrality scores, each of which could lead to a variety of different paths. While subjective, the ranks from FP appear to be a more holistic inventory of “lynchpin” animals, which provide nearby coverage for a large amount of others.\n8 Interestingly, dog never appears in centrality measures, and none of the networks connect dog to any other animal than cat. Meanwhile, wolf is associated more with fox, coyote, dingo, etc., which are notably all predators of farm animals.\nWith the primary community structures being what they are (context/location-based), it seems that humans tend to put dogs in a category all their own.\n\nThresholded Structure Preservation\nAnother beneficial feature of Forest Pursuit is how it fares under increased thresholding. Because co-occurrence methods prioritize cliques, those cliques will remain connected as other edges are removed, effectively destroying the global structure of the animal network past a certain threshold. As seen in Figure 9.10, in keeping only the top 2% of edges, they are used to connect separated islands of animal communities.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) co-occurrence retains local communities at the cost of global structure\n\n\n\n\n\n\n\n\n\n\n\n(b) clique-bias correction preserves central structure by disconnecting rare nodes\n\n\n\n\n\n\nFigure 9.10: Differences in structural preservation with over-thresholding.\n\n\n\n\n\nMeanwhile, the FP network is quite robust to this excessive thresholding, with the global structure preserved at 2%. The removed edges have simply detached some of the rarer animals from the network entirely. These isolates not only reflect a more metrologically-sound idea (that rarer nodes would be disconnected at higher certainty thresholds), but are also beneficial to analysts, since manually re-connecting isolates of rare animals is simpler than manually determining a global reconnection strategy for island groups.\n\n\n\nForest Pursuit as Preprocessing\nBecause Forest Pursuit creates a representation of the observed data in edge space, we can use Equation 6.4 in the forward direction, creating a “new”design matrix \\(X\\gets BR\\). As discussed in Problem Specification, Equation 6.4 will create a design matrix of interaction counts for each node (its degree in the steiner tree approximation), rather than a binary “on/off” indicator.\nBy supplying other algorithms with this new estimate for \\(X\\), this makes FP a kind of preprocessing on the data itself. We can do this to bias the other methods toward greater sparsity in the backbone, without explicitly relying on point estimates for each observation (any tree with those node degrees would do the same). As shown in Figure 9.11, GLASSO and DS both increase their connective efficiency under FP preprocessing.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Doubly Stochastic filter with FP (node degree) preprocessing\n\n\n\n\n\n\n\n\n\n\n\n(b) GLASSO Precision estimate with FP (node-degree) preprocessing\n\n\n\n\n\n\nFigure 9.11: Forest Pursuit preprocessing for Doubly-Stochastic and GLASSO recovered networks\n\n\n\n\n\nThis is likely due to the increase in “signal-to-noise” ratio for each datapoint, since observations are only similar when they have a node interacting the same “amount”, not merely when similar nodes are activated.\nBecause the entries are now integer counts, FP preprocessing might also give a better path to using distribution-based embedding and clustering techniques, such as Hellinger distances between multinomial sample counts. This goes for many other techniques from text processing that rely on multinomial assumptions (i.e. techniques otherwise inapplicable to binary data).\nFP Preprocessing, with further empirical and theoretical validation, might prove to be a powerful tool for practitioners to flexibly backbone and analyse their networks with a variety of new techniques.",
    "crumbs": [
      "Applications & Case Studies",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Recovery from Working Memory & Partial Orders</span>"
    ]
  },
  {
    "objectID": "content/09-conclusion.html",
    "href": "content/09-conclusion.html",
    "title": "Conclusion & Future Work",
    "section": "",
    "text": "Summary of contributions\nPractitioners have long struggled with a lack of techniques for metrological quantification and measurement error handling in network science. There is an ongoing need to specify valid network recovery models—ones that are not only assessed for precision, but designed for trueness. [1] provides a “reclassification” of measurement error in networks, focusing on the true/false positive/negative dichotomies for both edge and node reporting. This implicitly assumes that, like the karate-club graph [2], our observations are of network components (edges/nodes). When the object to be measured is not directly observable (as is the case for recovery from node activations), measurement error can also arise both from model noise sensitivity (lack of precision) and misspecification (lack of trueness). Because of this:\nTo develop the practice of taking measurement error into account, we have proposed a combination of problem framing, measurement aggregation techniques, and methods for bias correction when recovering network structure from observed random walk visits.\nThis thesis provides:\nWe lay this foundation in the hope of further improving the ability of practitioners to explore the structure of their data in a principled manner.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion & Future Work</span>"
    ]
  },
  {
    "objectID": "content/09-conclusion.html#summary-of-contributions",
    "href": "content/09-conclusion.html#summary-of-contributions",
    "title": "Conclusion & Future Work",
    "section": "",
    "text": "An interpretation of network recovery as an inverse problem comparable to sparse approximation, with concise definitions of data, edge, and node vector spaces from an underlying incidence-structure formalism.\nA taxonomy of structural assumptions used in literature to make network inference tractable:\n\nLocal, global, or resource/information-flow structural constraints;\nInverse-problem status (direct or indirect edge observation); and\nWhether activation observations are pre-aggregated before estimation.\n\nA method, Forest Pursuit, to address the need for a model with:\n\nLocal observation constraints,\nAn inverse-problem assumption for indirect edge observation, and\nDependence on the bipartite nature of node observations.\n\n\nA dataset and benchmarking toolkit (MENDR) to reproducibly compare algorithmic ability to recover network structure from random walk activations, which has been applied to demonstrate the scaling and accuracy of Forest Pursuit over other methods.\nGeneralization of Forest Pursuit by developing a probabilistic model for it as a sparse dictionary learning technique, for which we provide an expectation maximization scheme to estimate.\n\nApplication of Forest Pursuit as case studies in scientific collaboration networks, classic literature analysis, technical language processing, and semantic verbal fluency tests.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion & Future Work</span>"
    ]
  },
  {
    "objectID": "content/09-conclusion.html#sec-future-fp",
    "href": "content/09-conclusion.html#sec-future-fp",
    "title": "Conclusion & Future Work",
    "section": "Modifications and extensions to Forest Pursuit",
    "text": "Modifications and extensions to Forest Pursuit\nDesire Path Densities and Forest Pursuit are designed to be adaptable to several modalities of use1. However, there are key limitations of the model that could be addressed going forward, as well as future research directions inspired by our modeling paradigm.\n1 see for instance Forest Pursuit as Preprocessing\nMultiple sources and hidden nodes\nOne of the key assumptions to make the likelihood of the (marked) Random Spanning Forest tractable is to allow only one “source” node (the random walk starting node), and to sample it from a uniform categorical distribution. However, by explicitly adding a “root” node that is implied by the random forest distribution (see Figure 7.3), we would immediately achieve the possibility for multiple sources. A source would become any node incident to the root in a sampled spanning tree. To prevent every node from being activated, the spanning tree could be “pruned” at some depth away from the root (which is a parameter that we could model with e.g. a Geometric distribution). How many sources get selected in a minimum spanning tree could be controlled through the weights given to edges incident to the root (which, if the reader recalls, is already represented by \\(\\beta\\)). Also possible could be the use of independent Bernoulli activations of nodes as sources (rather than a categorical selector) though heavy regularization would be required to avoid each node always becoming its own source.\nAnother assumption is that there were no hidden nodes. This is different from false negative nodes, in the sense that here we did not know a node existed at all, but it is a source of dependency nonetheless. This might be explained as seeing a need to add a node so that observed distances between nodes are maximally tree-like. [4] uses a greedy algorithm (TreeRep) to learn a tree structure (hyperbolic metric) that has minimum distortion to a given distance metric. If the distortion is too great, they add a Steiner node and continue. Automatically building a tree and simultaneously adding Steiner nodes that reduce distortion would effectively result in recommendations for adding new nodes to the graph. An analyst might be able to review these suggestions, interactively, to find “hidden” nodes.\n\n[4] R. Sonthalia and A. C. Gilbert, “Tree! I am no tree! I am a low dimensional hyperbolic embedding,” arXiv; arXiv, arXiv:2005.03847, Oct. 2020. doi: 10.48550/arXiv.2005.03847.\n\n\nGeneralizing inner products on incidences\nSpeaking of hyperbolic, many authors have recently investigated the usefulness of hyperbolic space for embedding graphs (as vectors) in a way that preserves hierarchies and sparsity naturally [5], [6], [7], [8], [9]. If trees can be embedded losslessly [10] into \\(\\mathcal{H}^2\\)2, then traversing a tree as a random walker could be represented as a trajectory in an \\(\\mathcal{H}^{2+1}\\) de-Sitter space. Various techniques exist for finding embeddings of graphs as “causal-sets” in a Lorentzian spacetime [11], and this approach could be combined with a way to smoothly sample a discretized space with (hyperbolic) Voronoi cells [12], [13]\n\n[5] I. Chami, A. Gu, V. Chatziafratis, and C. Ré, “From trees to continuous embeddings and back: Hyperbolic hierarchical clustering,” in Advances in neural information processing systems, Curran Associates, Inc., 2020, pp. 15065–15076. Accessed: Jul. 19, 2022. [Online]. Available: https://proceedings.neurips.cc/paper/2020/hash/ac10ec1ace51b2d973cd87973a98d3ab-Abstract.html \n\n[6] O. Ganea, G. Becigneul, and T. Hofmann, “Hyperbolic entailment cones for learning hierarchical embeddings,” presented at the International conference on machine learning, PMLR, Jul. 2018, pp. 1646–1655. Accessed: Feb. 01, 2023. [Online]. Available: https://proceedings.mlr.press/v80/ganea18a.html\n\n[7] M. Nickel and D. Kiela, “Learning continuous hierarchies in the lorentz model of hyperbolic geometry,” presented at the International conference on machine learning, PMLR, Jul. 2018, pp. 3779–3788. Accessed: Feb. 01, 2023. [Online]. Available: https://proceedings.mlr.press/v80/nickel18a.html\n\n[8] F. Sala, C. D. Sa, A. Gu, and C. Re, “Representation tradeoffs for hyperbolic embeddings,” presented at the International conference on machine learning, PMLR, Jul. 2018, pp. 4460–4469. Accessed: Feb. 01, 2023. [Online]. Available: https://proceedings.mlr.press/v80/sala18a.html\n\n[9] L. Linzhuo, W. Lingfei, and E. James, “Social centralization and semantic collapse: Hyperbolic embeddings of networks and text,” Poetics, vol. 78, p. 101428, Feb. 2020, doi: 10.1016/j.poetic.2019.101428.\n\n[10] R. Sarkar, “Low distortion delaunay embedding of trees in hyperbolic plane,” in Graph drawing, M. van Kreveld and B. Speckmann, Eds., in Lecture notes in computer science. Berlin, Heidelberg: Springer, 2012, pp. 355–366. doi: 10.1007/978-3-642-25878-7_34.\n2  \\(\\mathcal{H}^2\\) is the 2D hyperbolic manifold, embedded in a 3D Euclidean space.\n[11] J. R. Clough and T. S. Evans, “Embedding graphs in lorentzian spacetime,” PLOS ONE, vol. 12, no. 11, p. e0187301, Nov. 2017, doi: 10.1371/journal.pone.0187301.\n\n[12] F. Nielsen and R. Nock, “Hyperbolic voronoi diagrams made easy,” in 2010 international conference on computational science and its applications, Mar. 2010, pp. 74–80. doi: 10.1109/ICCSA.2010.37.\n\n[13] R. T. Q. Chen, B. Amos, and M. Nickel, “Semi-discrete normalizing flows through differentiable tessellation,” in Advances in neural information processing systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., Curran Associates, Inc., 2022, pp. 14878–14889. Available: https://proceedings.neurips.cc/paper_files/paper/2022/file/5f61939af1699c82dab00ed36c887968-Paper-Conference.pdf \n\n\nApplication areas and case studies\nLastly, a critical test of Forest Pursuit will be its application to a wider variety of domains under the scrutiny of each domain’s experts. From the network community itself, for instance, recent interest has been shown in assessing the methodological reasons for observed assortativity [14]. The conditions under which controlling for clique-bias also reduces assortativity would be a useful tool when deciding to use different network cleaning techniques.\n\n[14] D. N. Fisher, M. J. Silk, and D. W. Franks, “The perceived assortativity of social networks: Methodological problems and solutions,” in Trends in social network analysis, Springer International Publishing, 2017, pp. 1–19. doi: 10.1007/978-3-319-53420-6_1.\n\n[15] T. J. Prescott, L. D. Newton, N. U. Mir, P. W. R. Woodruff, and R. W. Parks, “A new dissimilarity measure for finding semantic structure in category fluency data with implications for understanding memory organization in schizophrenia.” Neuropsychology, vol. 20, no. 6, pp. 685–699, Nov. 2006, doi: 10.1037/0894-4105.20.6.685.\n\n[16] N. Arias-Trejo et al., “Semantic verbal fluency: Network analysis in alzheimer’s and parkinson’s disease,” Journal of Cognitive Psychology, vol. 33, no. 5, pp. 557–567, Jun. 2021, doi: 10.1080/20445911.2021.1943414.\nFurther afield, semantic Verbal Fluency tests discussed in Forest Pursuit Animal Network are often administered for the purposes of assisting in diagnosis of Alzheimer’s and Schizophrenia patients. While experiments using inferred network structures [15], [16] have been used to detecting early-onset neurological disease from topological differences, it could be useful to re-assess these outcomes when clique-bias has been better accounted for.\nAll of these applications would be further assisted by Forest Pursuit’s ability to\n\nInfer network estimates quickly, on streaming data, and\nIncorporate prior (and incoming) knowledge from domain experts on edges.\n\nTogether, these properties should make for an ideal human-in-the-loop analysis tool. Indeed, for any qualitative study on nodes and discovering relationships between them, decreasing the annotation load (number of edges to assess)—while increasing edge diversity—will be critical to correctly inventory the important categories of relationships or dependencies3. Building tools that enable practitioners and researchers to undertake complex grounded coding [17] tasks like this is a rich area for possible Human-Systems Integration efforts going forward [18], [19].\n\n\n3 see Edge Connective Efficiency and Diversity\n[17] J. Saldaña, “The coding manual for qualitative researchers,” 2021.\n\n[18] J. F. Fung et al., Human-in-the-loop technical document annotation :: Developing and validating a system to provide machine-assistance for domain-specific text analysis. National Institute of Standards; Technology (U.S.), 2024. doi: 10.6028/nist.tn.2287.\n\n[19] C. Harper, A. Kumer, S. Stuart, and E. Meszaros, “AI-Informed Approaches to Metadata Tagging for Improved Resource Discovery,” in The Rise of AI: Implications and Applications of Artificial Intelligence in Academic Libraries, in PIL, no. 78., ACRL, 2022. Accessed: Mar. 08, 2024. [Online]. Available: https://www.choice360.org/libtech-insight/using-ai-for-metadata-tagging-to-improve-resource-discovery/",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusion & Future Work</span>"
    ]
  }
]