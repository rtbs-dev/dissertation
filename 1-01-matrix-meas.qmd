# Metrology as matrices



Where metrology is concerned, the actual unit of observation and how it is encoded for us is critical to how analysts may proceed with quantifying, modeling, and measuring uncertainty around observed phenomena. 
Experiment and observation tends to be organized as inputs and outputs, or, independent variables and dependent variables, specifically.
Independent variables are observed, multiple times ("observations"), and changes in outcome for each can be compared to the varying values associated with the independent variable input ("features").
For generality, say a practitioner records their measurements as scalar values, i.e. $x\in\mathbb{S}\in\{\mathbb{R,Z,N},\cdots\}$.
The structure most often used to record scalar values of  $n$ independent/input variable features over the course of $m$ observations is called a design matrix $X:\mathbb{S}^{m\times n}$.^[
  Not all observations are scalar, but they can become so.
  If individual measurments are higher-dimensional (e.g. images are 2D), X is a tensor, which can be transformed through unrolling or embedding into a lower dimensional representation before proceeding. 
  There are other techniques for dealing with e.g. categorical data, such as one-hot encoding (where the features are binary for each possible category, with boolean entries for each observation).
]


## Observation and feature "spaces"

If we index a set of observations and features, respectively, as
$$ i\in I=\{1,\cdots,m\}, \quad j\in J=\{1,\cdots,n\},\qquad I,J:\mathbb{N}$$
then the design matrix can map the index of an observation and a feature to the corresponding measurement.
$$x=X(i,j)\qquad X : I\times J \rightarrow \mathbb{S}$$
i.e.  the measured value of the $j$th independent variable from the $i$th observation.
In this scheme, an "observation" is a single row vector of features in $\mathbb{S}^{n\times 1}$ (or simply $\mathbb{S}^{n}$), such that each observation encodes a position in the space defined by the features, i.e. the _feature space_, and extracting a specific observation vector $i$ from the entire matrix can be denoted as
$$\mathbf{x}_i=X(i,\cdot),\quad \mathbf{x}:J\rightarrow\mathbb{S}$$
Similarly, every "feature" is associated with a single column vector in $\mathbb{S}^{1\times m}$, which can likewise be interpreted as a position in the space of observations (the _data space_):
$$\mathbf{x}_j^*=X(\cdot,j),\quad \mathbf{x}^*:I\rightarrow\mathbb{S}$$
Note that this definition could be swapped without loss of generality.
In other words, $\mathbf{x}$ and $\mathbf{x}^*$ being in row and column spaces is somewhat arbitrary, having more to do with the logistics of experiment design and data collection.
We could have measured our feature vectors one-at-a-time, measuring their values over an entire "population", in effect treating that as the independent variable set.^[
  In fact, vectors are often said to be in the column-space of a matrix, especially when using them as transformations in physics or deep learning layers.
  We generally follow a one-observation-per-row rule, unless otherwise stated.
]

To illustrate this formalism in a relevant domain, let's take another classic network example from academia: co-citation networks.
[DRAW BIPARTITE]
Lists of co-authors on publications is often reported as "network" data, and subjected to network analysis techniques.
For $m$ papers we might be aware of a total of $n$ authors. 
For a given paper, we are able to see which authors are involved, and we say those authors "activated" for that paper.
It makes sense that our observations are individual papers, while the features might be the set of possible authors. 
However, we are not given information about which author was invited by which other one, or when each author signed on.
In other words, the measured values are strictly boolean, and we can structure our dataset as a design matrix $X:\mathbb{B}^{m\times n}$.
We can then think of the $i^{\mathrm{th}}$ paper as being represented by a vector $\mathbf{x}_i:\mathbb{B}^n$, and proceed using it in our various statistical models.
If we desired to analyze the set of authors, say, in order to determine their relative neighborhoods or latent author communities, we could equally use the feature vectors for each paper, this time represented in a vector $\mathbf{x}^*_j:\mathbb{B}^{1\times m}$.


## Models & linear operators

Another powerful tool an analyst has is _modeling_ the observation process.
This is relevant when the observed data is hypothesized to be generated by a process we can represent mathematically, but we do not know the parameter values to best represent the observations (or the observations are "noisy" and we want to find a "best" parameters that account for this noise).
This is applicable to much of scientific inquiry, though one common use-case is the de-blurring of observed images (or de-noising of signals), since we might have a model for how blurring "operated" on the original image to give us the blurred one.
We call this "blurring" a _linear operator_ if it can be represented as a matrix^[in the finite-dimensional case], and applying it to a model with $l$ parameters is called the _forward map_:
$$\mathbf{x} = F\mathbf{p}\qquad F:\mathbb{R}^{l}\rightarrow \mathbb{R}^n$$
where $P$ is the space of possible parameter vectors, i.e. the _model space_.
The forward map takes a modeled vector and predicts a location in data space.

Of critical importance, then, is our ability to recover some model parameters from our observed data, e.g. if our images were blurred through convolution with a blurring kernel, then we are interested in _deconvolution_. 
If $F$ is invertible, the most direct solution might be to apply the operator to the data, as the _adjoint map_: 
$$ \mathbf{p} = F^H\mathbf{x}\qquad F^H:\mathbb{R}^{n}\rightarrow \mathbb{R}^l$$
which removes the effect of $F$ from the data $\mathbf{x}$ to recover the desired model $\mathbf{p}$.

Trivially we might have an orthogonal matrix $F$, so $F^H=F^{-1}$ is available directly.
In practice, other approaches are used to minimize the _residual_: $\hat{\mathbf{p}}^=\min_{\mathbf{p}} F\mathbf{p}-\mathbf{x}$.
Setting the gradient to 0  yeilds the normal equation, such that
$$ \hat{\mathbf{p}}=(F^TF)^{-1}F^T\mathbf{x}$$
This should be familiar to readers as equivalent to solving ordinary least-squares (OLS).
However, in that case it is more often shown as having the _design matrix_ $X$ in place of the operator $F$. 

_This is a critical distinction to make:_ OLS as a "supervised" learning method treats some of the observed data we represented as a design matrix previously as a target to be modeled, $y=X(\cdot,j)$, and the rest maps parameters into data space, $F=X(\cdot,J/j)$. 
With this paradigm, only the target is being "modeled" and the rest of the data is used to create the operator.
In the citation network example, it would be equivalent to trying to predict one author's participation in every paper, _given_ every other author's participation in them.

For simplicity, most work in the supervised setting treats the reduced data matrix as X, opting to treat $y$ as a separate _dependent variable_.
However, our setting will remain _unsupervised_, since no single target variable is of specific interest---all observations are "data".
In this, we more closely align with the deconvolution literature, such that we are seeking a model and an operation on it that will produce the observed behavior in an "optimal" way. 




## Measurement quantification & error 

- marginal sums
- rule of succession
- type I and II Error? 
- Epistemic and Alleatoric Uncertainty? 

Ultimately we are not great at specifying what "being related" actually means...

## Incidence vs. Proximity


### Incidence structures & dependency

foundational model of graph theory and incidence structures more broadly.
More to come, but get the terminology down.

Spring example, road example, etc.
Discuss Complex Systems and their representation. 


### Kernels & distances 

Importantly for the use of linear algebra, these values assigned for each feature are assumed to exist in a field (or, more generally, a semiring) $R$, equipped with operators analogous to addition ($\oplus$) and multiplication ($\otimes$) that allow for values to be aggregated through an inner product.
The matrix of all pairs of inner-products found by matrix multiplication (contracting over the feature space) is given by:

$$G(\mathbf{x}_i, \mathbf{x}_j,R) = G_{ij} = \bigoplus_{k=1}^{n} x_{ik} \otimes x_{kj} $$

such that real-valued entries and a traditional "plus-times" inner product recovers the Gram matrix $G_{ij}=\sum_{k=1}^{n} x_{ik}x_{kj}$, or simply $G=X^TX$.  

How "close" or "far away" things are.... Avrachenkov et al. 


Important: these measurements often assume distance is defined in terms of the measurements/objects/data, but for _inverse problems_, structure learning, etc., they are more often applied in terms of the features/operators.

Example with doc-term matrices 


The inner product between two papers will yield a "true" only if two papers share at least one author in common. 
This is called a _bipartite projection_[CITE], specifically the "papers" projection. 

Similarly, if our goal is to determine a network of "whether two authors ever coauthored", we could perform a bipartite projection using the boolean inner product in the observation space i.e. the "authors" projection.
It is this second projection, for determining a structure between features embedded into the  "observation" space, that we are primarily concerned with in this work, since it is the view that most closely resembles the concept of covariance or correlation between independent variables (features) in statistics more generally. 


### Implications for networks
Usually dependencies are taken as causing or enabling proximity.
E.g. shortest paths, vs. edges.


The approach taken by researchers/investigators...do they assume a level of interchangeability between the two kinds of "relation"?
Do they define
Or do they 
