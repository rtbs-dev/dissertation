---
subtitle: |
  Graduate Thesis Defense \
  [`dissertation.rtbs.dev`](https://dissertation.rtbs.dev)
format:
  metropolis-beamer-revealjs:
    slide-number: true
    history: false
    transition: slide
    margin: 0.1
    navigation-mode: vertical
    autostretch: false
    #chalkboard: true
    #multiplex: true
    #theme: resource/slides.scss
    include-in-header:
      text: |
        <script>
        MathJax = {
          loader: {
            load: ['[tex]/boldsymbol']
          },
          tex: {
            tags: "all",
            inlineMath: [['$','$'], ['\\(','\\)']],
            displayMath: [['$$','$$'], ['\\[','\\]']],
            processEscapes: true,
            processEnvironments: true,
            packages: {
              '[+]': ['boldsymbol']
            }
          }
        };
        </script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

embed-resources: false
revealjs-plugins:
  - attribution
auto-agenda:
  bullets: numbered
  heading: Agenda
filters:
  - pseudocode
  - reveal-auto-agenda
---





# Background

---

1. **Background** 
2. Organizing Current Approaches
3. Desire Path Density Networks
4. Forest Pursuit
5. Experimental Comparison
6. Case Studies
7. Conclusion

## Imagine...



::::{.columns align="center"}
:::{.column}
> You work at an institution where lots of people author publications...
> 
> _...sometimes together..._

Co-authorship $\rightarrow$ relationships of colleagues? 


> Build a network!

How good of a job did you do?

- There's no "ground truth" network...
- What does "related" mean? 
- How will you trust your answer?
:::
:::{.column}
![](content/images/papers-colleagues.svg)
:::
::::

## Problem & Scope

Assume
: We want _networks of dependence_ (influence, causation, etc.)

. . .

Research Question
: Can we _recover_ dependency relationships _from co-occurrences_?\

. . .

\
\

:::{.callout-warning title="Conditions"}

- Without forcing the (social, etc.) network to have a certain shape? \
- While retaining easily-interpretable metrological properties? \
- Fast and flexible enough for practitioners: rapid iteration and exploratory analysis? 
:::

. . .

:::{.callout-important title="This work:"}

1. _Organizes_ current approaches to identify **metrological** and **useability** gaps. 
2. Presents **Forest Pursuit**: method to recover dependencies that is **scalable** and **accurate** .
3. Provides **MENDR**, a community **testbench** and **dataset** for comparing network dependency reconstruction.
4. Application to common **case studies**.

:::

 
## A Need for Network Metrology



> [...] the practice of ignoring measurement error is still mainstream, and robust methods
to take it into account are underdeveloped.
>
> -- [Tiago Peixoto (2018) [@ReconstructingNetworksUnknown_Peixoto2018]]{.cite}

. . .


> Surprisingly, the development of theory and domain-specific applications often occur in isolation, risking an effective disconnect between theoretical and methodological advances and the way network science is employed in practice.
> 
> -- [Peel et al. (2022) [@Statisticalinferencelinks_Peel2022]]{.cite}

---


## Network "Metrology" -- What is it?

Metrology is more than just "units". In our context, we want to: 

- Quantify a network
- Consider the trueness of that quantification
- Consider the precision of that quantification.

![ISO 5725-1 Accuracy (trueness & precision) [@Accuracytruenessprecision_ISO1994]](https://upload.wikimedia.org/wikipedia/commons/9/92/Accuracy_%28trueness_and_precision%29.svg){width=100px}


::: {.attribution}
SV1XV, CC BY-SA 3.0 <https://creativecommons.org/licenses/by-sa/3.0>, via Wikimedia Commons
:::


---

Example: measurement Error for _Zachary's Karate Club_

:::: {.columns align=center .onlytextwidth}

:::{.column}

Trueness
: 
  - Actual "edges" are provided as a list [@InformationFlowModel_Zachary1977]
  - What they _mean_ (quantitatively) is ambiguous, so it's _underspecified_.  

Precision
: 
  - edge 78 simultaneously _exists_ and _doesn't_, depending on which section you believe
  - Any false negatives? No way to know...



:::


:::{.column }

{{< embed content/codefigs/graphs.qmd#fig-karate-club >}}

:::
::::

. . .

_It's effectively a "calibration artefact"_ $\rightarrow$ defined as true, with some precision uncertainty.

---


## Indirect Network Measurement

Edges are often measured _indirectly_ --- _i.e._ lists of co-occurrences (bipartite papers+authors) 

{{< embed content/codefigs/graphs.qmd#fig-obs-set >}}

. . .

::::{.columns}
:::{.column }
What if...

- co-occurrence $\rightarrow$ "acquaintance"?
- more papers $\rightarrow$ more acquainted?

So let's do a bipartite projection...

...done?

:::
:::{.column}

{{< embed content/codefigs/graphs.qmd#fig-collab >}}

:::
::::

. . .


:::{.notes}
notorious for leading to hairballs... already starting to look pretty dense at 4 papers... 
:::

## Dependencies Wanted

Have you ever had someone added to a paper by _someone else_? 

> Almost all paper authors were "asked" by one current author (not cornered by a mob).

. . .

::::{.columns}
:::{.column }

A social scientist "on the ground" would likely measure things differently.

> - Author **(g)** asked **(e, h, and c)** to co-author a paper, _all of whom agreed_
> - Author **(i)** asked **(f and j)**, but **(j)** wanted to _add_ **(b)**'s expertise before writing one of the sections. 
> - etc. 


:::
:::{.column}

{{< embed content/codefigs/graphs.qmd#fig-colleague >}}
:::
::::

This is a network of _who depends on who_!

:::{.notes}
Kinda like sardine tag, each new person is confronted by a mob? Not...usually.
:::

## The Goal

{{< embed content/codefigs/graphs.qmd#fig-recover >}}


# Roads to Network Recovery
_Organizing current approaches_

---

1. Background 
2. **Organizing Current Approaches**
3. Desire Path Density Networks
4. Forest Pursuit
5. Experimental Comparison
6. Case Studies
7. Conclusion



## Our Data, the Design Matrix

- Encode "papers" (observations) as rows, "authors" (features) as columns 

::::{#fig-incidence-struct layout="[[1,1]]" layout-valign="bottom"}

::: {#fig-biadj-mat}
$$
X^{m\times n}=
%X(\{1,2,3,4,\cdots\})=\\
\begin{array}{c c}
& \begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\ \end{array} \\
\begin{array}{c c } x_1\\x_2\\x_3\\x_4\\ \vdots \\ x_m\end{array} &
\left[
\begin{array}{c c c c c c c c c c}
  .  & .   &  1  & .   &  1  & .   &  1  &  1  & .   & .  \\
  1  & .   & .   & .   &  1  &  1  & .   & .   & .   & .  \\
  .  &  1  & .   & .   & .   &  1  & .   & .   &  1  &  1 \\
  .  & .   & .   &  1  &  1  & .   & .   &  1  & .   & .  \\
  &&&& \vdots &&&&&
  \\
  &&&& \vdots &&&&&
\end{array}
\right]
\end{array}
$$

: $X$ as a (bi-adjacency) matrix. 

:::

{{< embed content/codefigs/graphs.qmd#fig-bipartite >}}

::::

## Graphs as Incidence Matrices

::::{#fig-incidence-graph layout="[[1,1]]" layout-valign="center"}

::: {#fig-incidence-mat}

$$
B^{\omega \times n} =
\begin{array}{c c}
& \begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\ \end{array} \\
\begin{array}{c c } e_1\\e_2\\e_3\\e_4\\e_5\\e_6\\e_7\\e_8\\e_9\\e_{10}\\e_{11}\\ \vdots \\e_\omega\\\end{array} &
\left[
\begin{array}{}
  1  &  .  &  .  &  .  &  .  &  1  &  .  &  .  &  .  &  . \\
  1  &  .  &  .  &  .  &  1  &  .  &  .  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  1  &  1  &  .  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  .  &  1  &  .  &  .  &  1  &  . \\
  .  &  .  &  .  &  .  &  1  &  .  &  1  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  1  &  .  &  .  &  1  &  .  &  . \\
  .  &  1  &  .  &  .  &  .  &  .  &  .  &  .  &  .  &  1 \\
  .  &  .  &  .  &  .  &  .  &  .  &  .  &  .  &  1  &  1 \\
  .  &  .  &  1  &  .  &  .  &  .  &  1  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  .  &  .  &  1  &  1  &  .  &  . \\
  .  &  .  &  .  &  1  &  .  &  .  &  .  &  1  &  .  &  . \\
  &&&& \vdots &&&&&
  \\
  &&&& \vdots &&&&&
   \end{array}
\right]
\end{array}
$$

: Network $G$ as an _incidence matrix_ $B$. 

:::
{{< embed content/codefigs/graphs.qmd#fig-colleague >}}

::::


## The Goal
$$ \hat{B} \leftarrow f(X)$$ 

{{< embed content/codefigs/graphs.qmd#fig-recover-bipartite >}}

## Basics -- incidence as (conditional) dependence

A _start_: what does "incident" mean, statistically? $\rightarrow$ Markov Random Fields

- random variables $A,B\in J$
- remaining columns $C=X(\cdot,J\setminus \{A,B\})$

$$
P(A\cap B |C ) = P(A|C)P(B|C) \implies (A\perp B|C) \implies (A,B)\notin G
$$

{{< embed content/codefigs/graphs.qmd#fig-springs >}}

## Taxonomy of Modeling Assumptions -- I

_Structural Constraint Assumptions_

:::{.fragment}
### Local Structure/additivity 
- (Linear) combinations of observations
- Generally called _association measures_ (Cosine Sim./Hyperbolic Projection/Mutual Info.)\
- Built on (marginal) counts and reweighting schemes
:::


:::{.fragment}
### Global Structure Constraints
- _Whole network_ belongs to a class of graphs (or PGMs).
- Tree/block (MRF/Chow-Liu/GLASSO), Stochastic Block Model, etc.;
:::


::::{.fragment}
### Resource and Information Flow
- Limit total network activation a shared pool of "stuff"
- "Stuff" gets distributed somehow.
- E.g. Optimal Transport (Doubly Stochastic/Sinkhorn eOT)Resource Allocation, Geodesics (HSS)

:::{.callout-note}
Related to partial pooling in hierarchical bayes, sitting between unpooled and fully pooled priors.  
:::
::::

## Taxonomy of Modeling Assumptions -- II  

_Data Sampling Assumption: model vs. data_

. . .

Data is noisy, warped, etc. ...what makes it that way? 

Forward map (model $\rightarrow$ data)
: $\mathbf{x} = F(\mathbf{p})\qquad F:\mathbb{R}^{l}\rightarrow \mathbb{R}^n$

. . .

_Given_ the data space observations, we would need the _inverse map_ $F^{-1}(\mathbf{x})$,

> uncertainty is related to precision and **trueness**. 

. . .

If we _have_ model space observations (data assumed to match theoretical process),

> uncertainty is in _precision_ only. 

:::{.callout-important}
- Are we already in model-space, by assumption?
- Are we solving an _inverse problem?_
:::


##  Taxonomy of Modeling Assumptions -- III 
_Data Availability: observations vs. projection_

. . .

Does our method start with the _full_ bipartite dataset? \
Or is it post-processing from marginal counts alone?

:::{#fig-avail-data layout="[[1,2]]"  layout-valign="center"}
{{< embed content/codefigs/graphs.qmd#fig-bipartite >}}
{{< embed content/codefigs/graphs.qmd#fig-collab >}}
:::

## Organizing Recovery Methods

{{< embed content/codefigs/graphs.qmd#tbl-roads >}}

## A "Path" Forward

We would like a method with:

- Additive/local assumptions _(ease of computation, interpretation)_
- Data-space assumptions _(inverse problem)_
- Full usage of bipartite observations _(uncertainty quantification)_


# **Latent Graphs with Desire Paths**

---

1. Background 
2. Organizing Current Approaches
3. **Desire Path Density Networks**
4. Forest Pursuit
5. Experimental Comparison
6. Case Studies
7. Conclusion


## Gambit of the Group

Can we use co-occurrence counts for association measures? Are we:

> [...] interested primarily in whether people know one another, and collaboration on any topic is a reasonable indicator of acquaintance.
>
> Newman & Girvan (2004)

...i.e. assuming _model space_ observations?

. . .

If we're not so explicit, are we:  

> "So, consiously or unconsciously, many ethnologists studying social organization make what might be called the 'gambit of the group': they assume that animals which are clustered [...] are interacting with one another and then use membership of the same cluster [...] to define association."
>
> -- [Whitehead & Dufault (1999) [@Techniquesanalyzingvertebrate_Whitehead1999]]{.cite}

"Group-based methods" later cited as possible reason for bias in assortativity in social networks [@PerceivedAssortativitySocial_Fisher2017].

## Gambit of the Inner Product

The Gram matrix of co-occurrence underlies so many methods, but what is is _saying_?

Matrix Multiplication as sum of outer products
: $$
  G(j,j') = X^T X = \sum_{i=1}^m X(i,j)X(i,j')= \sum_{i=1}^m \mathbf{x}_i\mathbf{x}_i^T
  $$


- Creates $m\times m$ matrices
- Have a 1 in every $j,j'$ entry where nodes $j,j'$ both occurred
- Implicitly asserts that each observation is a _complete graph_

> _a sum of cliques_ $\rightarrow$ _clique bias_

---

{{< embed content/codefigs/graphs.qmd#fig-stack-outerprod >}}

---

:::{#fig-stacked-graphs layout="[[1,2]]"  layout-valign="center"}

{{< embed content/codefigs/graphs.qmd#fig-obs-set >}}
{{< embed content/codefigs/graphs.qmd#fig-stack-bow >}}

Inner-product projections as sums of cliques illustrating "clique bias". 
:::

## Does this make sense?

This framing lets practitioners do a "sanity-check"

> _Do clique observations make sense for my community_?

. . .

From a scaling standpoint:

- a "planar" community (office layout?) adds ~3 relations per author, per paper;
- a tree community (organizational structure?) adds ~1 relation 
- cliques add relationships _quadratically_ w.r.t. authors!

. . .

So...\
Bigger papers have _quadratically_ **more** information about connectivity in our social network?

. . .

Don't bigger papers tell us **less** about who "knows" who?  

## Networks as Unions of Subgraphs

_IDEA: why limit ourselves to cliques?_

- Map an operation onto each observation
- Reduce to an aggregate edge guess over all observations


## Graph Observations as Vectors

:::{.callout-note title="Recall"}
If we _were_ a social scientist "on the ground":

- _Author (g) asked (e), (h), and (c) to co-author a paper, all of whom agreed_
- _(i) asked (f) and (j), but (j) wanted to add (b)'s expertise before writing one of the sections_
- _...etc._ 
:::


::::{#fig-incidence-rep layout="[[1,1]]" layout-valign="center"}


{{< embed content/codefigs/graphs.qmd#fig-socsci >}}

::: {#fig-edge-mat}
$$
%X(\{1,2,3,4,\cdots\})=\\
R^{n\times\omega}=
\begin{array}{c c}\small
 & \begin{array}{} e_1 & e_2 & e_3 & e_4 & e_5 & e_6 & e_7 & e_8 & e_9 & e_{10} & e_{11}\\ \end{array} \\
\begin{array}{c c } x_1\\x_2\\ \vdots \end{array} &
\left[
\begin{array}{}
 \  .\: &  \:. \: &  \:. \: &  \:. \: &  \:1 \: &  \:. \: &  \:. \: &  \:. \: &  \:1 \: &  \:1 \: &  . \ \\
 \  .\: &  \:. \: &  \:. \: &  \:1 \: &  \:. \: &  \:. \: &  \:1 \: &  \:1 \: &  \:. \: &  \:. \: &  . \ \\
  &&&& \vdots &&&&&
\end{array}
\right]
\end{array}
$$

: Observations embedded in "edge space"

:::
::::


## Unions of Constrained Subgraphs

_Idea:_  

- Use domain knowledge to estimate ($\mathbf{r}_i$)
- Dependencies: **constrain** each paper to it's own distribution of dependencies. 
- Additive: use marginal counts of $R$ _rather than $X$_

> Principled way to "sum" over $\mathbf{x}_i$? 


---

![](https://upload.wikimedia.org/wikipedia/commons/2/2e/Desire_path_-_52849400711.jpg)

:::{.attribution}
dankeck, CC0, via Wikimedia Commons
:::


## Latent graphs as Desire Path Density Estimates

Complex Networks as _latent_ models. [@Measurementerrornetwork_Wang2012; @Statisticalinferencelinks_Peel2022]\
$\rightarrow$ We can't know if a link is "real", because _"real" is hard to define_!

- Is a "friendship" true/false?
- Is a colleague's influence boolean?

**Instead**: "would some expert reasonably answer YES vs NO?"

. . .

Alternate framing: would _paving_ a desire path make more sense than _not paving_? 

- pick a class of graphs to **constrain** dependencies
- each observation **treads** on certain edges.
- estimate probability that **paving** ($r=1$) is more likely than **not paving** ($r=0$)

## DPDE graph reduction: probability of "paving"

- Assume: edge "paving" given "treading" history is iid.
- Use Beta-Bernoulli on marginal counts $\sum_i \mathbf{r}_i$

What prior?

. . .

Presented with two equal-length desire paths:

- Will choose one tread more often before (the "more beaten" path).
- Nearly always, even for small difference $\rightarrow$ _bathtub distribution_. 
- coin-tossing game: "fraction of time **ahead**" $\rightarrow$ _arcsine_ 

$$
\begin{gathered}
\pi_e \sim \text{Beta}(\alpha, 1-\alpha), e\in E\\ 
E \sim \text{Bernoulli}(\pi_e), e \in E
\end{gathered}
$$

:::{.notes}
In a coin tossing game where each "heads" gives a point to player A and "tails" to player B, then the point differential is modeled as a random walk.
The arcsine distribution $\text{Beta}(\tfrac{1}{2},\tfrac{1}{2})$ is the fraction of time we expect one player to be "ahead" of the other. [@WhatisArcsine_Ackelsberg2018]  

> "_Contrary to popular opinion, it is quite likely that in a long coin-tossing game one of the players remains practically the whole time on the winning side, the other on the losing side."_
> 
> William Feller[@IntroductionProbabilityTheory_Feller1968, Chapter III]
:::


---

_what class of subgraphs do we choose for $\mathbf{r}_i$?_

. . .

Trees

# Forest Pursuit
Recovery from random walks in near-linear time

---

1. Background 
2. Organizing Current Approaches
3. Desire Path Density Networks
4. **Forest Pursuit**
5. Experimental Comparison
6. Case Studies
7. Conclusion

## Random walk activations

Random walk model of node activation:

> Visiting a node leads to its activation

- Spreading and diffusive processes
- Infectious disease, cognitive search, information cascades, etc. 

. . .

In our case two pieces of information would have been censored:

  - Individual jumps hidden (only visited nodes)
  - order of visits hidden (only binary activation)

What do we know? 

## Random walk dependencies are trees

Assume:

- Single-cause _(only one colleague's request preceded your joining)_
- Single-root _(only one colleague originated the "asking")_

. . . 

The global network might not be a tree, but ...

. . .

The _activation dependencies_ for each random walk must be.

- Each new activation comes from a single activated node
- Dependencies are connected with no loops

:::{.notes}
This could be justified from an empirical perspective as well: say we observe an author turn down requests for one paper from two individuals, but accept a third.
We could actually infer a _lowered_ dependency on the first two, _despite_ the eventual coauthorship.
Only the interaction that was observed as successful necessarily counts toward  success-dependency, barring any contradicting information.
:::

## Spanning Forest Distribution - Regularized Laplacian
::::{.columns}

:::{.column}
**Constrain** papers $\mathbf{r}_i$ to be _dependency trees_

Spanning forest
: a collection of disjointed trees covering every vertex

In our model, being on the same paper is being on the **same tree** in a forest. 

{{< embed content/codefigs/graphs.qmd#fig-socsci >}}

:::
:::{.column}

{{< embed content/codefigs/graphs.qmd#fig-stack-tree >}}
:::
::::



## Sparse Approximation
$$\mathbf{r}_i \gets f(\mathbf{x}_i)$$

. . .

What are we doing here? ... _Sparse Approximation_?

$$
\mathbf{\hat{r}} = \operatorname*{argmin}_{\mathbf{r}}{\|\mathbf{x}-B^T\mathbf{r} \|_2^2} \quad \text{s.t.} \quad \|\mathbf{r}\|_0 = \|\mathbf{x}\|_0 - 1
$${#eq-sparse-approx-tree}

. . .

Usually solve with _matching pursuit_[@Matchingpursuitstime_Mallat1993].

> Each iteration selects the atom with the largest inner product $\langle \mathbf{b}_{i'},\mathbf{x}\rangle$.

. . .

But this doesn't work (in the standard plus-times ring, see tropical factorization [@Sparsedataembedding_Omanovic2021]).

> $B^TR$ has the shape and sparsity of $X$, where each entry is the node's degree in its tree.  

## Forest Pursuit -- Sums of Steiner Trees

Instead, we will take an _Empirical Bayes_ approach: 

$$
\mathbf{\hat{r}} = \operatorname*{argmax}_{\mathbf{r}}{\mathcal{L}(\mathbf{r}|\mathbf{x})} \quad \text{s.t.}\quad \mathbf{r}\sim \mathcal{T}(G^*[\mathbf{x}])
$$

- Want: point estimate for most likely tree (per paper) 
- Use an empirical prior as shrinkage for a Max. Likelihood estimate.
- "Number of times authors were on the same tree"


. . .


Our point estimate is the mode of the spanning tree distribution on activated nodes\
...i.e. the Maximum Spanning Tree (MST)

> Prim's: each iteration selects the edge with the smallest distance to the current tree 

. . .

- Essentially the Chow-Liu tree for each observation (_Technically_ a "Steiner Tree"[@fastalgorithmSteiner_Kou1981])
- $\rightarrow$ _use Matrix Forest Theorem [@MatrixForestTheorem_Chebotarev2006; @Countingrootedforests_Knill2013; @Semisupervisedlearning_Avrachenkov2017] to link $X^TX$ to a metric closure of the dependency graph_.




---

> _does it work?_ 

# MENDR

**M**easurement **E**rror in **N**etwork **D**ependency **R**econstruction

Simulation Study & Test-bench

---

1. Background 
2. Organizing Current Approaches
3. Desire Path Density Networks
4. Forest Pursuit
5. **Experimental Comparison**
6. Case Studies
7. Conclusion


## Synthetic Dataset

Community tooling for community problem-solving

- Open dataset, testbench, and python library
- Facilitates consistent referencing of challenge problems, and execution of methods
- Reproducible via Data Version Control (import as a dependency anywhere!)
- Easily extensible: pull-request on Github can add new methods or graphs/datasets. 

{{< include content/part2/tbl-mendr.qmd >}}

## Compared Methods

{{< include content/part2/tbl-methods.qmd >}}

. . .

Metrics:
: - Expectation over thresholds $E[\text{score}]$
  - Matthew's Correlation Coefficient (MCC)
  - Fowlkes-Mallows (F-M)
- Average Precision Score (APS)



## Results -- overall


{{< embed content/codefigs/results.qmd#fig-fp-overall >}}

- FP is the only approach with greater than 0.5 expected score over _all_ thresholds!
- _By a large margin..._ 
- GLASSO has a good APS (but so does plain Cosine Similarity?) 
- By graph-type, better than GLASSO even where it is known to be accurate (trees, blocks)

## Results -- by graph type {visibility="hidden"}

::::{.columns }
:::{.column width="40%"}

- FP appears to have best APS for Trees
- GLASSO is not much better than CS for scale-free _or_ block graphs (despite theory)
- Scale-free networks are _hard_ to infer...but FP is a significant improvement. 

More to come on APS "discrepancy"...

:::

:::{.column width="60%"}
{{< embed content/codefigs/results.qmd#fig-fp-compare >}}

:::
::::


## Results -- trends {.scrollable}

{{< embed content/codefigs/results.qmd#fig-mendr-trends >}}


## Results -- partial residuals

How does "more data" impact the network estimate quality? 

Because of how MENDR samples random walks, we need to de-correlate those trends

. . .

{{< embed content/codefigs/results.qmd#fig-partials-mcc >}}

- CS Struggles with longer walks (more activations); FP unaffected.
- GLASSO & FP (less-so) struggle as network size gets bigger (more edges).
- FP benefits the _most_ from more observations. 

## Runtime Performance

::::{.columns }
:::{.column width="40%"}

_How fast is it?_

- FP consistently at the lower-bound of GLASSO runtime
- 1 - 3 orders of magnitude faster
- Ill-conditioned matrices lead to many convergence failures for GLASSO 

:::

:::{.column width="60%"}
{{< embed content/codefigs/results.qmd#fig-runtime >}}
:::
::::


## Runtime Performance -- partial residuals
Theoretically, _Forest Pursuit_ is:  

- _Linear_ (and parallel!) in observation count $n$
- Linear in expected walk-length $\|\textbf{x}_i\|_0$ (row-density) via Prim's
- _Constant_ in network size (given walk-length, i.e. diffusion-rate is known)

. . .

Empirically:

{{< embed content/codefigs/results.qmd#fig-partials-runtime >}}

:::{.notes}
scaling in n-jumps looks more like $O(n\log n)$ because we had to use Kruskal's 
:::

## Modifications to Forest Pursuit

### Re-weight by Interaction-rate (FPi) 

- Scale edge probability by co-occurrence rate
- APS differential w/GLASSO is _gone_
- Sacrifices stability, and $E[\text{score}]$

### Expected Forest Maximization (EFM) 

- Generative/Bayesian Formulation
- Root node spanning trees
- E-M Iteration: (very) slight improvement, high compute cost

> See full document for more detail :) 




# Case Study
_Les Misérables_


---

1. Background 
2. Organizing Current Approaches
3. Desire Path Density Networks
4. Forest Pursuit
5. Experimental Comparison
6. **Case Studies**
7. Conclusion



## Les Misérables Character Network

Network of character co-occurrence _by chapter_.

- What are we trying to measure? _What characters influence the appearance of others?_
- How is network topology (and derived metrics) affected?
- Does correcting for clique bias imply correction for other biases? 

## Les Misérables -- Co-Occurrence {transition="fade"}

{{< embed content/codefigs/qualitative.qmd#fig-lesmis-cooc >}}


## Les Misérables -- Co-Forest Pursuit {transition="fade"}

{{< embed content/codefigs/qualitative.qmd#fig-lesmis-fp >}}


## Les Misérables -- Centrality Ranks {transition="fade"}

{{< embed content/codefigs/qualitative.qmd#fig-lesmis-centrality >}}

## Les Misérables -- Summary {transition="fade"}

- Centrality for Fantine, Eponine, and Cosette much higher with FP
- Why were they so low in the first place? See:

  - "Bechdel Test"[@MediaMarginsPopular_Savigny2015]
  - Systemic agency reduction in fiction[@genderagencygap_Stuhler2024].

> Estimating character importance _despite_ systematic interaction-reduction bias from authors.

It's _still_ easier to explain the occurrence of many characters _because of_ key female characters, despite their overall lower co-occurrence. 

# Case Study
_Animal Concept Map_

## Verbal Fluency Networks

Study participants list out members of some category as fast as possible in a time limit: 

Animals
: _S.1_ --- dog, cat, sheep, horse, fox, lion, giraffe, rhino... 
: _S.2_ --- cat, dog, wolf, coyote, elk, deer, ox, bison, cow, sheep... 
: _S.3_ --- dog, cat, bird, hawk, owl, pidgeon, rat, crocodile, lizard... 

Based only on these lists, and the proximity between the words in them, let's try to make a "map" of animals _according to people's associations in memory_.  

. . .

- Using _windowed co-occurrences_ to reflect working memory limitations [@magicalnumberseven_Miller1956]...
 
- How efficiently can we represent these networks? (range between tree and complete-graph!)
- How diverse are edge types when investigation the ways humans connect animals? 

:::{.callout-important}
For each of the following, assume filtering of edges until the graph would otherwise be disconnected (unless specified otherwise). 
:::

<!-- . . . -->

<!-- Connective Efficiency -->
<!-- : $$ -->
  <!-- \begin{gathered} -->
  <!-- \psi_n(e) = \frac{e_{\text{min}}}{e}\frac{e_{\text{max}}-e}{e_{\text{max}}-e_{\text{min}}}\\ -->
  <!-- e_{\text{max}} = \frac{n(n-1)}{2} \quad e_{\text{min}} = n-1 --> 
  <!-- \end{gathered} -->
  <!-- $$ -->

---


{{< embed content/codefigs/qualitative.qmd#fig-fluency-combined >}}


## Fluency Animals -- Doubly-Stochastic {transition="fade"}
 

Not very efficient; main connectivity/community is context/location-based. 

{{< embed content/codefigs/qualitative.qmd#fig-fluency-dsmin >}}



## Fluency Animals -- Chow Liu Tree {transition="fade"}
{{< embed content/codefigs/qualitative.qmd#fig-fluency-tree >}}

## Fluency Animals -- GLASSO {transition="fade"}
{{< embed content/codefigs/qualitative.qmd#fig-fluency-glassomin >}}


## Fluency Animals -- Forest Pursuit {transition="fade"}
{{< embed content/codefigs/qualitative.qmd#fig-fluency-fpmin >}}


## Fluency Animals -- Summary {transition="fade"}
FP is by far the more efficient in edge usage while maintaining connectivity. 

An analyst would also have an easier job of creating "edge-type inventories", making the FP backbone an excellent exploratory assistant: animals can be related because they are: 

- Co-located
- Taxonomically similar (_cheetah_+_leopard_)
- Famous predator/prey pairs (_cat_+_mouse_)
- Pop-culture references (_lion_$\rightarrow$_tiger_$\rightarrow$_bear_)[^oz]
- Similar in ecological niche/role (_koala_+_sloth_)
- Related through conservation or public awareness (_panda_+_gorilla_)
- etc.

[^oz]: Note that the dependency-based methods correctly interpred these three as _not_ being mutually connected in a triad, but specifically with this ordering (_tiger_ in the middle). 

## Fluency Animals -- Importances {transition="fade"}

{{< embed content/codefigs/qualitative.qmd#fig-animal-centrality >}}


## Fluency Animals -- Structure Preservation {transition="fade" .scrollable}

{{< embed content/codefigs/qualitative.qmd#fig-fluency-preservation >}}



# Conclusion


---

1. Background 
2. Organizing Current Approaches
3. Desire Path Density Networks
4. Forest Pursuit
5. Experimental Comparison
6. Case Studies
7. **Conclusion**


## Thesis Contributions 


Taxonomy
: Novel organization of modeling assumptions for network inference to assess theoretical gaps.

. . .

Sparse Approximation
: Recontextualize network recovery in _edge space_ using Desire Path Density framework. 

. . .


Forest Pursuit
: Scalable and flexible algorithm for dependency recovery from random walks. 

. . .

MENDR
: Dataset and benchmarking toolkit to compare recovery capability.

. . .

Generative Model
: Generalization of Forest Pursuit inspired by probabilistic factorization & Dictionary Learning.

## Future Work

- Multiple Sources & Hidden Nodes

  - Allowing for multi-source random walks by explicitly including the "root" R
  - Allow for adding unseen nodes e.g. with addition of steiner nodes a la [@TreeIam_Sonthalia2020]

. . .

- Generalizing Inner Products

  - Non-euclidean metric spaces for Desire Path Densities
  - Causality-preserving Lorentz embedding [@EmbeddinggraphsLorentzian_Clough2017] applied to  voronoi normalizing flows [@HyperbolicVoronoiDiagrams_Nielsen2010; @SemiDiscreteNormalizing_Chen2022]

. . .

- New applications & case studies

  - Bigger inventories of effects on assortativity and centrality
  - Re-assess application of semantic fluency e.g. for early-onset Alzheimer's and Schizophrenia detection
  - Human-in-the-loop interfaces for grounded coding of _relationship types_ between concepts 

# Thank You 



# Literature Review  {visibility="uncounted"}

## Basics -- Sample Observations in Feature Space {visibility="uncounted"}


For generality, say a practitioner records their measurements as scalar values, _i.e._, $x\in\mathbb{S}\in\{\mathbb{R,Z,N},\cdots\}$.
Data can be represented as a "design matrix" $X:\mathbb{S}^{m\times n}$

- $n$ independent/input variable features
- over the course of $m$ observations
 
$$\begin{gathered}
x=X(i,j)\qquad X : I\times J \rightarrow \mathbb{S} \\
i\in I=\{1,\cdots,m\}, \quad j\in J=\{1,\cdots,n\},\qquad I,J:\mathbb{N}
\end{gathered}$$

. . .

observations
: $\mathbf{x}_i=X(i,\cdot),\quad \mathbf{x}:J\rightarrow\mathbb{S}$

features
: $\mathbf{x}_j'=X(\cdot,j),\quad \mathbf{x}':I\rightarrow\mathbb{S}$

Marginal sums give co-occurrences, matrix products give co-occurrences, etc.

:::{.callout-important}
We are interested in the _feature_ relationships (author-author), _i.e._ feature/representation learning when $\mathbb{S} \equiv \mathbb{B}$ 
:::

## Basics -- Incidence Structures {visibility="uncounted"}

Unifying graphs and design matrices: _edge-centric parameterizations!_^[
  Closed-form ID manipulation for $\{u_n(e),v_n(e)\}\leftrightarrow e_n(u,v)$ [@ParallelEuclideandistance_Angeletti2019] \
  Gives an unambiguous encoding for oriented and un-oriented incidence, and with them $L$, $A$, $D$, etc. 
]

Graphs via _complete_ incidence matrix $B$: 
: $$
  \begin{gathered}
  G  = (V,E,B) \quad s.t. \quad B : E\times V \rightarrow S\\
  v \in V = \{1,\cdots, n\}\quad 
  e \in E = \left\{1,\cdots, {n\choose 2} \right\}
  \end{gathered}
  $$ 

. . .

Observations of graphs (as _edges_)
: $$
  \begin{gathered}
  R(i,e) = \min(\{B_i(e, u_n(e)),B_i(e,v_n(e))\}) \\
  \quad \textrm{s.t.} \quad R:I\times E \rightarrow \mathbb{S}\\
  i\in I = \{1,\cdots,m\} \qquad e \in E=\left\{1,\cdots,\omega\right\}\\
  n=\tfrac{1}{2}(1+\sqrt{8\omega+1})
  \end{gathered}
  $$




## Taxonomy of Modeling Assumptions -- I {transition="fade" visibility="uncounted"}


Local Structure/additivity
: We could rely on linear combinations of observations having assumed edge representations. \
  Generally called _association measures_.\
  Built on (marginal) counts $\mathbf{s}$ via inner product (Gram matrix $X^TX$) e.g. co-occurrences.

. . .

:::{#nte-cs .callout-note title="Ochiai Coefficient (Cosine)"}
Effectively an uncentered correlation, but for binary observations the "cosine similarity" is also called the _Ochiai Coefficient_ between two sets $A,B$, where binary "1" stands for an element belonging to the set.[@Measuresecologicalassociation_Janson1981]
$$
\frac{|A \cap B |}{\sqrt{|A||B|}}=\sqrt{p_{1\bullet}p_{\bullet 1}} \rightarrow  \frac{X^TX}{\sqrt{\mathbf{s}_i\mathbf{s}_i^T}}\quad \mathbf{s}_i = \sum_i \mathbf{x}_i
$$  
:::

:::{#nte-hyp .callout-note title="Hyperbolic Projection"}
Attempts to account for the overcounting of co-occurrences on frequently occurring nodes, vs. rarer ones.[@Scientificcollaborationnetworks._Newman2001]
$$ X^T\text{diag}(1+\mathbf{s}_j)^{-1}X \quad \mathbf{s}_j = \sum_j{\mathbf{x}_j'}$$
This re-weights each observation by its degree in the original bipartite graph.  
:::

See also: _Yule's Q/Y_, _Mutual Information_, _odds ratios_, etc.  


  
## Taxonomy of Modeling Assumptions -- II {transition="fade" visibility="uncounted"}


Global Structure Constraints
: We could assume the _whole network_ belongs to a class of graphs.\
  e.g. Tree, block, SBM, etc.; common for MRF inference. 

. . .


:::{#nte-chowliu .callout-note title="Chow-Liu Spanning Tree"}
Enforces tree structure globally.
Approximates a joint probability
$$
P\left(\bigcap_{i=1}^n v_i\right) \approx P' = \prod_{e\in T} P(u_n(e)|v_n(e)) \quad T\in \mathcal{T}
$$
where $\mathcal{T}$ is the set of spanning trees for the nodes.
The Chow-Liu tree minimizes the Kullback-Leibler (KL) Divergence $\text{KL}(P \| P')$ by finding the minimum spanning tree over pairwise mutual information weights.[@Approximatingdiscreteprobability_Chow1968]
:::

:::{#nte-glasso .callout-note title="GLASSO"}

Semidefinite program to find (regularized) maximum likelihood precision of graph-structured multivariate Normal distribution using $\ell_1$ ("LASSO") penalty [@Sparseinversecovariance_Friedman2008]

$$
\operatorname*{argmax} \mathcal{L}(\Theta|\hat{\Sigma})
  = \operatorname*{argmin}_{\Theta \prec 0}\ \text{tr}(\hat{\Sigma} \Theta) - \log |\Theta|+ \rho\|\Theta\|_1
$$ {#eq-glasso}

In the binary case @eq-glasso is still guaranteed to find the structure of the generating MRF, but _only for graphs with singleton separators_, as shown in @Structureestimationdiscrete_Loh2012.
:::

## Taxonomy of Modeling Assumptions -- III {transition="fade" visibility="uncounted"}

Resource and Information Flow
: We limit total network activation to a pool of shared "stuff" that gets distributed somehow.\
  Related to partial pooling in hierarchical bayes, sitting between unpooled and fully pooled priors.  

. . .

:::{#nte-rp .callout-note title="Resource Allocation"}
Goes one step further than hyperbolic projection, by viewing each node as having some "amount" of a resource to spend, which gets re-allocated by observational unit. [@Bipartitenetworkprojection_Zhou2007]
$$ \text{diag}(\mathbf{s}_i)^{-1}X^T\text{diag}(\mathbf{s}_j)^{-1}X $$
_i.e._ two-step random-walk normalization of the bipartite adjacency matrix.\
First $X$ is row-normalized, then column-normalized.
:::


:::{#nte-ot .callout-note title="Doubly Stochastic"}
If $A\in \mathbb{S}^{n\times n}$ is positive, then there exists $d_1,d_2$ such that $$W=\text{diag}(d_1)A\text{diag}(d_2)$$ is doubly-stochastic, and $W(u,v)$ is the _optimal transport plan_ between $u$ and $v$ with regularized Euclidean distance between them on a graph.[@RobustInferenceManifold_Landa2023;@Sinkhorndistanceslightspeed_Cuturi2013]

The doubly-stochastic filter [@twostagealgorithm_Slater2009] removes edges from $W$ until just before the graph would become disconnected.
:::

## Taxonomy of Modeling Assumptions -- III {transition="fade" visibility="uncounted"}

Resource and Information Flow
: We limit total network activation to a pool of shared "stuff" that gets distributed somehow.\
  Related to partial pooling in hierarchical bayes, sitting between unpooled and fully pooled priors.  

:::{#nte-hss .callout-note title="High-Salience Skeleton"}
Count the number of shortest-path trees an edge participates in, out of all the shortest-path-trees (one for every node).
$$ \frac{1}{n}\sum_{i=1}^n \mathcal{T}_{\text{sp}}(i) $$
where $\mathcal{T}_{\text{sp}}(n)$ is the shortest-path tree rooted at $n$ 
[@Robustclassificationsalient_Grady2012]

:::

## Desire Path Math {visibility="uncounted"}

So, each $\mathbf{x}_i$ will induce a subgraph $g_i = G^*[V_i]$, where $V_i = \{v\in \mathcal{V} | X(i,\mathcal{V})=1\}$.

- domain knowledge takes the form of a constraint on edges within that subgraph, which will induce a family of subgraphs on $g_i$.
- This family  $\mathcal{C}_i$ belongs to a relevant class of graphs $\mathcal{C}$, limited to nodes $V_i$, _i.e._,

$$
\begin{gathered}
\mathcal{C}_i = \{(V,E,B_i) \in\mathcal{C}|B_i(e,v)=B^*(e,v)\mathbf{1}_{V_i}(v)\mathbf{1}_{E_i}(e)\}\\
E_i\in\{\mathcal{E}\in\mathcal{P}(E)| g_i[\mathcal{E}]\in\mathcal{C}\}
V_i = \{v\in \mathcal{V} | X(i,\mathcal{V})=1\}
\end{gathered}
$$

:::{.callout-important}
In other words, edges that "caused" to the node activations in a given observation

1. must _together_ belong to a graph that, in turn,
2. belongs to our desired class, which must occur on the nodes that were activated.
:::

## Matrix-Forest Theorem {visibility="uncounted"}

The regularized Laplacian $Q=(I+\beta L)^{-1}$

- Is always PD (provably on Birkhoff Polytope of doubly-stochastic matrices)
- for various $\beta$: 
  - $\beta=1$, $Q(j,j')$ encodes the fraction of spanning forests in which $j,j'$ share a tree (i.e. visited by the same walk i.e. **co-occurred**)
  - $\beta\rightarrow 0$ encodes shortest-path kernel
  - $\beta\rightarrow \infty$ encodes commute-time kernel (i.e. $L^+$)
- $Q(j,j)$ encodes an "isolation" measure (probability of not sharing a cascade)

::: notes
similar form to CAR kernel (for GPR/Cov prior), gaussian copula, etc. 
:::

. . .

**IDEA**: our data is measuring $\lim_{n\rightarrow \infty} X^TX \propto Q$, s.t. each $\mathbf{x}$ comes from a _tree_


:::{.notes}
It's important to add here that _mutual convincing_ by multiple collaborators simultaneously (or over time) is expressly left out.
In other words, only pairwise interactions are permitted.
This is not an additional assumption, but a key limitation of our use of graphs in the first place!
As Torres et al. go to great lengths elaborating in [@WhyHowWhen_Torres2021], it is critical to correctly model dependencies when selecting a structural representation of our problem to avoid data loss.
The possibility for multi-way interactions would necessitate the use of either a simplicial complex or a hypergraph as the carrier structure, _not a graph_. 
:::

## Forest Pursuit -- Algorithm {.scrollable visibility="uncounted"}

{{< include content/part2/alg-fp.qmd >}}

# Forest Pursuit Modifications & Extensions {visibility="uncounted"}


## FPi (Interaction Probability){visibility="uncounted"}

:::::{.columns}
::::{.column}
Let's fix our APS performance!

- GLASSO is estimating edge strength, not "existence"
- We can too: probability of edge traversal (i.e. rescale by co-occurrence probability)
- This gives "FPi", an estimate for the rate of direct interaction 

> Completely closes the APS gap, while sacrificing MCC/F-M!

::::

::::{.column .fragment}
::: {layout-ncol=2}

{{< embed content/codefigs/results.qmd#tbl-fpi >}}

{{< embed content/codefigs/results.qmd#fig-fpi >}}

:::
::::
:::::

## {visibility="uncounted"}


![P-R curves for two experiments](content/images/PR.svg){#fig-pr-curves}


## Generative Model {visibility="uncounted"}

::::{.columns}
:::{.column}
Random spanning trees on an augmented graph are precisely the random spanning forests on the original. 

Marking a single node can "root" a tree in the forest. 
:::
:::{.column}
{{< embed content/codefigs/graphs.qmd#fig-inject-plan >}}
:::
::::

Leads to generative model for multi-output binary activations on a graph: 



## Generative Model -- Specification {visibility="uncounted"} 

- Generate random spanning trees on $G_{+R}$, given the edge weights $z_E$.
- Uniformly sample a marked node $\phi$.
- Activate all nodes connected to $\phi$ in the induced spanning forest.

. . .

$$
\begin{aligned}
\pi_{e\in E} &\sim \operatorname{Beta}_E(\alpha, 1-\alpha)     \\
z_{e\in E} &\sim \operatorname{Bernoulli}_E(\pi_e)             \\
\phi_{i\in I, j\in J} &\sim \operatorname{Categorical}_I(\theta) \\
x_{i\in I,j\in J} &\sim \operatorname{RSFm}_K(\beta, z_e, \phi_n) \\
\end{aligned}
$$ 


## Generative model -- Forests {visibility="uncounted"}


$$
P(x_{ij}|z_E, \phi_{ij}) = \sum_{T\in\mathcal{T}_{+R}}P(x_{ij}| T,\phi_{ij}) P(T|z_E)
$$

Is the same as:

$$
P(x_{ij}|z_E, \phi_{ij}) = \sum_{F\in\mathcal{F}}P(x_{ij}| F,\phi_{ij}) P(F|z_E)
$$


$$
\begin{gathered}
P(F|z_E) = \frac{1}{Z_\mathcal{F}} \mu(F)\\
Z_\mathcal{F} = \det{(I+\beta L)}
\end{gathered}
$$


Likelihood: $\sum_\mathcal{F} P(x_{ij}|F,\phi_{ij})$ is the probability a node occurs on the same subtree as a "marked" on

## Expected Forest Maximization {visibility="uncounted"}
Alternate between estimating recovered network $B$ and representation of data in edge-space $R$:

1. _Forest Pursuit_ estimate $\hat{B}$
2. Estimate $d_Q$ via _Forest Kernel_ $\hat{Q} \gets (I+\beta B^TB)^{-1}$
3. Use $d_Q$ for Steiner tree approximation on metric closure ()
4. ...etc. 

## {visibility="uncounted"}

{{< include content/part2/alg-efm.qmd >}}

:::{.notes}
1. small $\beta$ to approximate shortest paths closure
2. symmetric norm-Laplacian to avoid degree-bias when inverting for $Q$ _locally_
:::

## EFM Results {visibility="uncounted"}

- Small but consistent improvement in reconstruction performance across MENDR dataset. 
- On a per-edge basis, (heavily CV-regularized) logistic regression study shows

  - Slightly positive odds ratio for a correct prediction. 
  - Improvement is agnostic to graph type. 

:::{#fig-efm-results layout="[[1,2]]"  layout-valign="center"}

{{< embed content/codefigs/results.qmd#fig-efm-mcc  >}}

{{< embed content/codefigs/results.qmd#fig-efm-logits  >}}

:::

## EFM Runtime {visibility="uncounted"}

::::{.columns}
:::{.column width="30%"}
- EFM still competitive with GLASSO, outperforming it in most cases
- Trend appears to have a fairly consistent multiplicative factor ($\approx10$), at all scales
:::
:::{.column width="70%"}
{{< embed content/codefigs/results.qmd#fig-efm-runtime >}}
:::
::::

## {visibility="uncounted"}

{{< embed content/codefigs/results.qmd#fig-efm-partials-runtime >}}

Most of the runtime penalty appears to occur from network size (perhaps the $\|\cdot\|_{\infty}$ norm?) 


# Metrics {visibility="uncounted"}

## Metrics -- Precision and Recall {visibility="uncounted"}
:::::{.columns}
::::{.column}

![](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg){height=500px}

:::{.attribution}
Walber, CC BY-SA 4.0 <https://creativecommons.org/licenses/by-sa/4.0>, via Wikimedia Commons
:::

::::
::::{.column}

Various goals when balancing true/false-positives and true/false-negatives: 

:::{#nte-precision .callout-note title="Precision (P)"}
Fraction of positive predictions that are true:
 $$P= \frac{TP}{TP+FP}$$

also called "positive predictive value" (PPV)
:::

:::{#nte-recall .callout-note title="Recall (R)"}
Fraction of true values that were returned as positive.
$$R=\frac{TP}{TP+FN} $$

_Inherent trade-off with precision._

Also called the TP-rate (TPR)

:::
::::
:::::

## Metric -- Aggregation{visibility="uncounted"}

:::::{.columns}
::::{.column}
Each metric compares a (binary) prediction to the (binary) ground truth.

- Class probabilities $\rightarrow$ _families_ of metric results 
- How can we pick one threshold in an unsupervised setting?


Let's find the "expected value" over all thresholds!

- Normalize all outputs to $[\epsilon,1-\epsilon]$
- "average" metric is what a practitioner expects if thresholding at random
- APS is more traditional, but is high if at least one threshold is good...

::::
::::{.column}

:::{#nte-mcc .callout-note title="Matthews Correlation Coefficient (MCC)"}
Balances all of TP,TN,FP,FN.

  $$\frac{TP\cdot TN - FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$
  
Preferred for class-imbalanced problems (like sparse recovery) [@statisticalcomparisonMatthews_Chicco2023]
:::

:::{#nte-fm .callout-note title="Fowlkes-Mallows (F-M)"}
Geometric mean of Precision and Recall

  $$\sqrt{P\cdot R}$$

Limit of the MCC as TN approaches infinity[@MCCapproachesgeometric_Crall2023]

:::

:::{#nte-aps .callout-note title="Average Precision Score (APS)"}
Expected precision over the possible recall values 
$$\text{APS} = \sum_{e=1}^{\omega} P(e)(R(e)-R(e-1))$$
_approximates the integral under the parametric P-R curve_
:::
::::
:::::


# NetSci Community{visibility="uncounted"}

## NetSci Dataset{visibility="uncounted"}

Recreating dataset from Newman & Girvan (2004) @Findingevaluatingcommunity_Newman2004

- Web of Science queries for all authors/papers from literature reviews on Network Science
  - Newman (2003) @StructureFunctionComplex_Newman2003
  - Boccaletti et al (2006) @ComplexnetworksStructure_BOCCALETTI2006
  - Additional paper by Sneppen & Newman (1997) @Coherentnoisescale_Sneppen1997 to retain connected component. 
- Initial coloring of detected communities from co-occurrence network
  - Modularity-based community detection (greedy algorithm)
  - Inset shows community (quotient) graph

> How does _degree distribution_ and _assortativity_ (node-neighbor-degree correlation) change?  

## NetSci Community -- Co-Occurrence {transition="fade" visibility="uncounted"}

{{< embed content/codefigs/qualitative.qmd#fig-netsci-cooc >}}

## NetSci Community -- Chow-Liu {transition="fade" visibility="uncounted"}

{{< embed content/codefigs/qualitative.qmd#fig-netsci-tree >}}


## NetSci Community -- Forest Pursuit {transition="fade" visibility="uncounted"}

{{< embed content/codefigs/qualitative.qmd#fig-netsci-fp >}}


## NetSci Community -- Summary {transition="fade" visibility="uncounted"}

- Forest Pursuit preserves the original community structure
- Balances co-occurrence and tree properties (less dense, no long chains)
- Tends to reduce assortativity and avg. degree (makes sense here?) 

{{< embed content/codefigs/qualitative.qmd#fig-netsci-degree >}}


# Working Memory & Partial Orders {visibility="uncounted"}

## Partially Ordered Set {visibility="uncounted"}
_Directed Acyclic Graphs_

Even our original data, if observed by the social scientist, is a "poset"

{{< embed content/codefigs/graphs.qmd#fig-obs-tree >}}

In that case we did not ultimately get orderings from the papers, _but_ other domains do!

> Technical Language Processing

Word order in sentences, tagging, etc. 


## TLP with initial-visit emitting random-walks{visibility="uncounted"} 

::::{.columns}
:::{.column}
We want to organize tags/concepts _using_ the way they occur in, say, Maintenance Work Orders (MWOs)

Other than Bag-of-Words (co-occurrence), common tool is order-$n$ _Markov Language Model

- assume the order-of-observation _is_ the order-of-activation
- use up to $n$ previous activations to help predict the next. 

$$
P_{MC}(t_i|T) \approx P(t_i | t_{i-1}, \cdots,t_{i-n})
$$

However, evidence points to memore recall being more like a random walk that reports initial-visits _only_.

:::
:::{.column}

{{< embed content/codefigs/graphs.qmd#fig-stack-markov >}}
:::
::::

## {visibility="uncounted"}

{visibility="uncounted"}

In Sexton & Fuge (2020)[@UsingSemanticFluency_Sexton2019; and @OrganizingTaggedKnowledge_Sexton2020] we demonstrate the usefulness of accounting for this. \
Compare label propagation on each network vs expert subsystem annotation in Excavator MWOs [@Cleaninghistoricalmaintenance_Hodkiewicz2016] 


![Semisupervised MWO Tag Classification with Network Label Propagation](content/images/ternary_ntags3_freq5_topn50_thres60_saveTrue.svg){#fig-excavate-ternary}

Still, nested stochastic gradient descent is _slow_. Can we use Forest Pursuit in a similar case?


# References {visibility="uncounted"}

## {visibility="uncounted"}
