@article{Scalablenetworkreconstruction_Peixoto2024,
  author      = {Peixoto, Tiago P.},
  date        = {2024-01-02},
  title       = {Scalable network reconstruction in subquadratic time},
  doi         = {10.48550/ARXIV.2401.01404},
  eprint      = {2401.01404},
  eprintclass = {cs.DS},
  eprinttype  = {arXiv},
  copyright   = {Creative Commons Attribution Share Alike 4.0 International},
  file        = {:Scalablenetworkreconstruction_Peixoto2024.pdf:PDF:http\://arxiv.org/pdf/2401.01404v5},
  keywords    = {Data Structures and Algorithms (cs.DS), Machine Learning (cs.LG), Data Analysis, Statistics and Probability (physics.data-an), Computation (stat.CO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Physical sciences},
  publisher   = {arXiv},
  year        = {2024},
}

@article{Networkreconstructionvia_Peixoto2024,
  author       = {Peixoto, Tiago P.},
  date         = {2024-05-02},
  journaltitle = {Phys. Rev. X 15, 011065 (2025)},
  title        = {Network reconstruction via the minimum description length principle},
  doi          = {10.1103/physrevx.15.011065},
  eprint       = {2405.01015},
  eprintclass  = {stat.ML},
  eprinttype   = {arXiv},
  issn         = {2160-3308},
  number       = {1},
  pages        = {011065},
  volume       = {15},
  copyright    = {Creative Commons Attribution Share Alike 4.0 International},
  file         = {:Networkreconstructionvia_Peixoto2024.pdf:PDF:http\://arxiv.org/pdf/2405.01015v3},
  keywords     = {Machine Learning (stat.ML), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Data Analysis, Statistics and Probability (physics.data-an), Populations and Evolution (q-bio.PE), FOS: Computer and information sciences, FOS: Physical sciences, FOS: Biological sciences},
  month        = mar,
  publisher    = {American Physical Society (APS)},
  year         = {2024},
}

@article{Assortmentoptimizationgiven_Vasilyev2025,
	author = {Vasilyev, Andrey and Maier, Sebastian and Seifert, Ralf W.},
	date = {2025-02-22},
	title = {Assortment optimization given basket shopping behavior using the
	         Ising model},
	doi = {10.48550/ARXIV.2502.16260},
	eprint = {2502.16260},
	eprintclass = {math.OC},
	eprinttype = {arXiv},
	copyright = {Creative Commons Attribution 4.0 International},
	file = {:Assortmentoptimizationgiven_Vasilyev2025.pdf:PDF:http\:
	        //arxiv.org/pdf/2502.16260v1},
	keywords = {Optimization and Control (math.OC), Data Structures and
	            Algorithms (cs.DS), FOS: Mathematics, FOS: Computer and
	            information sciences},
	publisher = {arXiv},
	year = {2025},
}

@article{Randomlysamplingbipartite_Neal2023,
	author = {Neal, Zachary P.},
	date = {2023-05-08},
	title = {Randomly sampling bipartite networks with fixed degree sequences},
	doi = {10.48550/ARXIV.2305.04937},
	eprint = {2305.04937},
	eprintclass = {math.NA},
	eprinttype = {arXiv},
	abstract = {Statistical analysis of bipartite networks frequently requires
	            randomly sampling from the set of all bipartite networks with the
	            same degree sequence as an observed network. Trade algorithms
	            offer an efficient way to generate samples of bipartite networks
	            by incrementally `trading' the positions of some of their edges.
	            However, it is difficult to know how many such trades are
	            required to ensure that the sample is random. I propose a
	            stopping rule that focuses on the distance between sampled
	            networks and the observed network, and stops performing trades
	            when this distribution stabilizes. Analyses demonstrate that, for
	            over 300 different degree sequences, using this stopping rule
	            ensures a random sample with a high probability, and that it is
	            practical for use in empirical applications.},
	copyright = {Creative Commons Attribution 4.0 International},
	file = {:Randomlysamplingbipartite_Neal2023.pdf:PDF:http\:
	        //arxiv.org/pdf/2305.04937v3},
	keywords = {Numerical Analysis (math.NA), Methodology (stat.ME), FOS:
	            Mathematics, FOS: Computer and information sciences},
	publisher = {arXiv},
	year = {2023},
}

@article{fastballfastalgorithm_Godard2022,
	author = {Godard, Karl and Neal, Zachary P},
	date = {2022-10},
	journaltitle = {Journal of Complex Networks},
	title = {fastball: a fast algorithm to randomly sample bipartite graphs with
	         fixed degree sequences},
	doi = {10.1093/comnet/cnac049},
	issn = {2051-1329},
	number = {6},
	volume = {10},
	file = {:fastballfastalgorithm_Godard2022.pdf:PDF:https\:
	        //arxiv.org/pdf/2112.04017},
	publisher = {Oxford University Press (OUP)},
}

@inproceedings{Dynamictopicmodels_Blei2006,
	author = {Blei, David M. and Lafferty, John D.},
	booktitle = {Proceedings of the 23rd international conference on Machine
	             learning - ICML ’06},
	date = {2006},
	title = {Dynamic topic models},
	doi = {10.1145/1143844.1143859},
	pages = {113--120},
	publisher = {ACM Press},
	series = {ICML ’06},
	collection = {ICML ’06},
}

@article{METTLEMETamorphicTesting_Xie2020,
	author = {Xie, Xiaoyuan and Zhang, Zhiyi and Chen, Tsong Yueh and Liu, Yang
	          and Poon, Pak-Lok and Xu, Baowen},
	date = {2020-12},
	journaltitle = {IEEE Transactions on Reliability},
	title = {METTLE: A METamorphic Testing Approach to Assessing and Validating
	         Unsupervised Machine Learning Systems},
	doi = {10.1109/tr.2020.2972266},
	issn = {1558-1721},
	number = {4},
	pages = {1293--1322},
	volume = {69},
	file = {:METTLEMETamorphicTesting_Xie2020.pdf:PDF:http\:
	        //arxiv.org/pdf/1807.10453v4},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@article{Inversestatisticalproblems_Nguyen2017,
	author = {Nguyen, H. Chau and Zecchina, Riccardo and Berg, Johannes},
	date = {2017-06},
	journaltitle = {Advances in Physics},
	title = {Inverse statistical problems: from the inverse Ising problem to
	         data science},
	doi = {10.1080/00018732.2017.1341604},
	issn = {1460-6976},
	number = {3},
	pages = {197--261},
	volume = {66},
	file = {:Inversestatisticalproblems_Nguyen2017.pdf:PDF},
	publisher = {Informa UK Limited},
}

@article{IndianBuffetProcess_Griffiths2011,
	author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
	date = {2011-07},
	journaltitle = {J. Mach. Learn. Res.},
	title = {The Indian Buffet Process: An Introduction and Review},
	issn = {1532-4435},
	number = {null},
	pages = {1185--1224},
	volume = {12},
	abstract = {The Indian buffet process is a stochastic process defining a
	            probability distribution over equivalence classes of sparse
	            binary matrices with a finite number of rows and an unbounded
	            number of columns. This distribution is suitable for use as a
	            prior in probabilistic models that represent objects using a
	            potentially infinite array of features, or that involve bipartite
	            graphs in which the size of at least one class of nodes is
	            unknown. We give a detailed derivation of this distribution, and
	            illustrate its use as a prior in an infinite latent feature
	            model. We then review recent applications of the Indian buffet
	            process in machine learning, discuss its extensions, and
	            summarize its connections to other stochastic processes.},
	file = {:IndianBuffetProcess_Griffiths2011.pdf:PDF:https\:
	        //www.jmlr.org/papers/volume12/griffiths11a/griffiths11a.pdf},
	issue_date = {2/1/2011},
	numpages = {40},
	publisher = {JMLR.org},
}

@misc{generationcorrelatedartificial_Leisch1998,
	author = {Leisch, Friedrich and Weingessel, Andreas and Hornik, Kurt},
	date = {1998},
	title = {On the generation of correlated artificial binary data},
	doi = {10.57938/6884F809-93BC-4497-AB2B-FC1611198F5B},
	language = {en},
	file = {:generationcorrelatedartificial_Leisch1998.pdf:PDF},
	publisher = {Vienna University of Economics and Business},
}

@book{Humanlooptechnical_Fung2024,
	author = {Fung, Juan F and Li, Zongxia and Stephens, Daniel Kofi and Mao,
	          Andrew and Goel, Pranav and Walpole, Emily and Dima, Alden and
	          Boyd-Graber, Jordan Lee},
	date = {2024-05},
	title = {Human-in-the-loop technical document annotation :: developing and
	         validating a system to provide machine-assistance for
	         domain-specific text analysis},
	doi = {10.6028/nist.tn.2287},
	groups = {people},
	institution = {National Institute of Standards and Technology (U.S.)},
}

@incollection{AIInformedApproaches_Harper2022,
	author = {Harper, Charlie and Kumer, Anne and Stuart, Shelby and Meszaros,
	          Evan},
	booktitle = {The {{Rise}} of {{AI}}: {{Implications}} and {{Applications}}
	             of {{Artificial Intelligence}} in {{Academic Libraries}}},
	date = {2022},
	title = {{{AI-Informed Approaches}} to {{Metadata Tagging}} for {{Improved
	         Resource Discovery}}},
	isbn = {978-0-8389-3911-6},
	number = {78},
	publisher = {ACRL},
	series = {{{PIL}}},
	url = {
	       https://www.choice360.org/libtech-insight/using-ai-for-metadata-tagging-to-improve-resource-discovery/
	       },
	urldate = {2024-03-08},
	abstract = {Academic and cultural institutions are grappling with problems
	            of how to organize, label, and search disparate bodies of texts.
	            As aggregators, preservers, and disseminators of substantial
	            repositories of digital texts, research libraries are naturally
	            situated at the heart of these problems. This chapter explores
	            how unsupervised machine learning may be used to capture and
	            simplify the complexity and nuances of text. Traditional
	            approaches to improving discoverability and accessibility of text
	            through metadata and controlled vocabularies have time-tested
	            strengths. As the volume of digital data explodes, the obstacles
	            and limitations of traditional approaches become more pronounced,
	            and machine learning “show(s) the potential to create
	            efficiencies that smooth the path to access, enhancing
	            description and expanding forms of discovery along the way.” 1 In
	            light of the need for new approaches to metadata generation to
	            facilitate discovery, the authors look at Doc2Vec and topic
	            modelling with Latent Dirichlet Allocation (LDA) to explore their
	            utility as assistive tools for authors, librarians, and readers.
	            The authors apply the two approaches to a corpus of electronic
	            theses and dissertations (ETDs) completed at Ohio universities
	            and colleges.},
	file = {
	        /home/rtbs/Zotero/storage/QP3CNR38/using-ai-for-metadata-tagging-to-improve-resource-discovery.html
	        },
	keywords = {HCII/abstract},
	langid = {american},
}

@article{Semanticverbalfluency_AriasTrejo2021,
	author = {Arias-Trejo, Natalia and Luna-Umanzor, Diana I. and Angulo-Chavira
	          , Armando and Ríos-Ponce, Alma E. and González-González, Martha M.
	          and Ramírez-Díaz, Jorge F. and Sánchez-Reyes, Minerva and
	          Marín-García, Gabriel and Arias-Carrión, Oscar},
	date = {2021-06},
	journaltitle = {Journal of Cognitive Psychology},
	title = {Semantic verbal fluency: network analysis in Alzheimer’s and
	         Parkinson’s disease},
	doi = {10.1080/20445911.2021.1943414},
	issn = {2044-592X},
	number = {5},
	pages = {557--567},
	volume = {33},
	publisher = {Informa UK Limited},
}

@article{Latentdirichletallocation_Blei2003,
	author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
	date = {2003},
	journaltitle = {Journal of machine Learning research},
	title = {Latent dirichlet allocation},
	doi = {10.7551/mitpress/1120.003.0082},
	number = {Jan},
	pages = {993--1022},
	volume = {3},
	groups = {graphical-models},
	publisher = {The MIT Press},
}

@book{Patternrecognition_Theodoridis2010,
	date = {2010},
	title = {Pattern recognition},
	edition = {4th ed},
	editor = {Sergios Theodoridis and Konstantinos Koutroumbas},
	isbn = {9780080949123},
	location = {Burlington, MA},
	note = {Previous ed.: San Diego, CA : Academic Press, c2006. - Includes
	        bibliographical references and index. - Description based on print
	        version record},
	pagetotal = {961},
	publisher = {Academic Press},
	groups = {graphical-models},
	ppn_gvk = {1651543941},
}

@phdthesis{Automaticmodelconstruction_Duvenaud2014,
	author = {Duvenaud, David},
	date = {2014},
	title = {Automatic model construction with Gaussian processes},
	groups = {graphical-models},
}

@article{Metricspacespositive_Schoenberg1938,
	author = {Schoenberg, I. J.},
	date = {1938},
	journaltitle = {Transactions of the American Mathematical Society},
	title = {Metric spaces and positive definite functions},
	doi = {10.1090/s0002-9947-1938-1501980-0},
	issn = {1088-6850},
	number = {3},
	pages = {522--536},
	volume = {44},
	publisher = {American Mathematical Society (AMS)},
}

@inbook{Topicsadvancedeconometrics_Bierens1996,
	author = {Bierens, Herman J.},
	booktitle = {Topics in advanced econometrics},
	date = {1996},
	title = {The Nadaraya–Watson kernel regression function estimator},
	edition = {1. paperback ed.},
	isbn = {0521565111},
	location = {New York},
	note = {Includes bibliographical references and index},
	pages = {212--247},
	publisher = {Cambridge Univ. Press},
	subtitle = {Estimation, testing, and specification of cross-section and time
	            series models},
	pagetotal = {258},
	ppn_gvk = {191657816},
}

@article{NonparametricEstimationIncomplete_Kaplan1958,
	author = {Kaplan, E. L. and Meier, Paul},
	date = {1958-06},
	journaltitle = {Journal of the American Statistical Association},
	title = {Nonparametric Estimation from Incomplete Observations},
	doi = {10.1080/01621459.1958.10501452},
	issn = {1537-274X},
	number = {282},
	pages = {457--481},
	volume = {53},
	publisher = {Informa UK Limited},
}

@article{ComplexnetworksStructure_BOCCALETTI2006,
	author = {BOCCALETTI, S and LATORA, V and MORENO, Y and CHAVEZ, M and HWANG,
	          D},
	date = {2006-02},
	journaltitle = {Physics Reports},
	title = {Complex networks: Structure and dynamics},
	doi = {10.1016/j.physrep.2005.10.009},
	issn = {0370-1573},
	number = {4–5},
	pages = {175--308},
	volume = {424},
	groups = {graphs},
	publisher = {Elsevier BV},
}

@inbook{MediaMarginsPopular_Savigny2015,
	author = {Raalte, Christa van},
	booktitle = {Media, Margins and Popular Culture},
	date = {2015},
	title = {1. No Small-Talk in Paradise: Why Elysium Fails the Bechdel Test,
	         and Why We Should Care},
	edition = {1st},
	editor = {Heather Savigny and Einar Thorsen and Daniel Jackson and Jennifer
	          Alexander},
	isbn = {9781137512819},
	location = {London},
	publisher = {Imprint: Palgrave Macmillan},
	pagetotal = {1264},
	ppn_gvk = {1837678626},
}

@techreport{PageRankCitationRanking_Page1999,
	author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry
	          Winograd},
	date = {1999-11},
	institution = {Stanford InfoLab},
	title = {The PageRank Citation Ranking: Bringing Order to the Web.},
	note = {Previous number = SIDL-WP-1999-0120},
	number = {1999-66},
	type = {Technical Report},
	url = {http://ilpubs.stanford.edu:8090/422/},
	abstract = {The importance of a Web page is an inherently subjective matter,
	            which depends on the readers interests, knowledge and attitudes.
	            But there is still much that can be said objectively about the
	            relative importance of Web pages. This paper describes PageRank,
	            a mathod for rating Web pages objectively and mechanically,
	            effectively measuring the human interest and attention devoted to
	            them. We compare PageRank to an idealized random Web surfer. We
	            show how to efficiently compute PageRank for large numbers of
	            pages. And, we show how to apply PageRank to search and to user
	            navigation.},
	publisher = {Stanford InfoLab},
}

@inbook{ApproximatingPageRankDegree_Fortunato,
	author = {Fortunato, Santo and Boguñá, Marián and Flammini, Alessandro and
	          Menczer, Filippo},
	booktitle = {Algorithms and Models for the Web-Graph},
	title = {Approximating PageRank from In-Degree},
	doi = {10.1007/978-3-540-78808-9_6},
	isbn = {9783540788089},
	pages = {59--71},
	publisher = {Springer Berlin Heidelberg},
	groups = {graphs},
	issn = {1611-3349},
}

@article{effectivenessrandomwalks_Kim2023,
	author = {Kim, Sooyeong and Breen, Jane and Dudkina, Ekaterina and Poloni,
	          Federico and Crisostomi, Emanuele},
	date = {2023-01},
	journaltitle = {PLOS ONE},
	title = {On the effectiveness of random walks for modeling epidemics on
	         networks},
	doi = {10.1371/journal.pone.0280277},
	editor = {Rodriguez, Pablo Martin},
	issn = {1932-6203},
	number = {1},
	pages = {e0280277},
	volume = {18},
	publisher = {Public Library of Science (PLoS)},
}

@book{Mathematicsnetworks_Newman2018,
	author = {Newman, Mark},
	date = {2018-10},
	title = {Mathematics of networks},
	doi = {10.1093/oso/9780198805090.003.0006},
	publisher = {Oxford University Press},
	journaltitle = {Oxford Scholarship Online},
}

@article{LittleBallFur_Rozemberczki2020,
	author = {Rozemberczki, Benedek and Kiss, Oliver and Sarkar, Rik},
	date = {2020},
	title = {Little Ball of Fur: A Python Library for Graph Sampling},
	doi = {10.48550/ARXIV.2006.04311},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Social and Information Networks (cs.SI), Machine Learning
	            (cs.LG), FOS: Computer and information sciences, FOS: Computer
	            and information sciences},
	publisher = {arXiv},
}

@inproceedings{ExploringNetworkStructure_Hagberg2008,
	author = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
	booktitle = {Proceedings of the 7th Python in Science Conference},
	date = {2008},
	title = {Exploring Network Structure, Dynamics, and Function using NetworkX},
	editor = {Ga\"el Varoquaux and Travis Vaught and Jarrod Millman},
	location = {Pasadena, CA USA},
	pages = {11 - 15},
}

@article{EmergenceScalingRandom_Barabasi1999,
	author = {Barabási, Albert-László and Albert, Réka},
	date = {1999-10},
	journaltitle = {Science},
	title = {Emergence of Scaling in Random Networks},
	doi = {10.1126/science.286.5439.509},
	issn = {1095-9203},
	number = {5439},
	pages = {509--512},
	volume = {286},
	groups = {graphical-models},
	publisher = {American Association for the Advancement of Science (AAAS)},
}

@book{Datastructuresnetwork_Tarjan1983,
	author = {Tarjan, Robert Endre},
	date = {1983},
	title = {Data structures and network algorithms},
	chapter = {6. Minimum spanning trees 6.2. Three classical algorithms},
	pages = {72-77},
	publisher = {Society for Industrial and Applied Mathematics},
	series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
	volume = {44},
}

@article{RandomPlaneNetworks_Gilbert1961,
	author = {Gilbert, E. N.},
	date = {1961-12},
	journaltitle = {Journal of the Society for Industrial and Applied
	                Mathematics},
	title = {Random Plane Networks},
	doi = {10.1137/0109045},
	issn = {2168-3484},
	number = {4},
	pages = {533--543},
	volume = {9},
	groups = {graphical-models},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
}

@article{StochasticblockmodelsFirst_Holland1983,
	author = {Holland, Paul W. and Laskey, Kathryn Blackmond and Leinhardt,
	          Samuel},
	date = {1983-06},
	journaltitle = {Social Networks},
	title = {Stochastic blockmodels: First steps},
	doi = {10.1016/0378-8733(83)90021-7},
	issn = {0378-8733},
	number = {2},
	pages = {109--137},
	volume = {5},
	groups = {graphs, graphical-models},
	publisher = {Elsevier BV},
}

@article{toolfilteringinformation_Tumminello2005,
	author = {Tumminello, Michele and Aste, Tomaso and Di Matteo, Tiziana and
	          Mantegna, Rosario N},
	date = {2005},
	journaltitle = {Proceedings of the National Academy of Sciences},
	title = {A tool for filtering information in complex systems},
	number = {30},
	pages = {10421--10426},
	volume = {102},
	groups = {graphs},
	publisher = {National Academy of Sciences},
}

@article{Relativeneighborhoodgraphs_Jaromczyk1992,
	author = {Jaromczyk, J.W. and Toussaint, G.T.},
	date = {1992},
	journaltitle = {Proceedings of the IEEE},
	title = {Relative neighborhood graphs and their relatives},
	doi = {10.1109/5.163414},
	issn = {0018-9219},
	number = {9},
	pages = {1502--1517},
	volume = {80},
	groups = {graphs},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@book{SpeechLanguageProcessing_Jurafsky2025,
	author = {Daniel Jurafsky and James H. Martin},
	date = {2025},
	title = {Speech and Language Processing: An Introduction to Natural Language
	         Processing, Computational Linguistics, and Speech Recognition with
	         Language Models},
	edition = {3rd},
	note = {Online manuscript released January 12, 2025},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
}

@misc{Accuracytruenessprecision_ISO1994,
	author = {ISO5725},
	date = {1994},
	title = {Accuracy (trueness and precision) of measurement methods and
	         results},
	publisher = {International Organization for Standardization Geneva},
}

@inproceedings{Methodoptimaldirections_Engan1999,
	author = {Engan, K. and Aase, S.O. and Hakon Husoy, J.},
	booktitle = {1999 IEEE International Conference on Acoustics, Speech, and
	             Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)},
	date = {1999},
	title = {Method of optimal directions for frame design},
	doi = {10.1109/icassp.1999.760624},
	publisher = {IEEE},
	file = {:Methodoptimaldirections_Engan1999.pdf:PDF},
}

@article{codingmanualqualitative_Saldana2021,
	author = {Salda{\~n}a, Johnny},
	date = {2021},
	title = {The coding manual for qualitative researchers},
	file = {:codingmanualqualitative_Saldana2021.pdf:PDF},
	groups = {people},
	publisher = {SAGE publications Ltd},
}

@inproceedings{SemiDiscreteNormalizing_Chen2022,
	author = {Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
	booktitle = {Advances in Neural Information Processing Systems},
	date = {2022},
	title = {Semi-Discrete Normalizing Flows through Differentiable Tessellation
	         },
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho
	          and A. Oh},
	pages = {14878--14889},
	publisher = {Curran Associates, Inc.},
	url = {
	       https://proceedings.neurips.cc/paper_files/paper/2022/file/5f61939af1699c82dab00ed36c887968-Paper-Conference.pdf
	       },
	volume = {35},
	file = {:SemiDiscreteNormalizing_Chen2022.pdf:PDF:https\:
	        //proceedings.neurips.cc/paper_files/paper/2022/file/5f61939af1699c82dab00ed36c887968-Paper-Conference.pdf
	        },
	groups = {graphical-models},
	priority = {prio1},
}

@article{semanticorganizationanimal_Goni2011,
	author = {Go{\~n}i, Joaqu{\'\i}n and Arrondo, Gonzalo and Sepulcre, Jorge
	          and Martincorena, Inigo and de Mendiz{\'a}bal, Nieves V{\'e}lez and
	          Corominas-Murtra, Bernat and Bejarano, Bartolom{\'e} and
	          Ardanza-Trevijano, Sergio and Peraita, Herminia and Wall, Dennis P
	          and others},
	date = {2011},
	journaltitle = {Cognitive processing},
	title = {The semantic organization of the animal category: evidence from
	         semantic verbal fluency and network theory},
	number = {2},
	pages = {183--196},
	volume = {12},
	groups = {text-as-data, graphs, people},
	publisher = {Springer},
}

@article{Efficientestimationword_Mikolov2013,
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	date = {2013},
	journaltitle = {arXiv preprint arXiv:1301.3781},
	title = {Efficient estimation of word representations in vector space},
	groups = {text-as-data},
}

@inproceedings{GloveGlobalvectors_Pennington2014,
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	booktitle = {Proceedings of the 2014 conference on empirical methods in
	             natural language processing (EMNLP)},
	date = {2014},
	title = {Glove: Global vectors for word representation},
	pages = {1532--1543},
	groups = {text-as-data},
}

@article{newdissimilaritymeasure_Prescott2006,
	author = {Prescott, Tony J. and Newton, Lisa D. and Mir, Nusrat U. and
	          Woodruff, Peter W. R. and Parks, Randolph W.},
	date = {2006-11},
	journaltitle = {Neuropsychology},
	title = {A new dissimilarity measure for finding semantic structure in
	         category fluency data with implications for understanding memory
	         organization in schizophrenia.},
	doi = {10.1037/0894-4105.20.6.685},
	issn = {0894-4105},
	number = {6},
	pages = {685--699},
	volume = {20},
	file = {:newdissimilaritymeasure_Prescott2006.pdf:PDF:https\:
	        //eprints.whiterose.ac.uk/id/eprint/107038/7/Prescott%20Neuropsychology%202006%20Preprint.pdf
	        },
	groups = {people, graphical-models},
	publisher = {American Psychological Association (APA)},
}

@article{algorithmguaranteedconvergence_Brent1971,
	author = {Brent, Richard P.},
	date = {1971},
	journaltitle = {The Computer Journal},
	title = {An algorithm with guaranteed convergence for finding a zero of a
	         function},
	number = {4},
	pages = {422--425},
	volume = {14},
	publisher = {Oxford University Press},
}

@article{Pomegranatefastflexible_Schreiber2018,
	author = {Schreiber, Jacob},
	date = {2018},
	journaltitle = {Journal of Machine Learning Research},
	title = {Pomegranate: fast and flexible probabilistic modeling in python},
	number = {164},
	pages = {1--6},
	volume = {18},
}

@inproceedings{AutomaticdifferentiationPyTorch_Paszke2017,
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan,
	          Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and
	          Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	booktitle = {NIPS-W},
	date = {2017},
	title = {Automatic differentiation in PyTorch},
}

@article{Observationphasetransitions_Shrager1987,
	author = {Shrager, Jeff and Hogg, Tad and Huberman, Bernardo A},
	date = {1987},
	journaltitle = {Science},
	title = {Observation of phase transitions in spreading activation networks},
	number = {4805},
	pages = {1092--1094},
	volume = {236},
	groups = {people},
	publisher = {American Association for the Advancement of Science},
}

@book{architecturecognition_Anderson2013,
	author = {Anderson, John R},
	date = {2013},
	title = {The architecture of cognition},
	publisher = {Psychology Press},
	groups = {people},
}

@inproceedings{Learninglocalglobal_Zhou2004,
	author = {Zhou, Dengyong and Bousquet, Olivier and Lal, Thomas N and Weston,
	          Jason and Sch{\"o}lkopf, Bernhard},
	booktitle = {Advances in neural information processing systems},
	date = {2004},
	title = {Learning with local and global consistency},
	pages = {321--328},
}

@phdthesis{sharedreliabilitydatabase_Ho2015,
	author = {Ho, Mark},
	date = {2015},
	institution = {University of Western Australia},
	title = {A shared reliability database for mobile mining equipment},
	groups = {text-as-data},
}

@article{Cleaninghistoricalmaintenance_Hodkiewicz2016,
	author = {Hodkiewicz, Melinda and Ho, Mark Tien-Wei},
	date = {2016},
	journaltitle = {Journal of Quality in Maintenance Engineering},
	title = {Cleaning historical maintenance work order data for reliability
	         analysis},
	number = {2},
	pages = {146--163},
	volume = {22},
	groups = {text-as-data},
	publisher = {Emerald Group Publishing Limited},
}

@inproceedings{Whyautonomousassets_Hodkiewicz2017,
	author = {Hodkiewicz, Melinda R and Batsioudis, Zac and Radomiljac, Tyler
	          and Ho, Mark TW},
	booktitle = {Annual Conference of the Prognostics and Health Management
	             Society 2017},
	date = {2017},
	title = {Why autonomous assets are good for reliability--the impact of
	         ‘operator-related component’failures on heavy mobile equipment
	         reliability},
	groups = {text-as-data},
}

@misc{RandomWalksElectric_Doyle2000,
	author = {Doyle, Peter G. and Snell, J. Laurie},
	date = {2000},
	title = {Random Walks and Electric Networks},
	doi = {10.48550/ARXIV.MATH/0001057},
	copyright = {Assumed arXiv.org perpetual, non-exclusive license to
	             distribute this article for submissions made before January 2004
	             },
	keywords = {Probability (math.PR), FOS: Mathematics, FOS: Mathematics},
	publisher = {arXiv},
}

@article{Dynamicsearchworking_Hills2012,
	author = {Hills, Thomas T. and Pachur, Thorsten},
	date = {2012},
	journaltitle = {Journal of Experimental Psychology: Learning, Memory, and
	                Cognition},
	title = {Dynamic search and working memory in social recall.},
	doi = {10.1037/a0025161},
	issn = {0278-7393},
	number = {1},
	pages = {218--228},
	volume = {38},
	groups = {people},
	publisher = {American Psychological Association (APA)},
}

@article{ForagingSemanticFields_Hills2015,
	author = {Hills, Thomas T. and Todd, Peter M. and Jones, Michael N.},
	date = {2015-06},
	journaltitle = {Topics in Cognitive Science},
	title = {Foraging in Semantic Fields: How We Search Through Memory},
	doi = {10.1111/tops.12151},
	issn = {1756-8765},
	number = {3},
	pages = {513--534},
	volume = {7},
	groups = {people},
	publisher = {Wiley},
}
@article{magicalnumberseven_Miller1956,
	author = {Miller, George A.},
	date = {1956-03},
	journaltitle = {Psychological Review},
	title = {The magical number seven, plus or minus two: Some limits on our
	         capacity for processing information.},
	doi = {10.1037/h0043158},
	issn = {0033-295X},
	number = {2},
	pages = {81--97},
	volume = {63},
	groups = {people},
	publisher = {American Psychological Association (APA)},
}
@article{genderagencygap_Stuhler2024,
	author = {Stuhler, Oscar},
	date = {2024-07},
	journaltitle = {Proceedings of the National Academy of Sciences},
	title = {The gender agency gap in fiction writing (1850 to 2010)},
	doi = {10.1073/pnas.2319514121},
	issn = {1091-6490},
	number = {29},
	volume = {121},
	file = {:genderagencygap_Stuhler2024.pdf:PDF},
	groups = {people},
	publisher = {Proceedings of the National Academy of Sciences},
}

@book{StanfordGraphBaseplatform_Knuth1993,
	author = {Knuth, Donald Ervin},
	date = {1993},
	title = {The Stanford GraphBase: a platform for combinatorial computing},
	publisher = {AcM Press New York},
	volume = {1},
	groups = {graphs},
}

@article{AssortativeMixingNetworks_Newman2002,
	author = {Newman, M. E. J.},
	date = {2002-10},
	journaltitle = {Physical Review Letters},
	title = {Assortative Mixing in Networks},
	doi = {10.1103/physrevlett.89.208701},
	issn = {1079-7114},
	number = {20},
	pages = {208701},
	volume = {89},
	publisher = {American Physical Society (APS)},
}

@article{Findingcommunitystructure_Clauset2004,
	author = {Clauset, Aaron and Newman, M. E. J. and Moore, Cristopher},
	date = {2004-12},
	journaltitle = {Physical Review E},
	title = {Finding community structure in very large networks},
	doi = {10.1103/physreve.70.066111},
	issn = {1550-2376},
	number = {6},
	pages = {066111},
	volume = {70},
	file = {:Findingcommunitystructure_Clauset2004.pdf:PDF:http\:
	        //arxiv.org/pdf/cond-mat/0408187v2},
	publisher = {American Physical Society (APS)},
}

@article{Coherentnoisescale_Sneppen1997,
	author = {Sneppen, Kim and Newman, M.E.J.},
	date = {1997-12},
	journaltitle = {Physica D: Nonlinear Phenomena},
	title = {Coherent noise, scale invariance and intermittency in large systems
	         },
	doi = {10.1016/s0167-2789(97)00128-0},
	issn = {0167-2789},
	number = {3–4},
	pages = {209--222},
	volume = {110},
	publisher = {Elsevier BV},
}

@article{StructureFunctionComplex_Newman2003,
	author = {Newman, M. E. J.},
	date = {2003-01},
	journaltitle = {SIAM Review},
	title = {The Structure and Function of Complex Networks},
	doi = {10.1137/s003614450342480},
	issn = {1095-7200},
	number = {2},
	pages = {167--256},
	volume = {45},
	file = {:StructureFunctionComplex_Newman2003.pdf:PDF:https\:
	        //arxiv.org/pdf/cond-mat/0303516},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
}

@article{Findingevaluatingcommunity_Newman2004,
	author = {Newman, M. E. J. and Girvan, M.},
	date = {2004-02},
	journaltitle = {Physical Review E},
	title = {Finding and evaluating community structure in networks},
	doi = {10.1103/physreve.69.026113},
	issn = {1550-2376},
	number = {2},
	pages = {026113},
	volume = {69},
	file = {:Findingevaluatingcommunity_Newman2004.pdf:PDF},
	publisher = {American Physical Society (APS)},
}

@inproceedings{HybridApproachHierarchical_Malzer2020,
	author = {Malzer, Claudia and Baum, Marcus},
	booktitle = {2020 IEEE International Conference on Multisensor Fusion and
	             Integration for Intelligent Systems (MFI)},
	date = {2020-09},
	title = {A Hybrid Approach To Hierarchical Density-based Cluster Selection},
	doi = {10.1109/mfi49285.2020.9235263},
	pages = {223--228},
	publisher = {IEEE},
}

@article{MethodsMeasuringAssociation_Yule1912,
	author = {Yule, G. Udny},
	date = {1912-05},
	journaltitle = {Journal of the Royal Statistical Society},
	title = {On the Methods of Measuring Association Between Two Attributes},
	doi = {10.2307/2340126},
	issn = {0952-8385},
	number = {6},
	pages = {579},
	volume = {75},
	publisher = {JSTOR},
}

@article{MCCapproachesgeometric_Crall2023,
	author = {Crall, Jon},
	date = {2023-04-30},
	title = {The MCC approaches the geometric mean of precision and recall as
	         true negatives approach infinity},
	doi = {10.48550/ARXIV.2305.00594},
	eprint = {2305.00594},
	eprintclass = {cs.CV},
	eprinttype = {arXiv},
	abstract = {The performance of a binary classifier is described by a
	            confusion matrix with four entries: the number of true positives
	            (TP), true negatives (TN), false positives (FP), and false
	            negatives (FN). The Matthew's Correlation Coefficient (MCC), F1,
	            and Fowlkes--Mallows (FM) scores are scalars that summarize a
	            confusion matrix. Both the F1 and FM scores are based on only
	            three of the four entries in the confusion matrix (they ignore
	            TN). In contrast, the MCC takes into account all four entries of
	            the confusion matrix and thus can be seen as providing a more
	            representative picture. However, in object detection problems,
	            measuring the number of true negatives is so large it is often
	            intractable. Thus we ask, what happens to the MCC as the number
	            of true negatives approaches infinity? This paper provides
	            insight into the relationship between the MCC and FM score by
	            proving that the FM-measure is equal to the limit of the MCC as
	            the number of true negatives approaches infinity.},
	copyright = {Creative Commons Attribution 4.0 International},
	file = {:MCCapproachesgeometric_Crall2023.pdf:PDF:http\:
	        //arxiv.org/pdf/2305.00594v2},
	keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer
	            and information sciences},
	publisher = {arXiv},
	year = {2023},
}

@article{Robustclassificationsalient_Grady2012,
	author = {Grady, Daniel and Thiemann, Christian and Brockmann, Dirk},
	date = {2012-05},
	journaltitle = {Nature Communications},
	title = {Robust classification of salient links in complex networks},
	doi = {10.1038/ncomms1847},
	issn = {2041-1723},
	number = {1},
	volume = {3},
	file = {:Robustclassificationsalient_Grady2012.pdf:PDF:https\:
	        //www.nature.com/articles/ncomms1847.pdf},
	publisher = {Springer Science and Business Media LLC},
}

@inproceedings{Sinkhorndistanceslightspeed_Cuturi2013,
	author = {Cuturi, Marco},
	booktitle = {Proceedings of the 27th International Conference on Neural
	             Information Processing Systems - Volume 2},
	date = {2013},
	title = {Sinkhorn distances: lightspeed computation of optimal transport},
	location = {Lake Tahoe, Nevada},
	pages = {2292--2300},
	publisher = {Curran Associates Inc.},
	series = {NIPS'13},
	abstract = {Optimal transport distances are a fundamental family of
	            distances for probability measures and histograms of features.
	            Despite their appealing theoretical properties, excellent
	            performance in retrieval tasks and intuitive formulation, their
	            computation involves the resolution of a linear program whose
	            cost can quickly become prohibitive whenever the size of the
	            support of these measures or the histograms' dimension exceeds a
	            few hundred. We propose in this work a new family of optimal
	            transport distances that look at transport problems from a
	            maximum-entropy perspective. We smooth the classic optimal
	            transport problem with an entropic regularization term, and show
	            that the resulting optimum is also a distance which can be
	            computed through Sinkhorn's matrix scaling algorithm at a speed
	            that is several orders of magnitude faster than that of transport
	            solvers. We also show that this regularized distance improves
	            upon classic optimal transport distances on the MNIST
	            classification problem.},
	address = {Red Hook, NY, USA},
	file = {:Sinkhorndistanceslightspeed_Cuturi2013.pdf:PDF:https\:
	        //papers.nips.cc/paper_files/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf
	        },
	numpages = {9},
}

@article{Measuresecologicalassociation_Janson1981,
	author = {Janson, Svante and Vegelius, Jan},
	date = {1981-07},
	journaltitle = {Oecologia},
	title = {Measures of ecological association},
	doi = {10.1007/bf00347601},
	issn = {1432-1939},
	number = {3},
	pages = {371--376},
	volume = {49},
	groups = {graphs},
	publisher = {Springer Science and Business Media LLC},
}

@article{fastalgorithmSteiner_Kou1981,
	author = {Kou, L. and Markowsky, G. and Berman, L.},
	date = {1981},
	journaltitle = {Acta Informatica},
	title = {A fast algorithm for Steiner trees},
	doi = {10.1007/bf00288961},
	issn = {1432-0525},
	number = {2},
	pages = {141--145},
	volume = {15},
	groups = {graphs},
	publisher = {Springer Science and Business Media LLC},
}

@article{upperboundprobability_Hunter1976,
	author = {Hunter, David},
	date = {1976-09},
	journaltitle = {Journal of Applied Probability},
	title = {An upper bound for the probability of a union},
	doi = {10.2307/3212481},
	issn = {1475-6072},
	number = {3},
	pages = {597--603},
	volume = {13},
	publisher = {Cambridge University Press (CUP)},
}

@article{EmpiricalBayesianStrategy_Wipf2007,
	author = {Wipf, David P. and Rao, Bhaskar D.},
	date = {2007-07},
	journaltitle = {IEEE Transactions on Signal Processing},
	title = {An Empirical Bayesian Strategy for Solving the Simultaneous Sparse
	         Approximation Problem},
	doi = {10.1109/tsp.2007.894265},
	issn = {1053-587X},
	number = {7},
	pages = {3704--3716},
	volume = {55},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@article{Sparsedataembedding_Omanovic2021,
	author = {Omanović, Amra and Kazan, Hilal and Oblak, Polona and Curk, Tomaž},
	date = {2021-02},
	journaltitle = {BMC Bioinformatics},
	title = {Sparse data embedding and prediction by tropical matrix
	         factorization},
	doi = {10.1186/s12859-021-04023-9},
	issn = {1471-2105},
	number = {1},
	volume = {22},
	publisher = {Springer Science and Business Media LLC},
}

@article{tropicalGrassmannian_2004,
	date = {2004-07},
	journaltitle = {advg},
	title = {The tropical Grassmannian},
	doi = {10.1515/advg.2004.023},
	issn = {1615-715X},
	number = {3},
	pages = {389--411},
	volume = {4},
	publisher = {Walter de Gruyter GmbH},
}

@article{SparseApproximateSolutions_Natarajan1995,
	author = {Natarajan, B. K.},
	date = {1995-04},
	journaltitle = {SIAM Journal on Computing},
	title = {Sparse Approximate Solutions to Linear Systems},
	doi = {10.1137/s0097539792240406},
	issn = {1095-7111},
	number = {2},
	pages = {227--234},
	volume = {24},
	file = {:SparseApproximateSolutions_Natarajan1995.pdf:PDF},
	publisher = {Society for Industrial & Applied Mathematics (SIAM)},
}

@article{Matchingpursuitstime_Mallat1993,
	author = {Mallat, S.G. and Zhifeng Zhang},
	date = {1993},
	journaltitle = {IEEE Transactions on Signal Processing},
	title = {Matching pursuits with time-frequency dictionaries},
	doi = {10.1109/78.258082},
	issn = {1053-587X},
	number = {12},
	pages = {3397--3415},
	volume = {41},
	file = {:Matchingpursuitstime_Mallat1993.pdf:PDF},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@article{EfficientimplementationK_Rubinstein2008,
	author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
	date = {2008},
	journaltitle = {Cs Technion},
	title = {Efficient implementation of the K-SVD algorithm using batch
	         orthogonal matching pursuit},
	number = {8},
	pages = {1--15},
	volume = {40},
	file = {:EfficientimplementationK_Rubinstein2008.pdf:PDF:https\:
	        //csaws.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf},
}

@inproceedings{Generatingrandomspanning_Wilson1996,
	author = {Wilson, David Bruce},
	booktitle = {Proceedings of the twenty-eighth annual ACM symposium on Theory
	             of computing - STOC ’96},
	date = {1996},
	title = {Generating random spanning trees more quickly than the cover time},
	doi = {10.1145/237814.237880},
	pages = {296--303},
	publisher = {ACM Press},
	series = {STOC ’96},
	collection = {STOC ’96},
	file = {:Generatingrandomspanning_Wilson1996.pdf:PDF},
	groups = {graphs},
}

@article{WhatisArcsine_Ackelsberg2018,
	author = {Ackelsberg, Ethan},
	date = {2018},
	title = {What is the Arcsine Law?},
	url = {
	       https://math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf
	       },
	file = {:WhatisArcsine_Ackelsberg2018.pdf:PDF:https\:
	        //math.osu.edu/sites/math.osu.edu/files/What_is_2018_Arcsine_Law.pdf},
	groups = {I've Cited},
}
@book{IntroductionProbabilityTheory_Feller1968,
	author = {William Feller},
	date = {1968},
	title = {An Introduction to Probability Theory and its Applications, Volume
	         1},
	publisher = {J. Wiley \& Sons: New York},
}

@article{Incidencegeometry_Moorhouse2007,
	author = {Moorhouse, G Eric},
	title = {Incidence geometry},
	pages = {3--5},
	date = {2007},
	file = {:Incidencegeometry_Moorhouse2007.pdf:PDF:https\:
	        //ericmoorhouse.org/handouts/Incidence_Geometry.pdf},
	journaltitle = {University of Wyoming},
}

@misc{GradientEstimationStochastic_Paulus2020,
	author = {Paulus, Max B. and Choi, Dami and Tarlow, Daniel and Krause,
	          Andreas and Maddison, Chris J.},
	title = {Gradient Estimation with Stochastic Softmax Tricks},
	copyright = {arXiv.org perpetual, non-exclusive license},
	date = {2020},
	doi = {10.48550/ARXIV.2006.08063},
	file = {:GradientEstimationStochastic_Paulus2020.pdf:PDF:http\:
	        //arxiv.org/pdf/2006.08063},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS:
	            Computer and information sciences, FOS: Computer and information
	            sciences},
	publisher = {arXiv},
}

@article{Techniquesanalyzingvertebrate_Whitehead1999,
	author = {Whitehead, Hal and Dufault, Susan},
	date = {1999},
	journaltitle = {Advances in the Study of Behavior},
	title = {Techniques for analyzing vertebrate social structure using
	         identified individuals: review and recommendations},
	number = {28},
	pages = {33--74},
	volume = {28},
	file = {:Techniquesanalyzingvertebrate_Whitehead1999.pdf:PDF:http\:
	        //whitelab.biology.dal.ca/hw/Whitehead_and_Dufault_1999.pdf},
}
@inbook{PerceivedAssortativitySocial_Fisher2017,
	author = {Fisher, David N. and Silk, Matthew J. and Franks, Daniel W.},
	pages = {1--19},
	publisher = {Springer International Publishing},
	title = {The Perceived Assortativity of Social Networks: Methodological
	         Problems and Solutions},
	isbn = {9783319534206},
	booktitle = {Trends in Social Network Analysis},
	date = {2017},
	doi = {10.1007/978-3-319-53420-6_1},
	file = {:PerceivedAssortativitySocial_Fisher2017.pdf:PDF:https\:
	        //arxiv.org/pdf/1701.08671},
	groups = {graphs, I've Cited},
	issn = {2190-5436},
	priority = {prio1},
}

@article{MultivariateBernoullidistribution_Dai2013,
	author = {Dai, Bin and Ding, Shilin and Wahba, Grace},
	title = {Multivariate Bernoulli distribution},
	issn = {1350-7265},
	number = {4},
	volume = {19},
	date = {2013-09},
	doi = {10.3150/12-bejsp10},
	file = {:MultivariateBernoullidistribution_Dai2013.pdf:PDF:https\:
	        //projecteuclid.org/journals/bernoulli/volume-19/issue-4/Multivariate-Bernoulli-distribution/10.3150/12-BEJSP10.pdf
	        },
	groups = {graphical-models},
	journaltitle = {Bernoulli},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
}

@report{Countingrootedforests_Knill2013,
	author = {Knill, Oliver},
	date = {2013-07-18},
	institution = {{arXiv}},
	title = {Counting rooted forests in a network},
	doi = {10.48550/arXiv.1307.3810},
	eprint = {1307.3810},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000014 type: article},
	number = {{arXiv}:1307.3810},
	abstract = {We use a recently found generalization of the Cauchy-Binet
	            theorem to give a new proof of the Chebotarev-Shamis forest
	            theorem telling that det(1+L) is the number of rooted spanning
	            forests in a finite simple graph G with Laplacian L. More
	            generally, we show that det(1+k L) is the number of rooted
	            edge-k-colored spanning forests in G. If a forest with an even
	            number of edges is called even, then det(1-L) is the difference
	            between even and odd rooted spanning forests in G.},
	file = {:Countingrootedforests_Knill2013 - Counting Rooted Forests in a
	        Network.pdf:PDF},
	keywords = {Mathematics - Spectral Theory, Computer Science - Discrete
	            Mathematics, Computer Science - Social and Information Networks,
	            Mathematical Physics, 05C50 05C30 05C05 91D30},
}

@article{MachineLearningas_BarberisCanonico2018,
	author = {Barberis Canonico, Lorenzo and {McNeese}, Nathan J. and Duncan,
	          Chris},
	date = {2018-09-01},
	journaltitle = {Proceedings of the Human Factors and Ergonomics Society
	                Annual Meeting},
	title = {Machine Learning as Grounded Theory: Human-Centered Interfaces for
	         Social Network Research through Artificial Intelligence},
	doi = {10.1177/1541931218621287},
	issn = {2169-5067},
	number = {1},
	pages = {1252--1256},
	urldate = {2022-02-28},
	volume = {62},
	abstract = {Internet technologies have created unprecedented opportunities
	            for people to come together and through their collective effort
	            generate large amounts of data about human behavior. With the
	            increased popularity of grounded theory, many researchers have
	            sought to use ever-increasingly large datasets to analyze and
	            draw patterns about social dynamics. However, the data is simply
	            too big to enable a single human to derive effective models for
	            many complex social phenomena. Computational methods offer a
	            unique opportunity to analyze a wide spectrum of sociological
	            events by leveraging the power of artificial intelligence. Within
	            the human factors community, machine learning has emerged as the
	            dominant {AI}-approach to deal with big data. However, along with
	            its many benefits, machine learning has introduced a unique
	            challenge: interpretability. The models of macro-social behavior
	            generated by {AI} are so complex that rarely can they translated
	            into human understanding. We propose a new method to conduct
	            grounded theory research by leveraging the power of machine
	            learning to analyze complex social phenomena through social
	            network analysis while retaining interpretability as a core
	            feature.},
	file = {:MachineLearningas_BarberisCanonico2018 - Machine Learning As
	        Grounded Theory_ Human Centered Interfaces for Social Network
	        Research through Artificial Intelligence.pdf:PDF},
	groups = {Research Topics, people},
	keywords = {grounded-theory, hcai, social-network},
	langid = {english},
	publisher = {{SAGE} Publications Inc},
	shortjournal = {Proceedings of the Human Factors and Ergonomics Society
	                Annual Meeting},
	shorttitle = {Machine Learning as Grounded Theory},
}

@report{TreeIam_Sonthalia2020,
	author = {Sonthalia, Rishi and Gilbert, Anna C.},
	date = {2020-10-22},
	institution = {{arXiv}},
	title = {Tree! I am no Tree! I am a Low Dimensional Hyperbolic Embedding},
	doi = {10.48550/arXiv.2005.03847},
	eprint = {2005.03847},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:2005.03847},
	abstract = {Given data, finding a faithful low-dimensional hyperbolic
	            embedding of the data is a key method by which we can extract
	            hierarchical information or learn representative geometric
	            features of the data. In this paper, we explore a new method for
	            learning hyperbolic representations by taking a metric-first
	            approach. Rather than determining the low-dimensional hyperbolic
	            embedding directly, we learn a tree structure on the data. This
	            tree structure can then be used directly to extract hierarchical
	            information, embedded into a hyperbolic manifold using Sarkar's
	            construction {\textbackslash}cite\{sarkar\}, or used as a tree
	            approximation of the original metric. To this end, we present a
	            novel fast algorithm {\textbackslash}textsc\{{TreeRep}\} such
	            that, given a \${\textbackslash}delta\$-hyperbolic metric (for
	            any \${\textbackslash}delta {\textbackslash}geq 0\$), the
	            algorithm learns a tree structure that approximates the original
	            metric. In the case when \${\textbackslash}delta = 0\$, we show
	            analytically that {\textbackslash}textsc\{{TreeRep}\} exactly
	            recovers the original tree structure. We show empirically that {
	            \textbackslash}textsc\{{TreeRep}\} is not only many orders of
	            magnitude faster than previously known algorithms, but also
	            produces metrics with lower average distortion and higher mean
	            average precision than most previous algorithms for learning
	            hyperbolic embeddings, extracting hierarchical information, and
	            approximating metrics via tree metrics.},
	comment = {https://github.com/rsonthal/TreeRep},
	copyright = {arXiv.org perpetual, non-exclusive license},
	file = {:TreeIam_Sonthalia2020 - Tree! I Am No Tree! I Am a Low Dimensional
	        Hyperbolic Embedding.pdf:PDF},
	groups = {Research Topics, Tree Editing Interface, hyperbolic trees},
	keywords = {Computer Science - Machine Learning, Mathematics - Metric
	            Geometry, Statistics - Machine Learning},
	priority = {prio1},
	publisher = {arXiv},
	ranking = {rank5},
}

@report{TaskProgrammingLearning_Sun2021,
	author = {Sun, Jennifer J. and Kennedy, Ann and Zhan, Eric and Anderson,
	          David J. and Yue, Yisong and Perona, Pietro},
	date = {2021-03-29},
	institution = {{arXiv}},
	title = {Task Programming: Learning Data Efficient Behavior Representations},
	doi = {10.48550/arXiv.2011.13917},
	eprint = {2011.13917},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:2011.13917},
	abstract = {Specialized domain knowledge is often necessary to accurately
	            annotate training sets for in-depth analysis, but can be
	            burdensome and time-consuming to acquire from domain experts.
	            This issue arises prominently in automated behavior analysis, in
	            which agent movements or actions of interest are detected from
	            video tracking data. To reduce annotation effort, we present {
	            TREBA}: a method to learn annotation-sample efficient trajectory
	            embedding for behavior analysis, based on multi-task
	            self-supervised learning. The tasks in our method can be
	            efficiently engineered by domain experts through a process we
	            call "task programming", which uses programs to explicitly encode
	            structured knowledge from domain experts. Total domain expert
	            effort can be reduced by exchanging data annotation time for the
	            construction of a small number of programmed tasks. We evaluate
	            this trade-off using data from behavioral neuroscience, in which
	            specialized domain knowledge is used to identify behaviors. We
	            present experimental results in three datasets across two
	            domains: mice and fruit flies. Using embeddings from {TREBA}, we
	            reduce annotation burden by up to a factor of 10 without
	            compromising accuracy compared to state-of-the-art features. Our
	            results thus suggest that task programming and self-supervision
	            can be an effective way to reduce annotation effort for domain
	            experts.},
	file = {:TaskProgrammingLearning_Sun2021 - Task Programming_ Learning Data
	        Efficient Behavior Representations.pdf:PDF},
	groups = {Research Topics, Human Factors},
	keywords = {Computer Science - Computer Vision and Pattern Recognition,
	            Computer Science - Machine Learning},
	shorttitle = {Task Programming},
}

@thesis{MetricRepresentationLearning_Sonthalia2021,
	author = {Sonthalia, Rishi Saurabh},
	date = {2021},
	title = {Metric and Representation Learning},
	type = {Thesis},
	doi = {10.7302/2783},
	url = {http://deepblue.lib.umich.edu/handle/2027.42/169738},
	urldate = {2022-07-19},
	abstract = {All data has some inherent mathematical structure. I am
	            interested in understanding the intrinsic geometric and
	            probabilistic structure of data to design effective algorithms
	            and tools that can be applied to machine learning and across all
	            branches of science. The focus of this thesis is to increase the
	            effectiveness of machine learning techniques by developing a
	            mathematical and algorithmic framework using which, given any
	            type of data, we can learn an optimal representation.
	            Representation learning is done for many reasons. It could be
	            done to fix the corruption given corrupted data or to learn a low
	            dimensional or simpler representation, given high dimensional
	            data or a very complex representation of the data. It could also
	            be that the current representation of the data does not capture
	            the important geometric features of the data. One of the many
	            challenges in representation learning is determining ways to
	            judge the quality of the representation learned. In many cases,
	            the consensus is that if d is the natural metric on the
	            representation, then this metric should provide meaningful
	            information about the data. Many examples of this can be seen in
	            areas such as metric learning, manifold learning, and graph
	            embedding. However, most algorithms that solve these problems
	            learn a representation in a metric space first and then extract a
	            metric. A large part of my research is exploring what happens if
	            the order is switched, that is, learn the appropriate metric
	            first and the embedding later. The philosophy behind this
	            approach is that understanding the inherent geometry of the data
	            is the most crucial part of representation learning. Often,
	            studying the properties of the appropriate metric on the input
	            data sets indicates the type of space, we should be seeking for
	            the representation. Hence giving us more robust representations.
	            Optimizing for the appropriate metric can also help overcome
	            issues such as missing and noisy data. My projects fall into
	            three different areas of representation learning. 1) Geometric
	            and probabilistic analysis of representation learning methods. 2)
	            Developing methods to learn optimal metrics on large datasets. 3)
	            Applications. For the category of geometric and probabilistic
	            analysis of representation learning methods, we have three
	            projects. First, designing optimal training data for denoising
	            autoencoders. Second, formulating a new optimal transport problem
	            and understanding the geometric structure. Third, analyzing the
	            robustness to perturbations of the solutions obtained from the
	            classical multidimensional scaling algorithm versus that of the
	            true solutions to the multidimensional scaling problem. For
	            learning optimal metric, we are given a dissimilarity matrix \$
	            hat\{D\}\$, some function \$f\$ and some a subset \$S\$ of the
	            space of all metrics and we want to find \$D in S\$ that
	            minimizes \$f(D,hat\{D\})\$. In this thesis, we consider the
	            version of the problem when \$S\$ is the space of metrics defined
	            on a fixed graph. That is, given a graph \$G\$, we let \$S\$, be
	            the space of all metrics defined via \$G\$. For this \$S\$, we
	            consider the sparse objective function as well as convex
	            objective functions. We also looked at the problem where we want
	            to learn a tree. We also show how the ideas behind learning the
	            optimal metric can be applied to dimensionality reduction in the
	            presence of missing data. Finally, we look at an application to
	            real world data. Specifically trying to reconstruct ancient Greek
	            text.},
	file = {:MetricRepresentationLearning_Sonthalia2021 - Metric and
	        Representation Learning.pdf:PDF},
	groups = {Research Topics, hyperbolic trees},
	howpublished = {Thesis},
	langid = {american},
	priority = {prio3},
}

@article{UnsupervisedKeyphraseExtraction_Haarman2021,
	author = {Haarman, Tim and Zijlema, Bastiaan and Wiering, Marco},
	date = {2021},
	journaltitle = {Multimodal Technologies and Interaction},
	title = {Unsupervised Keyphrase Extraction for Web Pages},
	doi = {10.3390/mti3030058},
	number = {3},
	pages = {58},
	volume = {3},
	file = {:UnsupervisedKeyphraseExtraction_Haarman2021 - Unsupervised
	        Keyphrase Extraction for Web Pages.pdf:PDF},
	groups = {Research Topics, hyperbolic trees, text-as-data},
	publisher = {MDPI AG},
}

@report{NeuralEdgeEditing_Makino2021,
	author = {Makino, Kohei and Miwa, Makoto and Sasaki, Yutaka},
	date = {2021-06-17},
	institution = {{arXiv}},
	title = {A Neural Edge-Editing Approach for Document-Level Relation Graph
	         Extraction},
	doi = {10.48550/arXiv.2106.09900},
	eprint = {2106.09900},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:2106.09900},
	abstract = {In this paper, we propose a novel edge-editing approach to
	            extract relation information from a document. We treat the
	            relations in a document as a relation graph among entities in
	            this approach. The relation graph is iteratively constructed by
	            editing edges of an initial graph, which might be a graph
	            extracted by another system or an empty graph. The way to edit
	            edges is to classify them in a close-first manner using the
	            document and temporally-constructed graph information; each edge
	            is represented with a document context information by a
	            pretrained transformer model and a graph context information by a
	            graph convolutional neural network model. We evaluate our
	            approach on the task to extract material synthesis procedures
	            from materials science texts. The experimental results show the
	            effectiveness of our approach in editing the graphs initialized
	            by our in-house rule-based system and empty graphs.},
	file = {:NeuralEdgeEditing_Makino2021 - A Neural Edge Editing Approach for
	        Document Level Relation Graph Extraction.pdf:PDF},
	groups = {Research Topics, graphs, text-as-data},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{TreesContinuousEmbeddings_Chami2020,
	author = {Chami, Ines and Gu, Albert and Chatziafratis, Vaggos and Ré,
	          Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	date = {2020},
	title = {From Trees to Continuous Embeddings and Back: Hyperbolic
	         Hierarchical Clustering},
	pages = {15065--15076},
	publisher = {Curran Associates, Inc.},
	url = {
	       https://proceedings.neurips.cc/paper/2020/hash/ac10ec1ace51b2d973cd87973a98d3ab-Abstract.html
	       },
	urldate = {2022-07-19},
	volume = {33},
	abstract = {Similarity-based Hierarchical Clustering ({HC}) is a classical
	            unsupervised machine learning algorithm that has traditionally
	            been solved with heuristic algorithms like Average-Linkage.
	            Recently, Dasgupta reframed {HC} as a discrete optimization
	            problem by introducing a global cost function measuring the
	            quality of a given tree. In this work, we provide the first
	            continuous relaxation of Dasgupta's discrete optimization problem
	            with provable quality guarantees. The key idea of our method, {
	            HypHC}, is showing a direct correspondence from discrete trees to
	            continuous representations (via the hyperbolic embeddings of
	            their leaf nodes) and back (via a decoding algorithm that maps
	            leaf embeddings to a dendrogram), allowing us to search the space
	            of discrete binary trees with continuous optimization. Building
	            on analogies between trees and hyperbolic space, we derive a
	            continuous analogue for the notion of lowest common ancestor,
	            which leads to a continuous relaxation of Dasgupta's discrete
	            objective. We can show that after decoding, the global minimizer
	            of our continuous relaxation yields a discrete tree with a
	            (1+eps)-factor approximation for Dasgupta's optimal tree, where
	            eps can be made arbitrarily small and controls optimization
	            challenges. We experimentally evaluate {HypHC} on a variety of {
	            HC} benchmarks and find that even approximate solutions found
	            with gradient descent have superior clustering quality than
	            agglomerative heuristics or other gradient based algorithms.
	            Finally, we highlight the flexibility of {HypHC} using end-to-end
	            training in a downstream classification task.},
	file = {:TreesContinuousEmbeddings_Chami2020 - From Trees to Continuous
	        Embeddings and Back_ Hyperbolic Hierarchical Clustering.pdf:PDF},
	groups = {Research Topics, Tree Editing Interface, hyperbolic trees},
	priority = {prio2},
	shorttitle = {From Trees to Continuous Embeddings and Back},
}

@article{Novelmetrichyperbolic_Matsumoto2021,
	author = {Matsumoto, Hirotaka and Mimori, Takahiro and Fukunaga, Tsukasa},
	date = {2021-01-01},
	journaltitle = {Biology Methods and Protocols},
	title = {Novel metric for hyperbolic phylogenetic tree embeddings},
	doi = {10.1093/biomethods/bpab006},
	issn = {2396-8923},
	number = {1},
	pages = {bpab006},
	urldate = {2022-07-19},
	volume = {6},
	abstract = {Advances in experimental technologies, such as {DNA} sequencing,
	            have opened up new avenues for the applications of phylogenetic
	            methods to various fields beyond their traditional application in
	            evolutionary investigations, extending to the fields of
	            development, differentiation, cancer genomics, and
	            immunogenomics. Thus, the importance of phylogenetic methods is
	            increasingly being recognized, and the development of a novel
	            phylogenetic approach can contribute to several areas of
	            research. Recently, the use of hyperbolic geometry has attracted
	            attention in artificial intelligence research. Hyperbolic space
	            can better represent a hierarchical structure compared to
	            Euclidean space, and can therefore be useful for describing and
	            analyzing a phylogenetic tree. In this study, we developed a
	            novel metric that considers the characteristics of a phylogenetic
	            tree for representation in hyperbolic space. We compared the
	            performance of the proposed hyperbolic embeddings, general
	            hyperbolic embeddings, and Euclidean embeddings, and confirmed
	            that our method could be used to more precisely reconstruct
	            evolutionary distance. We also demonstrate that our approach is
	            useful for predicting the nearest-neighbor node in a partial
	            phylogenetic tree with missing nodes. Furthermore, we proposed a
	            novel approach based on our metric to integrate multiple trees
	            for analyzing tree nodes or imputing missing distances. This
	            study highlights the utility of adopting a geometric approach for
	            further advancing the applications of phylogenetic methods.},
	file = {:Novelmetrichyperbolic_Matsumoto2021 - Novel Metric for Hyperbolic
	        Phylogenetic Tree Embeddings.pdf:PDF},
	groups = {hyperbolic trees},
	shortjournal = {Biology Methods and Protocols},
}

@online{CombinatorialHyperbolicEmbeddings_meiji1632021,
	author = {meiji163},
	date = {2021-08-24},
	title = {Combinatorial Hyperbolic Embeddings},
	url = {https://meiji163.github.io/post/combo-hyperbolic-embedding/},
	urldate = {2022-07-19},
	abstract = {Introduction The manifold hypothesis says that most real-world
	            datasets lie approximately on a low-dimensional manifold, but by
	            some Kantian twist of fate we rarely have direct access to this
	            manifold1. As a result, most machine learning techniques utilize
	            only local structure (e.g. the “loss + {SGD}” machine). In
	            contrast, Topological and Geometric Data Analysis ({TGDA}) is a
	            burgeoning field that studies the global structure of data. This
	            is exciting not only because of potential domain applications –
	            it also opens possibilities for porting a lot of powerful and
	            beautiful math to the {ML} realm.},
	groups = {Tree Editing Interface, hyperbolic trees},
	langid = {english},
	priority = {prio3},
}

@inproceedings{HyperbolicDistanceMatrices_Tabaghi2020,
	author = {Tabaghi, Puoya and Dokmanić, Ivan},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} International Conference
	             on Knowledge Discovery \& Data Mining},
	date = {2020-08-20},
	title = {Hyperbolic Distance Matrices},
	doi = {10.1145/3394486.3403224},
	isbn = {9781450379984},
	location = {New York, {NY}, {USA}},
	pages = {1728--1738},
	publisher = {Association for Computing Machinery},
	series = {{KDD} '20},
	urldate = {2023-01-03},
	abstract = {Hyperbolic space is a natural setting for mining and visualizing
	            data with hierarchical structure. In order to compute a
	            hyperbolic embedding from comparison or similarity information,
	            one has to solve a hyperbolic distance geometry problem. In this
	            paper, we propose a unified framework to compute hyperbolic
	            embeddings from an arbitrary mix of noisy metric and non-metric
	            data. Our algorithms are based on semidefinite programming and
	            the notion of a hyperbolic distance matrix, in many ways parallel
	            to its famous Euclidean counterpart. A central ingredient we put
	            forward is a semidefinite characterization of the hyperbolic
	            Gramian---a matrix of Lorentzian inner products. This
	            characterization allows us to formulate a semidefinite relaxation
	            to efficiently compute hyperbolic embeddings in two stages: first
	            , we complete and denoise the observed hyperbolic distance
	            matrix; second, we propose a spectral factorization method to
	            estimate the embedded points from the hyperbolic distance matrix.
	            We show through numerical experiments how the flexibility to mix
	            metric and non-metric constraints allows us to efficiently
	            compute embeddings from arbitrary data.},
	file = {:HyperbolicDistanceMatrices_Tabaghi2020 - Hyperbolic Distance
	        Matrices.pdf:PDF},
	groups = {Research Topics, Tree Editing Interface, hyperbolic trees},
	keywords = {spectral factorization, distance geometry, hyperbolic space,
	            semidefinite program},
	priority = {prio2},
}

@article{Spectralsparsificationgraphs_Batson2013,
	author = {Batson, Joshua and Spielman, Daniel A. and Srivastava, Nikhil and
	          Teng, Shang-Hua},
	date = {2013-08-01},
	journaltitle = {Communications of the {ACM}},
	title = {Spectral sparsification of graphs: theory and algorithms},
	doi = {10.1145/2492007.2492029},
	issn = {0001-0782},
	number = {8},
	pages = {87--94},
	urldate = {2023-02-01},
	volume = {56},
	abstract = {Graph sparsification is the approximation of an arbitrary graph
	            by a sparse graph. We explain what it means for one graph to be a
	            spectral approximation of another and review the development of
	            algorithms for spectral sparsification. In addition to being an
	            interesting concept, spectral sparsification has been an
	            important tool in the design of nearly linear-time algorithms for
	            solving systems of linear equations in symmetric, diagonally
	            dominant matrices. The fast solution of these linear systems has
	            already led to breakthrough results in combinatorial optimization
	            , including a faster algorithm for finding approximate maximum
	            flows and minimum cuts in an undirected network.},
	file = {:Spectralsparsificationgraphs_Batson2013 - Spectral Sparsification
	        of Graphs_ Theory and Algorithms.pdf:PDF},
	groups = {laplacian-systems},
	priority = {prio2},
	shortjournal = {Commun. {ACM}},
	shorttitle = {Spectral sparsification of graphs},
}

@report{SpectralSparsificationGraphs_Spielman2010,
	author = {Spielman, Daniel A. and Teng, Shang-Hua},
	date = {2010-07-20},
	institution = {{arXiv}},
	title = {Spectral Sparsification of Graphs},
	doi = {10.48550/arXiv.0808.4134},
	eprint = {0808.4134},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:0808.4134},
	abstract = {We introduce a new notion of graph sparsificaiton based on
	            spectral similarity of graph Laplacians: spectral sparsification
	            requires that the Laplacian quadratic form of the sparsifier
	            approximate that of the original. This is equivalent to saying
	            that the Laplacian of the sparsifier is a good preconditioner for
	            the Laplacian of the original. We prove that every graph has a
	            spectral sparsifier of nearly linear size. Moreover, we present
	            an algorithm that produces spectral sparsifiers in time \${
	            \textbackslash}{softO}\{m\}\$, where \$m\$ is the number of edges
	            in the original graph. This construction is a key component of a
	            nearly-linear time algorithm for solving linear equations in
	            diagonally-dominant matrcies. Our sparsification algorithm makes
	            use of a nearly-linear time algorithm for graph partitioning that
	            satisfies a strong guarantee: if the partition it outputs is very
	            unbalanced, then the larger part is contained in a subgraph of
	            high conductance.},
	file = {:Spielman2010 - Spectral Sparsification of Graphs.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {Computer Science - Data Structures and Algorithms, Computer
	            Science - Discrete Mathematics},
}

@article{Lxb_Vishnoi2013,
	author = {Vishnoi, Nisheeth K.},
	date = {2013-05-22},
	journaltitle = {Foundations and Trends® in Theoretical Computer Science},
	title = {Lx = b},
	doi = {10.1561/0400000054},
	issn = {1551-305X, 1551-3068},
	number = {1},
	pages = {1--141},
	url = {https://www.nowpublishers.com/article/Details/TCS-054},
	urldate = {2023-02-01},
	volume = {8},
	abstract = {Lx = b},
	file = {:Lxb_Vishnoi2013 - Lx = B.pdf:PDF},
	groups = {laplacian-systems},
	publisher = {Now Publishers, Inc.},
	shortjournal = {{TCS}},
}

@inproceedings{Graphsparsificationeffective_Spielman2008,
	author = {Spielman, Daniel A. and Srivastava, Nikhil},
	booktitle = {Proceedings of the fortieth annual {ACM} symposium on Theory of
	             computing},
	date = {2008-05-17},
	title = {Graph sparsification by effective resistances},
	doi = {10.1145/1374376.1374456},
	isbn = {9781605580470},
	location = {New York, {NY}, {USA}},
	pages = {563--568},
	publisher = {Association for Computing Machinery},
	series = {{STOC} '08},
	urldate = {2023-02-01},
	abstract = {We present a nearly-linear time algorithm that produces
	            high-quality sparsifiers of weighted graphs. Given as input a
	            weighted graph G=(V,E,w) and a parameter ε{\textgreater}0, we
	            produce a weighted subgraph H=(V,{\textasciitilde}E,{
	            \textasciitilde}w) of G such that {\textbar}{\textasciitilde}E{
	            \textbar}=O(n log n/ε2) and for all vectors x in {RV}. (1-ε) ∑uv
	            ∈ E (x(u)-x(v))2wuv≤ ∑uv in {\textasciitilde}E(x(u)-x(v))2{
	            \textasciitilde}wuv ≤ (1+ε)∑uv ∈ E(x(u)-x(v))2wuv. This improves
	            upon the sparsifiers constructed by Spielman and Teng, which had
	            O(n logc n) edges for some large constant c, and upon those of
	            Benczur and Karger, which only satisfied (1) for x in \{0,1\}V.
	            We conjecture the existence of sparsifiers with O(n) edges,
	            noting that these would generalize the notion of expander graphs,
	            which are constant-degree sparsifiers for the complete graph. A
	            key ingredient in our algorithm is a subroutine of independent
	            interest: a nearly-linear time algorithm that builds a data
	            structure from which we can query the approximate effective
	            resistance between any two vertices in a graph in O(log n) time.},
	file = {:Graphsparsificationeffective_Spielman2008 - Graph Sparsification by
	        Effective Resistances.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {random sampling, spectral graph theory, electrical flows},
	priority = {prio2},
}

@article{NearlyLinearTime_Spielman2014,
	author = {Spielman, Daniel A. and Teng, Shang-Hua},
	date = {2014-01},
	journaltitle = {{SIAM} Journal on Matrix Analysis and Applications},
	title = {Nearly Linear Time Algorithms for Preconditioning and Solving
	         Symmetric, Diagonally Dominant Linear Systems},
	doi = {10.1137/090771430},
	issn = {0895-4798},
	number = {3},
	pages = {835--885},
	urldate = {2023-02-01},
	volume = {35},
	abstract = {We present a randomized algorithm that on input a symmetric,
	            weakly diagonally dominant 𝑛 n -by- 𝑛 n matrix 𝐴 A with 𝑚 m
	            nonzero entries and an 𝑛 n -vector 𝑏 b produces an 𝑥 ̃ x{
	            \textasciitilde} such that ‖ 𝑥 ̃ − 𝐴 † 𝑏 ‖ 𝐴 ≤𝜖‖ 𝐴 † 𝑏 ‖ 𝐴 ‖x{
	            \textasciitilde}−A†b‖A≤ϵ‖A†b‖A in expected time 𝑂(𝑚 log 𝑐
	            𝑛log(1/𝜖)) O(mlogc⁡nlog⁡(1/ϵ)) for some constant 𝑐 c . By
	            applying this algorithm inside the inverse power method, we
	            compute approximate Fiedler vectors in a similar amount of time.
	            The algorithm applies subgraph preconditioners in a recursive
	            fashion. These preconditioners improve upon the subgraph
	            preconditioners first introduced by Vaidya in 1990. For any
	            symmetric, weakly diagonally dominant matrix 𝐴 A with nonpositive
	            off-diagonal entries and 𝑘≥1 k≥1 , we construct in time 𝑂(𝑚 log 𝑐
	            𝑛) O(mlogc⁡n) a preconditioner 𝐵 B of 𝐴 A with at most
	            2(𝑛−1)+𝑂((𝑚/𝑘) log 39 𝑛) 2(n−1)+O((m/k)log39⁡n) nonzero
	            off-diagonal entries such that the finite generalized condition
	            number 𝜅 𝑓 (𝐴,𝐵) κf(A,B) is at most 𝑘 k , for some other constant
	            𝑐 c . In the special case when the nonzero structure of the
	            matrix is planar the corresponding linear system solver runs in
	            expected time 𝑂(𝑛 log 2 𝑛+𝑛log𝑛 loglog𝑛 log(1/𝜖))
	            O(nlog2⁡n+nlog⁡n log⁡log⁡n log⁡(1/ϵ)) . We hope that our
	            introduction of algorithms of low asymptotic complexity will lead
	            to the development of algorithms that are also fast in practice.},
	groups = {laplacian-systems},
	keywords = {support theory, preconditioning, linear equation solvers,
	            symmetric, diagonally dominant matrices, Laplacians, 65F08, 68Q25
	            },
	publisher = {Society for Industrial and Applied Mathematics},
	shortjournal = {{SIAM} J. Matrix Anal. Appl.},
}

@inproceedings{FastSequenceBased_Rozemberczki2018,
	author = {Rozemberczki, Benedek and Sarkar, Rik},
	booktitle = {Complex Networks {IX}},
	date = {2018},
	title = {Fast Sequence-Based Embedding with Diffusion Graphs},
	doi = {10.1007/978-3-319-73198-8_9},
	editor = {Cornelius, Sean and Coronges, Kate and Gonçalves, Bruno and
	          Sinatra, Roberta and Vespignani, Alessandro},
	isbn = {9783319731988},
	location = {Cham},
	pages = {99--107},
	publisher = {Springer International Publishing},
	series = {Springer Proceedings in Complexity},
	abstract = {A {graphRozemberczki}, {BenedekembeddingSarkar}, Rikis a
	            representation of graph vertices in a low- dimensional space,
	            which approximately preserves properties such as distances
	            between nodes. Vertex sequence-based embedding procedures use
	            features extracted from linear sequences of nodes to create
	            embeddings using a neural network. In this paper, we propose
	            diffusion graphs as a method to rapidly generate vertex sequences
	            for network embedding. Its computational efficiency is superior
	            to previous methods due to simpler sequence generation, and it
	            produces more accurate results. In experiments, we found that the
	            performance relative to other methods improves with increasing
	            edge density in the graph. In a community detection task,
	            clustering nodes in the embedding space produces better results
	            compared to other sequence-based embedding methods.},
	file = {Full Text PDF:https\:
	        //link.springer.com/content/pdf/10.1007%2F978-3-319-73198-8_9.pdf:application/pdf
	        },
	groups = {hyperbolic trees},
	keywords = {Graph Diffusion, Vertex Sequence, Community Detection, Simpler
	            Sequence Generation, Eulerian Walk},
	langid = {english},
}

@article{leastsquaresformulation_Shu2015,
	author = {Shu, Xin and Xu, Huanliang and Tao, Liang},
	date = {2015-05-25},
	journaltitle = {Neurocomputing},
	title = {A least squares formulation of multi-label linear discriminant
	         analysis},
	doi = {10.1016/j.neucom.2014.12.057},
	issn = {0925-2312},
	pages = {221--230},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231214017214},
	urldate = {2023-02-01},
	volume = {156},
	abstract = {The classical linear discriminant analysis has been recently
	            extended to the multi-label dimensionality reduction. However,
	            Multi-label Linear Discriminant Analysis ({MLDA}) involves dense
	            matrices eigen-decomposition that is known to be computationally
	            expensive for the large-scale problems. In this paper, we present
	            that the formulation of {MLDA} can be equivalently casted as a
	            new least-squares framework so as to significantly mitigate the
	            computational overhead and scale to the data collections with
	            higher dimension. Further, it is also found that appealing
	            regularization techniques can be incorporated into the
	            least-squares model to boost generalization accuracy.
	            Experimental results on several popular multi-label benchmarks
	            not only verify the established equivalence relationship, but
	            also corroborate the effectiveness and efficiency of our proposed
	            algorithms.},
	file = {:leastsquaresformulation_Shu2015 - A Least Squares Formulation of
	        Multi Label Linear Discriminant Analysis.html:URL},
	groups = {laplacian-systems},
	keywords = {Multi-label linear discriminant analysis, Least squares,
	            Dimension reduction, Spectral regression},
	langid = {english},
	shortjournal = {Neurocomputing},
}

@article{orientedhypergraphicapproach_Reff2012,
	author = {Reff, Nathan and Rusnak, Lucas J.},
	date = {2012-11-01},
	journaltitle = {Linear Algebra and its Applications},
	title = {An oriented hypergraphic approach to algebraic graph theory},
	doi = {10.1016/j.laa.2012.06.011},
	issn = {0024-3795},
	number = {9},
	pages = {2262--2270},
	url = {https://www.sciencedirect.com/science/article/pii/S0024379512004466},
	urldate = {2023-02-01},
	volume = {437},
	abstract = {An oriented hypergraph is a hypergraph where each vertex-edge
	            incidence is given a label of +1 or -1. We define the adjacency,
	            incidence and Laplacian matrices of an oriented hypergraph and
	            study each of them. We extend several matrix results known for
	            graphs and signed graphs to oriented hypergraphs. New matrix
	            results that are not direct generalizations are also presented.
	            Finally, we study a new family of matrices that contains walk
	            information.},
	file = {:orientedhypergraphicapproach_Reff2012 - An Oriented Hypergraphic
	        Approach to Algebraic Graph Theory.pdf:PDF:https\:
	        //core.ac.uk/download/pdf/82035031.pdf},
	groups = {incidence structures},
	keywords = {Oriented hypergraph, Hypergraph Laplacian matrix, Hypergraph
	            adjacency matrix, Incidence matrix, Signed graph},
	langid = {english},
	priority = {prio2},
	shortjournal = {Linear Algebra and its Applications},
}

@book{GraphAlgorithmsLanguage_Kepner2011,
	author = {Kepner, Jeremy and Gilbert, John},
	date = {2011-08-04},
	title = {Graph Algorithms in the Language of Linear Algebra},
	isbn = {9780898719901},
	location = {Philadelphia},
	pagetotal = {375},
	publisher = {Society for Industrial and Applied Mathematics},
	abstract = {The field of graph algorithms has become one of the pillars of
	            theoretical computer science, informing research in such diverse
	            areas as combinatorial optimization, complexity theory and
	            topology. To improve the computational performance of graph
	            algorithms, researchers have proposed a shift to a parallel
	            computing paradigm. This book addresses the challenges of
	            implementing parallel graph algorithms by exploiting the
	            well-known duality between a canonical representation of graphs
	            as abstract collections of vertices and edges and a sparse
	            adjacency matrix representation. This linear algebraic approach
	            is widely accessible to scientists and engineers who may not be
	            formally trained in computer science. The authors show how to
	            leverage existing parallel matrix computation techniques and the
	            large amount of software infrastructure that exists for these
	            computations to implement efficient and scalable parallel graph
	            algorithms. The benefits of this approach are reduced algorithmic
	            complexity, ease of implementation and improved performance.},
	file = {:GraphAlgorithmsLanguage_Kepner2011 - Graph Algorithms in the
	        Language of Linear Algebra.html:URL},
	groups = {incidence structures},
	priority = {prio2},
}

@inproceedings{HyperbolicManifoldRegression_Marconi2020,
	author = {Marconi, Gian and Ciliberto, Carlo and Rosasco, Lorenzo},
	date = {2020-06-03},
	title = {Hyperbolic Manifold Regression},
	eventtitle = {International Conference on Artificial Intelligence and
	              Statistics},
	pages = {2570--2580},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v108/marconi20a.html},
	urldate = {2023-02-01},
	abstract = {Geometric representation learning has shown great promise for
	            important tasks inartificial intelligence and machine learning.
	            However, an open problem is yethow to integrate non-Euclidean
	            representations with standard machine learningmethods.In this
	            work, we consider the task of regression onto hyperbolic space
	            for whichwe propose two approaches: a non-parametric
	            kernel-method for which we also proveexcess risk bounds and a
	            parametric deep learning model that is informed bythe geodesics
	            of the target space.By recasting predictions on trees as manifold
	            regression problems we demonstrate the applications of our
	            approach on two challenging tasks: 1)hierarchical classification
	            via label embeddings and 2) inventing new conceptsby predicting
	            their embedding in a continuous representation of a base
	            taxonomy.In our experiments, we find that the proposed estimators
	            outperform their naivecounterparts that perform regression in the
	            ambient Euclidean space.},
	file = {:HyperbolicManifoldRegression_Marconi2020 - Hyperbolic Manifold
	        Regression.pdf:PDF},
	groups = {hyperbolic trees},
	issn = {2640-3498},
	langid = {english},
}

@inproceedings{ManifoldStructuredPrediction_Rudi2018,
	author = {Rudi, Alessandro and Ciliberto, Carlo and Marconi, {GianMaria} and
	          Rosasco, Lorenzo},
	booktitle = {Advances in Neural Information Processing Systems},
	date = {2018},
	title = {Manifold Structured Prediction},
	publisher = {Curran Associates, Inc.},
	url = {
	       https://proceedings.neurips.cc/paper/2018/hash/f6185f0ef02dcaec414a3171cd01c697-Abstract.html
	       },
	urldate = {2023-02-01},
	volume = {31},
	abstract = {Structured prediction provides a general framework to deal with
	            supervised problems where the outputs have semantically rich
	            structure. While classical approaches consider finite, albeit
	            potentially huge, output spaces, in this paper we discuss how
	            structured prediction can be extended to a continuous scenario.
	            Specifically, we study a structured prediction approach to
	            manifold-valued regression. We characterize a class of problems
	            for which the considered approach is statistically consistent and
	            study how geometric optimization can be used to compute the
	            corresponding estimator. Promising experimental results on both
	            simulated and real data complete our study.},
	file = {:ManifoldStructuredPrediction_Rudi2018 - Manifold Structured
	        Prediction.pdf:PDF},
	groups = {hyperbolic trees},
}

@inproceedings{Structureestimationdiscrete_Loh2012,
	author = {Loh, Po-ling and Wainwright, Martin J},
	booktitle = {Advances in Neural Information Processing Systems},
	date = {2012},
	title = {Structure estimation for discrete graphical models: Generalized
	         covariance matrices and their inverses},
	doi = {10.1214/13-aos1162},
	publisher = {Curran Associates, Inc.},
	url = {
	       https://proceedings.neurips.cc/paper/2012/hash/6ba1085b788407963fe0e89c699a7396-Abstract.html
	       },
	urldate = {2023-02-01},
	volume = {25},
	abstract = {We investigate a curious relationship between the structure of a
	            discrete graphical model and the support of the inverse of a
	            generalized covariance matrix. We show that for certain graph
	            structures, the support of the inverse covariance matrix of
	            indicator variables on the vertices of a graph reflects the
	            conditional independence structure of the graph. Our work extends
	            results that have previously been es- tablished only in the
	            context of multivariate Gaussian graphical models, thereby
	            addressing an open question about the significance of the inverse
	            covariance ma- trix of a non-Gaussian distribution. Based on our
	            population-level results, we show how the graphical Lasso may be
	            used to recover the edge structure of cer- tain classes of
	            discrete graphical models, and present simulations to verify our
	            theoretical results.},
	file = {:Structureestimationdiscrete_Loh2012 - Structure Estimation for
	        Discrete Graphical Models_ Generalized Covariance Matrices and Their
	        Inverses.pdf:PDF},
	groups = {graphical-models, laplacian-systems},
	priority = {prio1},
	shorttitle = {Structure estimation for discrete graphical models},
}

@report{Betterunderstandingmultivariate_Duan2021,
	author = {Duan, X. G.},
	date = {2021-01-02},
	institution = {{arXiv}},
	title = {Better understanding of the multivariate hypergeometric
	         distribution with implications in design-based survey sampling},
	doi = {10.48550/arXiv.2101.00548},
	eprint = {2101.00548},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:2101.00548},
	abstract = {Multivariate hypergeometric distribution arises frequently in
	            elementary statistics and probability courses, for simultaneously
	            studying the occurence law of specified events, when sampling
	            without replacement from a finite population with fixed number of
	            classification. Covariance matrix of this distribution is well
	            known to be identical to its multinomial counterpart multiplied
	            by 1-(n-1)/(N-1), with N and n being population and sample sizes,
	            respectively. It appears to however, have been less discussed in
	            the literature about the meaning of this relationship, especially
	            regarding the specific form of the multiplier. Based on an
	            augmenting argument together with probabilistic symmetry, we
	            present a more transparent understanding for the covariance
	            structure of the multivariate hypergeometric distribution. We
	            discuss implications of these combined techniques and provide a
	            unified description about the relative efficiency for estimating
	            population mean based on simple random sampling, probability
	            proportional-to-size sampling and adaptive cluster sampling, with
	            versus without replacement. We also provide insight into the
	            classic random group method for variance estimation.},
	file = {:Betterunderstandingmultivariate_Duan2021 - Better Understanding of
	        the Multivariate Hypergeometric Distribution with Implications in
	        Design Based Survey Sampling.pdf:PDF},
	groups = {incidence structures},
	keywords = {Mathematics - Statistics Theory, Statistics - Other Statistics},
}

@article{Someapproximationsmultivariate_Childs2000,
	author = {Childs, Aaron and Balakrishnan, N.},
	date = {2000-12-28},
	journaltitle = {Computational Statistics \& Data Analysis},
	title = {Some approximations to the multivariate hypergeometric distribution
	         with applications to hypothesis testing},
	doi = {10.1016/S0167-9473(00)00007-4},
	issn = {0167-9473},
	number = {2},
	pages = {137--154},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947300000074},
	urldate = {2023-02-01},
	volume = {35},
	abstract = {In this paper, we will examine some approximations to the
	            multivariate hypergeometric distribution by continuous random
	            variables. The continuous random variables will be chosen so as
	            to have the same range of variation, means, variances and
	            covariances as their discrete counterparts. We then show how
	            these approximations can be used in testing hypotheses about the
	            parameters of the multivariate hypergeometric distribution.},
	file = {:Someapproximationsmultivariate_Childs2000 - Some Approximations to
	        the Multivariate Hypergeometric Distribution with Applications to
	        Hypothesis Testing.html:URL},
	groups = {incidence structures},
	keywords = {Order statistics, Multivariate hypergeometric distribution,
	            Dirichlet distribution},
	langid = {english},
	shortjournal = {Computational Statistics \& Data Analysis},
}

@report{LearningGroupImportance_Sutter2022,
	author = {Sutter, Thomas M. and Manduchi, Laura and Ryser, Alain and Vogt,
	          Julia E.},
	date = {2022-03-03},
	institution = {{ETH} Zurich, Departement of Computer Science},
	title = {Learning Group Importance using the Differentiable Hypergeometric
	         Distribution},
	type = {Working Paper},
	doi = {10.3929/ethz-b-000588775},
	url = {https://www.research-collection.ethz.ch/handle/20.500.11850/588775},
	urldate = {2023-02-01},
	abstract = {Partitioning a set of elements into subsets of a priori unknown
	            sizes is essential in many applications. These subset sizes are
	            rarely explicitly learned - be it the cluster sizes in clustering
	            applications or the number of shared versus independent
	            generative latent factors in weakly-supervised learning.
	            Probability distributions over correct combinations of subset
	            sizes are non-differentiable due to hard constraints, which
	            prohibit gradient-based optimization. In this work, we propose
	            the differentiable hypergeometric distribution. The
	            hypergeometric distribution models the probability of different
	            group sizes based on their relative importance. We introduce
	            reparameterizable gradients to learn the importance between
	            groups and highlight the advantage of explicitly learning the
	            size of subsets in two typical applications: weakly-supervised
	            learning and clustering. In both applications, we outperform
	            previous approaches, which rely on suboptimal heuristics to model
	            the unknown size of groups.},
	file = {:LearningGroupImportance_Sutter2022 - Learning Group Importance
	        Using the Differentiable Hypergeometric Distribution.pdf:PDF},
	groups = {incidence structures},
	howpublished = {Working Paper},
	langid = {english},
	rights = {http://rightsstatements.org/page/{InC}-{NC}/1.0/},
}

@inproceedings{HyperbolicEntailmentCones_Ganea2018,
	author = {Ganea, Octavian and Becigneul, Gary and Hofmann, Thomas},
	date = {2018-07-03},
	title = {Hyperbolic Entailment Cones for Learning Hierarchical Embeddings},
	eventtitle = {International Conference on Machine Learning},
	pages = {1646--1655},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v80/ganea18a.html},
	urldate = {2023-02-01},
	abstract = {Learning graph representations via low-dimensional embeddings
	            that preserve relevant network properties is an important class
	            of problems in machine learning. We here present a novel method
	            to embed directed acyclic graphs. Following prior work, we first
	            advocate for using hyperbolic spaces which provably model
	            tree-like structures better than Euclidean geometry. Second, we
	            view hierarchical relations as partial orders defined using a
	            family of nested geodesically convex cones. We prove that these
	            entailment cones admit an optimal shape with a closed form
	            expression both in the Euclidean and hyperbolic spaces, and they
	            canonically define the embedding learning process. Experiments
	            show significant improvements of our method over strong recent
	            baselines both in terms of representational capacity and
	            generalization.},
	file = {:HyperbolicEntailmentCones_Ganea2018 - Hyperbolic Entailment Cones
	        for Learning Hierarchical Embeddings.pdf:PDF;:ganea_hyperbolic_2018 -
	        Hyperbolic Entailment Cones for Learning Hierarchical
	        Embeddings.pdf:PDF},
	groups = {hyperbolic trees},
	issn = {2640-3498},
	langid = {english},
	priority = {prio2},
}

@techreport{CollaborativeCreationCommunal_Heymann2006,
	author = {Paul Heymann and Hector Garcia-Molina},
	date = {2006-04},
	institution = {Stanford InfoLab},
	title = {Collaborative Creation of Communal Hierarchical Taxonomies in
	         Social Tagging Systems},
	number = {2006-10},
	type = {Technical Report},
	url = {http://ilpubs.stanford.edu:8090/775/},
	abstract = {Collaborative tagging systems---systems where many casual users
	            annotate objects with free-form strings (tags) of their
	            choosing---have recently emerged as a powerful way to label and
	            organize large collections of data. During our recent
	            investigation into these types of systems, we discovered a simple
	            but remarkably effective algorithm for converting a large corpus
	            of tags annotating objects in a tagging system into a navigable
	            hierarchical taxonomy of tags. We first discuss the algorithm and
	            then present a preliminary model to explain why it is so
	            effective in these types of systems.},
	file = {:CollaborativeCreationCommunal_Heymann2006 - Collaborative Creation
	        of Communal Hierarchical Taxonomies in Social Tagging Systems.pdf:PDF
	        },
	groups = {hyperbolic trees, text-as-data},
	keywords = {Tagging, taxonomy, hierarchy, annotation, metadata management,
	            CSCW.},
	priority = {prio2},
	publisher = {Stanford},
}

@inproceedings{LearningContinuousHierarchies_Nickel2018,
	author = {Nickel, Maximillian and Kiela, Douwe},
	date = {2018-07-03},
	title = {Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic
	         Geometry},
	eventtitle = {International Conference on Machine Learning},
	pages = {3779--3788},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v80/nickel18a.html},
	urldate = {2023-02-01},
	abstract = {We are concerned with the discovery of hierarchical
	            relationships from large-scale unstructured similarity scores.
	            For this purpose, we study different models of hyperbolic space
	            and find that learning embeddings in the Lorentz model is
	            substantially more efficient than in the Poincar\{é\}-ball model.
	            We show that the proposed approach allows us to learn
	            high-quality embeddings of large taxonomies which yield
	            improvements over Poincar\{é\} embeddings, especially in low
	            dimensions. Lastly, we apply our model to discover hierarchies in
	            two real-world datasets: we show that an embedding in hyperbolic
	            space can reveal important aspects of a company’s organizational
	            structure as well as reveal historical relationships between
	            language families.},
	file = {:LearningContinuousHierarchies_Nickel2018 - Learning Continuous
	        Hierarchies in the Lorentz Model of Hyperbolic Geometry.pdf:PDF},
	groups = {hyperbolic trees},
	issn = {2640-3498},
	langid = {english},
	priority = {prio2},
}

@inproceedings{LowDistortionDelaunay_Sarkar2012,
	author = {Sarkar, Rik},
	booktitle = {Graph Drawing},
	date = {2012},
	title = {Low Distortion Delaunay Embedding of Trees in Hyperbolic Plane},
	doi = {10.1007/978-3-642-25878-7_34},
	editor = {van Kreveld, Marc and Speckmann, Bettina},
	isbn = {9783642258787},
	location = {Berlin, Heidelberg},
	pages = {355--366},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	abstract = {This paper considers the problem of embedding trees into the
	            hyperbolic plane. We show that any tree can be realized as the
	            Delaunay graph of its embedded vertices. Particularly, a weighted
	            tree can be embedded such that the weight on each edge is
	            realized as the hyperbolic distance between its embedded
	            vertices. Thus the embedding preserves the metric information of
	            the tree along with its topology. The distance distortion between
	            non adjacent vertices can be made arbitrarily small – less than a
	            (1 + ε) factor for any given ε. Existing results on low
	            distortion of embedding discrete metrics into trees carry over to
	            hyperbolic metric through this result. The Delaunay character
	            implies useful properties such as guaranteed greedy routing and
	            realization as minimum spanning trees.},
	file = {:LowDistortionDelaunay_Sarkar2012 - Low Distortion Delaunay
	        Embedding of Trees in Hyperbolic Plane.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Minimum Span Tree, Voronoi Diagram, Voronoi Cell, Hyperbolic
	            Plane, Weighted Tree},
	langid = {english},
	priority = {prio1},
}

@article{surveygraphkernels_Kriege2020,
	author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
	date = {2020-12},
	journaltitle = {Applied Network Science},
	title = {A survey on graph kernels},
	doi = {10.1007/s41109-019-0195-3},
	issn = {2364-8228},
	number = {1},
	pages = {1--42},
	urldate = {2023-02-01},
	volume = {5},
	abstract = {Graph kernels have become an established and widely-used
	            technique for solving classification tasks on graphs. This survey
	            gives a comprehensive overview of techniques for kernel-based
	            graph classification developed in the past 15 years. We describe
	            and categorize graph kernels based on properties inherent to
	            their design, such as the nature of their extracted graph
	            features, their method of computation and their applicability to
	            problems in practice. In an extensive experimental evaluation, we
	            study the classification accuracy of a large suite of graph
	            kernels on established benchmarks as well as new datasets. We
	            compare the performance of popular kernels with several baseline
	            methods and study the effect of applying a Gaussian {RBF} kernel
	            to the metric induced by a graph kernel. In doing so, we find
	            that simple baselines become competitive after this
	            transformation on some datasets. Moreover, we study the extent to
	            which existing graph kernels agree in their predictions (and
	            prediction errors) and obtain a data-driven categorization of
	            kernels as result. Finally, based on our experimental results, we
	            derive a practitioner’s guide to kernel-based graph
	            classification.},
	file = {:surveygraphkernels_Kriege2020 - A Survey on Graph Kernels.pdf:PDF},
	groups = {laplacian-systems},
	howpublished = {{ReviewPaper}},
	langid = {english},
	priority = {prio2},
	publisher = {{SpringerOpen}},
	rights = {2019 The Author(s)},
	shortjournal = {Appl Netw Sci},
	type = {{ReviewPaper}},
}

@inproceedings{MeasuringProximityAttributed_Aynulin2021,
	author = {Aynulin, Rinat and Chebotarev, Pavel},
	booktitle = {Complex Networks \& Their Applications {IX}},
	date = {2021},
	title = {Measuring Proximity in Attributed Networks for Community Detection},
	doi = {10.1007/978-3-030-65347-7_3},
	editor = {Benito, Rosa M. and Cherifi, Chantal and Cherifi, Hocine and Moro,
	          Esteban and Rocha, Luis Mateus and Sales-Pardo, Marta},
	isbn = {9783030653477},
	location = {Cham},
	pages = {27--37},
	publisher = {Springer International Publishing},
	series = {Studies in Computational Intelligence},
	abstract = {Proximity measures on graphs have a variety of applications in
	            network analysis, including community detection. Previously they
	            have been mainly studied in the context of networks without
	            attributes. If node attributes are taken into account, however,
	            this can provide more insight into the network structure. In this
	            paper, we extend the definition of some well-studied proximity
	            measures to attributed networks. To account for attributes,
	            several attribute similarity measures are used. Finally, the
	            obtained proximity measures are applied to detect the community
	            structure in some real-world networks using the spectral
	            clustering algorithm.},
	file = {:MeasuringProximityAttributed_Aynulin2021 - Measuring Proximity in
	        Attributed Networks for Community Detection.html:URL},
	groups = {laplacian-systems},
	keywords = {Attributed networks, Community detection, Proximity measure,
	            Kernel on graph},
	langid = {english},
}

@report{Hittingcommutetimes_Luxburg2011,
	author = {von Luxburg, Ulrike and Radl, Agnes and Hein, Matthias},
	date = {2011-05-26},
	institution = {{arXiv}},
	title = {Hitting and commute times in large graphs are often misleading},
	doi = {10.48550/arXiv.1003.1266},
	eprint = {1003.1266},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:1003.1266},
	abstract = {Next to the shortest path distance, the second most popular
	            distance function between vertices in a graph is the commute
	            distance (resistance distance). For two vertices u and v, the
	            hitting time H\_\{uv\} is the expected time it takes a random
	            walk to travel from u to v. The commute time is its symmetrized
	            version C\_\{uv\} = H\_\{uv\} + H\_\{vu\}. In our paper we study
	            the behavior of hitting times and commute distances when the
	            number n of vertices in the graph is very large. We prove that as
	            n converges to infinty, hitting times and commute distances
	            converge to expressions that do not take into account the global
	            structure of the graph at all. Namely, the hitting time H\_\{uv\}
	            converges to 1/d\_v and the commute time to 1/d\_u + 1/d\_v where
	            d\_u and d\_v denote the degrees of vertices u and v. In these
	            cases, the hitting and commute times are misleading in the sense
	            that they do not provide information about the structure of the
	            graph. We focus on two major classes of random graphs: random
	            geometric graphs (k-nearest neighbor graphs, epsilon-graphs,
	            Gaussian similarity graphs) and random graphs with given expected
	            degrees (in particular, Erdos-Renyi graphs with and without
	            planted partitions)},
	file = {:Hittingcommutetimes_Luxburg2011 - Hitting and Commute Times in
	        Large Graphs Are Often Misleading.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {Computer Science - Data Structures and Algorithms, Computer
	            Science - Machine Learning, Mathematics - Probability},
}

@article{MoorePenroseinverse_Bozzo2013,
	author = {Enrico Bozzo},
	date = {2013},
	title = {The Moore-Penrose inverse of the normalized graph Laplacian},
	doi = {10.1016/j.laa.2013.08.039},
	issn = {0024-3795},
	pages = {3038--3043},
	url = {https://www.sciencedirect.com/science/article/pii/S0024379513005582},
	volume = {439},
	accessdate = {2023-02-01},
	groups = {laplacian-systems},
}

@article{HyperbolicVectorRandom_Du2012,
	author = {Juan Du and Nikolai Leonenko and Chunsheng Ma and Hong Shu},
	date = {2012},
	title = {Hyperbolic Vector Random Fields with Hyperbolic Direct and Cross
	         Covariance Functions},
	doi = {10.1080/07362994.2012.684325},
	issn = {0736-2994},
	pages = {662--674},
	volume = {30},
	accessdate = {2023-02-01},
	file = {:HyperbolicVectorRandom_Du2012 - Hyperbolic Vector Random Fields
	        with Hyperbolic Direct and Cross Covariance Functions.html:URL},
	groups = {hyperbolic trees},
	priority = {prio3},
}

@article{Mahalanobisdistanceinformed_Lahav2019,
	author = {Almog Lahav and Ronen Talmon and Yuval Kluger},
	date = {2019},
	title = {Mahalanobis distance informed by clustering},
	doi = {10.1093/imaiai/iay011},
	eprint = {1708.03914},
	eprinttype = {arxiv},
	issn = {2049-8772},
	pages = {377--406},
	volume = {8},
	accessdate = {2023-02-01},
	file = {:Mahalanobisdistanceinformed_Lahav2019 - Mahalanobis Distance
	        Informed by Clustering.pdf:PDF},
	groups = {hyperbolic trees},
}

@inproceedings{MathematicalfoundationsGraphBLAS_Kepner2016,
	author = {Kepner, Jeremy and Aaltonen, Peter and Bader, David and Buluç,
	          Aydin and Franchetti, Franz and Gilbert, John and Hutchison, Dylan
	          and Kumar, Manoj and Lumsdaine, Andrew and Meyerhenke, Henning and
	          {McMillan}, Scott and Yang, Carl and Owens, John D. and Zalewski,
	          Marcin and Mattson, Timothy and Moreira, Jose},
	booktitle = {2016 {IEEE} High Performance Extreme Computing Conference ({
	             HPEC})},
	date = {2016-09},
	title = {Mathematical foundations of the {GraphBLAS}},
	doi = {10.1109/HPEC.2016.7761646},
	eventtitle = {2016 {IEEE} High Performance Extreme Computing Conference ({
	              HPEC})},
	pages = {1--9},
	abstract = {The {GraphBLAS} standard ({GraphBlas}.org) is being developed to
	            bring the potential of matrix-based graph algorithms to the
	            broadest possible audience. Mathematically, the {GraphBLAS}
	            defines a core set of matrix-based graph operations that can be
	            used to implement a wide class of graph algorithms in a wide
	            range of programming environments. This paper provides an
	            introduction to the mathematics of the {GraphBLAS}. Graphs
	            represent connections between vertices with edges. Matrices can
	            represent a wide range of graphs using adjacency matrices or
	            incidence matrices. Adjacency matrices are often easier to
	            analyze while incidence matrices are often better for
	            representing data. Fortunately, the two are easily connected by
	            matrix multiplication. A key feature of matrix mathematics is
	            that a very small number of matrix operations can be used to
	            manipulate a very wide range of graphs. This composability of a
	            small number of operations is the foundation of the {GraphBLAS}.
	            A standard such as the {GraphBLAS} can only be effective if it
	            has low performance overhead. Performance measurements of
	            prototype {GraphBLAS} implementations indicate that the overhead
	            is low.},
	file = {:MathematicalfoundationsGraphBLAS_Kepner2016 - Mathematical
	        Foundations of the GraphBLAS.pdf:PDF},
	groups = {incidence structures},
	keywords = {Matrices, Sparse matrices, Finite element analysis, Standards,
	            Additives},
	priority = {prio2},
}

@article{Annotatedhypergraphsmodels_Chodrow2020,
	author = {Chodrow, Philip and Mellor, Andrew},
	date = {2020-01-30},
	journaltitle = {Applied Network Science},
	title = {Annotated hypergraphs: models and applications},
	doi = {10.1007/s41109-020-0252-y},
	issn = {2364-8228},
	number = {1},
	pages = {9},
	urldate = {2023-02-01},
	volume = {5},
	abstract = {Hypergraphs offer a natural modeling language for studying
	            polyadic interactions between sets of entities. Many polyadic
	            interactions are asymmetric, with nodes playing distinctive
	            roles. In an academic collaboration network, for example, the
	            order of authors on a paper often reflects the nature of their
	            contributions to the completed work. To model these networks, we
	            introduce annotated hypergraphs as natural polyadic
	            generalizations of directed graphs. Annotated hypergraphs form a
	            highly general framework for incorporating metadata into polyadic
	            graph models. To facilitate data analysis with annotated
	            hypergraphs, we construct a role-aware configuration null model
	            for these structures and prove an efficient Markov Chain Monte
	            Carlo scheme for sampling from it. We proceed to formulate
	            several metrics and algorithms for the analysis of annotated
	            hypergraphs. Several of these, such as assortativity and
	            modularity, naturally generalize dyadic counterparts. Other
	            metrics, such as local role densities, are unique to the setting
	            of annotated hypergraphs. We illustrate our techniques on six
	            digital social networks, and present a detailed case-study of the
	            Enron email data set.},
	file = {:Annotatedhypergraphsmodels_Chodrow2020 - Annotated Hypergraphs_
	        Models and Applications.pdf:PDF},
	groups = {incidence structures},
	keywords = {Hypergraphs, Null models, Network science, Statistical inference
	            , Community detection},
	langid = {english},
	priority = {prio1},
	shortjournal = {Appl Netw Sci},
	shorttitle = {Annotated hypergraphs},
}

@article{WhyHowWhen_Torres2021,
	author = {Torres, Leo and Blevins, Ann S. and Bassett, Danielle and
	          Eliassi-Rad, Tina},
	date = {2021-01},
	journaltitle = {{SIAM} Review},
	title = {The Why, How, and When of Representations for Complex Systems},
	doi = {10.1137/20M1355896},
	issn = {0036-1445},
	number = {3},
	pages = {435--485},
	urldate = {2023-02-01},
	volume = {63},
	abstract = {Complex systems, composed at the most basic level of units and
	            their interactions, describe phenomena in a wide variety of
	            domains, from neuroscience to computer science and economics. The
	            wide variety of applications has resulted in two key challenges:
	            the generation of many domain-specific strategies for complex
	            systems analyses that are seldom revisited, and the
	            compartmentalization of representation and analysis ideas within
	            a domain due to inconsistency in complex systems language. In
	            this work we propose basic, domain-agnostic language in order to
	            advance toward a more cohesive vocabulary. We use this language
	            to evaluate each step of the complex systems analysis pipeline,
	            beginning with the system under study and data collected, then
	            moving through different mathematical frameworks for encoding the
	            observed data (i.e., graphs, simplicial complexes, and
	            hypergraphs), and relevant computational methods for each
	            framework. At each step we consider different types of
	            dependencies; these are properties of the system that describe
	            how the existence of an interaction among a set of units in a
	            system may affect the possibility of the existence of another
	            relation. We discuss how dependencies may arise and how they may
	            alter the interpretation of results or the entirety of the
	            analysis pipeline. We close with two real-world examples using
	            coauthorship data and email communications data that illustrate
	            how the system under study, the dependencies therein, the
	            research question, and the choice of mathematical representation
	            influence the results. We hope this work can serve as an
	            opportunity for reflection for experienced complex systems
	            scientists, as well as an introductory resource for new
	            researchers.},
	file = {:WhyHowWhen_Torres2021 - The Why, How, and When of Representations
	        for Complex Systems.pdf:PDF},
	groups = {incidence structures},
	keywords = {complex systems, dependencies, graph theory, simplicial
	            complexes, hypergraphs, 00-02, 00A69, 00A71},
	priority = {prio1},
	publisher = {Society for Industrial and Applied Mathematics},
	ranking = {rank5},
	shortjournal = {{SIAM} Rev.},
}

@report{Notjustprogrammers_CrystalOrnelas2023,
	author = {Crystal-Ornelas, Robert and Edwards, Brandon P. M. and Hébert,
	          Katherine and Hudgins, Emma J. and Reyes, Luna L. Sánchez and Scott
	          , Eric R. and Grainger, Matthew J. and Foroughirad, Vivienne and
	          Binley, Allison D. and Brookson, Cole B. and Gaynor, Kaitlyn M. and
	          Sabet, Saeed Shafiei and Güncan, Ali and Hillemann, Friederike and
	          Weierbach, Helen and Gomes, Dylan G. E. and Braga, Pedro Henrique
	          Pereira},
	date = {2023-01-30},
	institution = {Manubot},
	title = {Not just for programmers: How {GitHub} can accelerate collaborative
	         and reproducible research in ecology and evolution},
	url = {https://SORTEE-Github-Hackathon.github.io/manuscript/},
	urldate = {2023-02-01},
	file = {:Notjustprogrammers_CrystalOrnelas2023 - Not Just for Programmers_
	        How GitHub Can Accelerate Collaborative and Reproducible Research in
	        Ecology and Evolution.pdf:PDF},
	groups = {Human Factors},
	langid = {american},
	shorttitle = {Not just for programmers},
}

@article{DiscreteGreensFunctions_Chung2000,
	author = {Chung, Fan and Yau, S. -T.},
	date = {2000-07-01},
	journaltitle = {Journal of Combinatorial Theory, Series A},
	title = {Discrete Green's Functions},
	doi = {10.1006/jcta.2000.3094},
	issn = {0097-3165},
	number = {1},
	pages = {191--214},
	url = {https://www.sciencedirect.com/science/article/pii/S0097316500930942},
	urldate = {2023-02-01},
	volume = {91},
	abstract = {We study discrete Green's functions and their relationship with
	            discrete Laplace equations. Several methods for deriving Green's
	            functions are discussed. Green's functions can be used to deal
	            with diffusion-type problems on graphs, such as chip-firing, load
	            balancing, and discrete Markov chains.},
	file = {:DiscreteGreensFunctions_Chung2000 - Discrete Green's
	        Functions.html:URL},
	groups = {laplacian-systems},
	langid = {english},
	shortjournal = {Journal of Combinatorial Theory, Series A},
}

@inproceedings{RepresentationTradeoffsHyperbolic_Sala2018,
	author = {Sala, Frederic and Sa, Chris De and Gu, Albert and Re, Christopher
	          },
	date = {2018-07-03},
	title = {Representation Tradeoffs for Hyperbolic Embeddings},
	eventtitle = {International Conference on Machine Learning},
	pages = {4460--4469},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v80/sala18a.html},
	urldate = {2023-02-01},
	abstract = {Hyperbolic embeddings offer excellent quality with few
	            dimensions when embedding hierarchical data structures. We give a
	            combinatorial construction that embeds trees into hyperbolic
	            space with arbitrarily low distortion without optimization. On {
	            WordNet}, this algorithm obtains a mean-average-precision of
	            0.989 with only two dimensions, outperforming existing work by
	            0.11 points. We provide bounds characterizing the
	            precision-dimensionality tradeoff inherent in any hyperbolic
	            embedding. To embed general metric spaces, we propose a
	            hyperbolic generalization of multidimensional scaling (h-{MDS}).
	            We show how to perform exact recovery of hyperbolic points from
	            distances, provide a perturbation analysis, and give a recovery
	            result that enables us to reduce dimensionality. Finally, we
	            extract lessons from the algorithms and theory above to design a
	            scalable {PyTorch}-based implementation that can handle
	            incomplete information.},
	file = {:pdfs/sala_representation_2018 - Representation Tradeoffs for
	        Hyperbolic
	        Embeddings.pdf:PDF;:RepresentationTradeoffsHyperbolic_Sala2018 -
	        Representation Tradeoffs for Hyperbolic Embeddings.pdf:PDF},
	groups = {hyperbolic trees},
	issn = {2640-3498},
	langid = {english},
	priority = {prio3},
}

@article{Socialcentralizationsemantic_Linzhuo2020,
	author = {Linzhuo, Li and Lingfei, Wu and James, Evans},
	date = {2020-02-01},
	journaltitle = {Poetics},
	title = {Social centralization and semantic collapse: Hyperbolic embeddings
	         of networks and text},
	doi = {10.1016/j.poetic.2019.101428},
	issn = {0304-422X},
	pages = {101428},
	series = {Discourse, Meaning, and Networks: Advances in Socio-Semantic
	          Analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0304422X1830295X},
	urldate = {2023-02-01},
	volume = {78},
	abstract = {Modern advances in transportation and communication technology
	            from airplanes to the internet alongside global expansions of
	            media, migration, and trade have made the modern world more
	            connected than ever before. But what does this bode for the
	            convergence of global culture? Here we explore the relationship
	            between centralization in social networks and contraction or
	            collapse in the diversity of semantic expressions such as ideas,
	            opinions and tastes. We advance formal examination of this
	            relationship by introducing new methods of manifold learning that
	            allow us to map social networks and semantic combinations into
	            comparable hyperbolic spaces. Hyperbolic representations natively
	            represent both hierarchy and diversity within a system. In a
	            Poincaré disk—a two-dimensional hyperbolic embedding—radius from
	            center traces the position of an actor in a social hierarchy or
	            an idea in a semantic hierarchy. Angle of the disk required to
	            inscribe connected actors or ideas captures their diversity. We
	            illustrate this method by examining the relationship between
	            social centralization and semantic diversity within 21st Century
	            physics, empirically demonstrating how dense, centralized
	            collaboration is associated with a reduction in the space of
	            ideas and how these patterns generalize to all modern scholarship
	            and science. We discuss the complex of causes underlying this
	            association, and theorize the dynamic interplay between
	            structural centralization and semantic contraction, arguing that
	            it introduces an essential tension between the supply and demand
	            of difference.},
	file = {:Socialcentralizationsemantic_Linzhuo2020 - Social Centralization
	        and Semantic Collapse_ Hyperbolic Embeddings of Networks and
	        Text.html:URL},
	groups = {hyperbolic trees},
	keywords = {Social networks, Semantic networks, Auto-encoders, Unsupervised
	            learning, Hyperbolic embedding, Poincaré disk, Machine learning,
	            Science of science},
	langid = {english},
	priority = {prio3},
	shortjournal = {Poetics},
	shorttitle = {Social centralization and semantic collapse},
}

@report{NeuralEmbeddingsGraphs_Chamberlain2017,
	author = {Chamberlain, Benjamin Paul and Clough, James and Deisenroth, Marc
	          Peter},
	date = {2017-05-29},
	institution = {{arXiv}},
	title = {Neural Embeddings of Graphs in Hyperbolic Space},
	doi = {10.48550/arXiv.1705.10359},
	eprint = {1705.10359},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:1705.10359},
	abstract = {Neural embeddings have been used with great success in Natural
	            Language Processing ({NLP}). They provide compact representations
	            that encapsulate word similarity and attain state-of-the-art
	            performance in a range of linguistic tasks. The success of neural
	            embeddings has prompted significant amounts of research into
	            applications in domains other than language. One such domain is
	            graph-structured data, where embeddings of vertices can be
	            learned that encapsulate vertex similarity and improve
	            performance on tasks including edge prediction and vertex
	            labelling. For both {NLP} and graph based tasks, embeddings have
	            been learned in high-dimensional Euclidean spaces. However,
	            recent work has shown that the appropriate isometric space for
	            embedding complex networks is not the flat Euclidean space, but
	            negatively curved, hyperbolic space. We present a new concept
	            that exploits these recent insights and propose learning neural
	            embeddings of graphs in hyperbolic space. We provide experimental
	            evidence that embedding graphs in their natural geometry
	            significantly improves performance on downstream tasks for
	            several real-world public datasets.},
	file = {:NeuralEmbeddingsGraphs_Chamberlain2017 - Neural Embeddings of
	        Graphs in Hyperbolic Space.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Statistics - Machine Learning, Computer Science - Machine
	            Learning},
}

@inproceedings{ActiveLearningStrategies_Esuli2009,
	author = {Esuli, Andrea and Sebastiani, Fabrizio},
	booktitle = {Advances in Information Retrieval},
	date = {2009},
	title = {Active Learning Strategies for Multi-Label Text Classification},
	doi = {10.1007/978-3-642-00958-7_12},
	editor = {Boughanem, Mohand and Berrut, Catherine and Mothe, Josiane and
	          Soule-Dupuy, Chantal},
	isbn = {9783642009587},
	location = {Berlin, Heidelberg},
	pages = {102--113},
	publisher = {Springer},
	series = {Lecture Notes in Computer Science},
	abstract = {Active learning refers to the task of devising a ranking
	            function that, given a classifier trained from relatively few
	            training examples, ranks a set of additional unlabeled examples
	            in terms of how much further information they would carry, once
	            manually labeled, for retraining a (hopefully) better classifier.
	            Research on active learning in text classification has so far
	            concentrated on single-label classification; active learning for
	            multi-label classification, instead, has either been tackled in a
	            simulated (and, we contend, non-realistic) way, or neglected tout
	            court. In this paper we aim to fill this gap by examining a
	            number of realistic strategies for tackling active learning for
	            multi-label classification. Each such strategy consists of a rule
	            for combining the outputs returned by the individual binary
	            classifiers as a result of classifying a given unlabeled
	            document. We present the results of extensive experiments in
	            which we test these strategies on two standard text
	            classification datasets.},
	file = {:ActiveLearningStrategies_Esuli2009 - Active Learning Strategies for
	        Multi Label Text Classification.html:URL},
	groups = {Research Topics, active-learning, text-as-data},
	langid = {english},
}

@incollection{EmpiricalComparisonGraph_Boman2015,
	author = {Boman, Erik G. and Deweese, Kevin and Gilbert, John R.},
	date = {2015-12-30},
	title = {An Empirical Comparison of Graph Laplacian Solvers},
	doi = {10.1137/1.9781611974317.15},
	isbn = {9781611974317},
	pages = {174--188},
	publisher = {Society for Industrial and Applied Mathematics},
	series = {Proceedings},
	urldate = {2023-02-01},
	abstract = {Laplacian matrices of graphs arise in large-scale computational
	            applications such as semisupervised machine learning; spectral
	            clustering of images, genetic data, and web pages; transportation
	            network flows; electrical resistor circuits; and elliptic partial
	            differential equations discretized on unstructured grids with
	            finite elements. A lean algebraic multigrid ({LAMG}) solver of
	            the symmetric linear system \$Ax=b\$ is presented, where \$A\$ is
	            a graph Laplacian. {LAMG}'s run time and storage are empirically
	            demonstrated to scale linearly with the number of edges. {LAMG}
	            consists of a setup phase, during which a sequence of
	            increasingly coarser Laplacian systems is constructed, and an
	            iterative solve phase using multigrid cycles. General graphs pose
	            algorithmic challenges not encountered in traditional multigrid
	            applications. {LAMG} combines a lean piecewise-constant
	            interpolation, judicious node aggregation based on a new node
	            proximity measure (the affinity), and an energy correction of
	            coarse-level systems. This results in fast convergence and
	            substantial setup and memory savings. A serial {LAMG}
	            implementation scaled linearly for a diverse set of 3774
	            real-world graphs with up to 47 million edges, with no parameter
	            tuning. {LAMG} was more robust than the {UMFPACK} direct solver
	            and combinatorial multigrid ({CMG}), although {CMG} was faster
	            than {LAMG} on average. Our methodology is extensible to
	            eigenproblems and other graph computations.},
	file = {:Boman2015 - An Empirical Comparison of Graph Laplacian
	        Solvers.pdf:PDF},
	groups = {laplacian-systems},
	langid = {english},
	priority = {prio3},
}

@inproceedings{spectralmatchingalgorithm_Bao2017,
	author = {Bao, Wen-xia and Yu, Guo-fen and Hu, Gen-sheng and Liang, Dong and
	          Yan, Shao-mei},
	booktitle = {2017 10th International Congress on Image and Signal Processing
	             , {BioMedical} Engineering and Informatics ({CISP}-{BMEI})},
	date = {2017-10},
	title = {The spectral matching algorithm based on hyperbolic mahalanobis
	         metric},
	doi = {10.1109/CISP-BMEI.2017.8302019},
	eventtitle = {2017 10th International Congress on Image and Signal
	              Processing, {BioMedical} Engineering and Informatics ({CISP}-{
	              BMEI})},
	pages = {1--6},
	abstract = {In order to solve the problem of the traditional image matching
	            algorithm based on the spectral feature, the spectral matching
	            algorithm based on hyperbolic mahalanobis metric is proposed in
	            this paper. The algorithm first introduces a hyperbolic metric
	            that has better adaptability to the sample data. And the
	            hyperbolic mahalanobis metric is defined according to the
	            statistical properties of the data. For a point in a given
	            pointset, the sub point-set is selected according to the
	            hyperbolic mahalanobis metric and the weighted graph of the sub
	            point-set is constructed. The eigenvalue vector and the spectral
	            gap vector are obtained by the singular value decomposition ({SVD
	            }) of the adjacency matrix of the weighted graph, which construct
	            the hyperbolic mahalanobis metric spectral feature. Finally, the
	            matching matrix is constructed based on the similarity between
	            the hyperbolic mahalanobis metric spectral feature and geometric
	            relations between feature points. Thereby establish the matching
	            mathematical model and introduce the greedy algorithm to obtain
	            the matching results. A large number of experimental results show
	            that the proposed algorithm improves the matching accuracy and
	            the robustness. And the algorithm extends the application range
	            of the spectral matching algorithm.},
	file = {:spectralmatchingalgorithm_Bao2017 - The Spectral Matching Algorithm
	        Based on Hyperbolic Mahalanobis Metric.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {Measurement, Signal processing algorithms, Matrix decomposition,
	            Feature extraction, Eigenvalues and eigenfunctions, Geometry,
	            Manganese, image matching, hyperbolic mahalanobis metric,
	            spectral feature},
	priority = {prio2},
}

@inproceedings{Classificationmixturescurved_Nielsen2016,
	author = {Nielsen, Frank and Muzellec, Boris and Nock, Richard},
	booktitle = {2016 {IEEE} International Conference on Image Processing ({ICIP
	             })},
	date = {2016-09},
	title = {Classification with mixtures of curved mahalanobis metrics},
	doi = {10.1109/ICIP.2016.7532355},
	eventtitle = {2016 {IEEE} International Conference on Image Processing ({
	              ICIP})},
	note = {{ISSN}: 2381-8549},
	pages = {241--245},
	abstract = {We study the classification with respect to the class of curved
	            Mahalanobis metrics that extend the celebrated flat Mahalanobis
	            distances to constant curvature spaces. We prove that these
	            curved Mahalanobis k-{NN} classifiers define piecewise linear
	            decision boundaries, and report the performance of learning those
	            metrics within the framework of the Large Margin Nearest Neighbor
	            ({LMNN}). Finally, we show experimentally that a mixture of
	            curved Mahalanobis metrics define a composite metric distance
	            that improves the classification performance.},
	file = {:Classificationmixturescurved_Nielsen2016 - Classification with
	        Mixtures of Curved Mahalanobis Metrics.pdf:PDF},
	groups = {laplacian-systems},
	issn = {2381-8549},
	keywords = {Geometry, Matrix decomposition, Euclidean distance, Symmetric
	            matrices, Shape, Covariance matrices, Classification, Mahalanobis
	            distance, metric learning, Large Margin Nearest Neighbor ({LMNN})
	            , Cayley-Klein geometry},
	priority = {prio1},
}

@inproceedings{MahalanobisMetricCayley_Bi2015,
	author = {Bi, Yanhong and Fan, Bin and Wu, Fuchao},
	date = {2015},
	title = {Beyond Mahalanobis Metric: Cayley-Klein Metric Learning},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and
	              Pattern Recognition},
	pages = {2339--2347},
	url = {
	       https://openaccess.thecvf.com/content_cvpr_2015/html/Bi_Beyond_Mahalanobis_Metric_2015_CVPR_paper.html
	       },
	urldate = {2023-02-03},
	file = {:MahalanobisMetricCayley_Bi2015 - Beyond Mahalanobis Metric_ Cayley
	        Klein Metric Learning.pdf:PDF},
	groups = {laplacian-systems},
	priority = {prio3},
	shorttitle = {Beyond Mahalanobis Metric},
}

@inproceedings{LearningIsingmodels_Dagan2021,
	author = {Dagan, Yuval and Daskalakis, Constantinos and Dikkala, Nishanth
	          and Kandiros, Anthimos Vardis},
	booktitle = {Proceedings of the 53rd Annual {ACM} {SIGACT} Symposium on
	             Theory of Computing},
	date = {2021-06-15},
	title = {Learning Ising models from one or multiple samples},
	doi = {10.1145/3406325.3451074},
	isbn = {9781450380539},
	location = {New York, {NY}, {USA}},
	pages = {161--168},
	publisher = {Association for Computing Machinery},
	series = {{STOC} 2021},
	urldate = {2023-02-08},
	abstract = {There have been two main lines of work on estimating Ising
	            models: (1) estimating them from multiple independent samples
	            under minimal assumptions about the model's interaction matrix ;
	            and (2) estimating them from one sample in restrictive settings.
	            We propose a unified framework that smoothly interpolates between
	            these two settings, enabling significantly richer estimation
	            guarantees from one, a few, or many samples. Our main theorem
	            provides guarantees for one-sample estimation, quantifying the
	            estimation error in terms of the metric entropy of a family of
	            interaction matrices. As corollaries of our main theorem, we
	            derive bounds when the model's interaction matrix is a (sparse)
	            linear combination of known matrices, or it belongs to a finite
	            set, or to a high-dimensional manifold. In fact, our main result
	            handles multiple independent samples by viewing them as one
	            sample from a larger model, and can be used to derive estimation
	            bounds that are qualitatively similar to those obtained in the
	            afore-described multiple-sample literature. Our technical
	            approach benefits from sparsifying a model's interaction network,
	            conditioning on subsets of variables that make the dependencies
	            in the resulting conditional distribution sufficiently weak. We
	            use this sparsification technique to prove strong concentration
	            and anti-concentration results for the Ising model, which we
	            believe have applications beyond the scope of this paper.},
	file = {:LearningIsingmodels_Dagan2021 - Learning Ising Models from One or
	        Multiple Samples.pdf:PDF},
	groups = {graphical-models},
	keywords = {Dependent Data, Maximum Likelihood Estimation, Ising model,
	            Single-Sample Estimation, Low temperature, Concentration
	            Inequalities},
}

@incollection{FastGenerationRandom_Madry2014,
	author = {Madry, Aleksander and Straszak, Damian and Tarnawski, Jakub},
	date = {2014-12-22},
	title = {Fast Generation of Random Spanning Trees and the Effective
	         Resistance Metric},
	doi = {10.1137/1.9781611973730.134},
	isbn = {9781611973747},
	pages = {2019--2036},
	publisher = {Society for Industrial and Applied Mathematics},
	series = {Proceedings},
	urldate = {2023-02-08},
	abstract = {We present a nearly linear time algorithm that produces
	            high-quality spectral sparsifiers of weighted graphs. Given as
	            input a weighted graph \$G=(V,E,w)\$ and a parameter \${
	            \textbackslash}epsilon{\textgreater}0\$, we produce a weighted
	            subgraph \$H=(V,{\textbackslash}tilde\{E\},{\textbackslash}tilde
	            \{w\})\$ of G such that \${\textbar}{\textbackslash}tilde\{E\}{
	            \textbar}=O(n{\textbackslash}log n/{\textbackslash}epsilon{
	            \textasciicircum}2)\$ and all \$x{\textbackslash}in{
	            \textbackslash}mathbb\{R\}{\textasciicircum}V\$ satisfy \$(1-{
	            \textbackslash}epsilon){\textbackslash}sum\_\{uv{\textbackslash}
	            in E\}{\textbackslash},(x(u)-x(v)){\textasciicircum}2w\_\{uv\}{
	            \textbackslash}leq{\textbackslash}sum\_\{uv{\textbackslash}in{
	            \textbackslash}tilde\{E\}\}{\textbackslash},(x(u)-x(v)){
	            \textasciicircum}2{\textbackslash}tilde\{w\}\_\{uv\}{
	            \textbackslash}leq(1+{\textbackslash}epsilon){\textbackslash}sum
	            \_\{uv{\textbackslash}in E\}{\textbackslash},(x(u)-x(v)){
	            \textasciicircum}2w\_\{uv\}\$. This improves upon the spectral
	            sparsifiers constructed by Spielman and Teng, which had \$O(n{
	            \textbackslash}log{\textasciicircum}\{c\}n)\$ edges for some
	            large constant c, and upon the cut sparsifiers of Benczúr and
	            Karger, which only satisfied these inequalities for \$x{
	            \textbackslash}in{\textbackslash}\{0,1{\textbackslash}\}{
	            \textasciicircum}V\$. A key ingredient in our algorithm is a
	            subroutine of independent interest: a nearly linear time
	            algorithm that builds a data structure from which we can query
	            the approximate effective resistance between any two vertices in
	            a graph in \$O({\textbackslash}log n)\$ time.},
	file = {:FastGenerationRandom_Madry2014 - Fast Generation of Random Spanning
	        Trees and the Effective Resistance Metric.pdf:PDF},
	groups = {laplacian-systems},
	priority = {prio1},
}

@inproceedings{Linkpredictionincidence_Yokoi2016,
	author = {Yokoi, Sho and Kajino, Hiroshi and Kashima, Hisashi},
	booktitle = {Proceedings of the Twenty-second European Conference on
	             Artificial Intelligence},
	date = {2016-08-29},
	title = {Link prediction by incidence matrix factorization},
	doi = {10.3233/978-1-61499-672-9-1730},
	isbn = {9781614996712},
	location = {{NLD}},
	pages = {1730--1731},
	publisher = {{IOS} Press},
	series = {{ECAI}'16},
	urldate = {2023-02-08},
	abstract = {Link prediction suffers from the data sparsity problem. This
	            paper presents and validates our hypothesis that, for sparse
	            networks, incidence matrix factorization ({IMF}) could perform
	            better than adjacency matrix factorization ({AMF}), which has
	            been used in many previous studies. A key observation supporting
	            the hypothesis is that {IMF} models a partially-observed graph
	            more accurately than {AMF}. A technical challenge for validating
	            our hypothesis is that, unlike {AMF} approach, there does not
	            exist an obvious method to make predictions using a factorized
	            incidence matrix. To this end, we newly develop an
	            optimization-based link prediction method adopting {IMF}. We have
	            conducted thorough experiments using synthetic and real-world
	            datasets to investigate the relationship between the sparsity of
	            a network and the performance of the aforementioned two methods.
	            The experimental results show that {IMF} performs better than {
	            AMF} as networks become sparser, which strongly validates our
	            hypothesis.},
	file = {:Linkpredictionincidence_Yokoi2016 - Link Prediction by Incidence
	        Matrix Factorization.pdf:PDF},
	groups = {incidence structures},
	priority = {prio3},
}

@article{Dependencynetworksinference_Chickering2000,
	author = {Chickering, David and Maxwell, David and Meek, Christopher and
	          Rounthwaite, Robert and Kadie, Carl},
	date = {2000},
	journaltitle = {Journal of Machine Learning Research},
	title = {Dependency networks for inference, collaborative filtering, and
	         data visualization},
	number = {Oct},
	pages = {49--75},
	volume = {1},
	file = {:Dependencynetworksinference_Chickering2000 - Dependency Networks
	        for Inference, Collaborative Filtering, and Data
	        Visualization.pdf:PDF},
	groups = {graphical-models},
	priority = {prio2},
}

@article{Highdimensionalgraphs_Meinshausen2006,
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	date = {2006-06},
	journaltitle = {The Annals of Statistics},
	title = {High-dimensional graphs and variable selection with the Lasso},
	doi = {10.1214/009053606000000281},
	issn = {0090-5364, 2168-8966},
	number = {3},
	pages = {1436--1462},
	urldate = {2023-02-08},
	volume = {34},
	abstract = {The pattern of zero entries in the inverse covariance matrix of
	            a multivariate normal distribution corresponds to conditional
	            independence restrictions between variables. Covariance selection
	            aims at estimating those structural zeros from data. We show that
	            neighborhood selection with the Lasso is a computationally
	            attractive alternative to standard covariance selection for
	            sparse high-dimensional graphs. Neighborhood selection estimates
	            the conditional independence restrictions separately for each
	            node in the graph and is hence equivalent to variable selection
	            for Gaussian linear models. We show that the proposed
	            neighborhood selection scheme is consistent for sparse
	            high-dimensional graphs. Consistency hinges on the choice of the
	            penalty parameter. The oracle value for optimal prediction does
	            not lead to a consistent neighborhood estimate. Controlling
	            instead the probability of falsely joining some distinct
	            connectivity components of the graph, consistent estimation for
	            sparse graphs is achieved (with exponential rates), even when the
	            number of variables grows as the number of observations raised to
	            an arbitrary power.},
	file = {:Highdimensionalgraphs_Meinshausen2006 - High Dimensional Graphs and
	        Variable Selection with the Lasso.pdf:PDF},
	groups = {graphical-models},
	keywords = {62F12, 62H20, 62J07, covariance selection, Gaussian graphical
	            models, Linear regression, penalized regression},
	priority = {prio2},
	publisher = {Institute of Mathematical Statistics},
}

@inproceedings{HandlingSparsityvia_Carvalho2009,
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	date = {2009-04-15},
	title = {Handling Sparsity via the Horseshoe},
	eventtitle = {Artificial Intelligence and Statistics},
	pages = {73--80},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v5/carvalho09a.html},
	urldate = {2023-02-08},
	abstract = {This paper presents a general, fully Bayesian framework for
	            sparse supervised-learning problems based on the horseshoe prior.
	            The horseshoe prior is a member of the family of multivariate
	            scale mixtures of normals, and is therefore closely related to
	            widely used approaches for sparse Bayesian learning, including,
	            among others, Laplacian priors (e.g. the {LASSO}) and Student-t
	            priors (e.g. the relevance vector machine). The advantages of the
	            horseshoe are its robustness at handling unknown sparsity and
	            large outlying signals. These properties are justifed
	            theoretically via a representation theorem and accompanied by
	            comprehensive empirical experiments that compare its performance
	            to benchmark alternatives.},
	file = {:HandlingSparsityvia_Carvalho2009 - Handling Sparsity Via the
	        Horseshoe.pdf:PDF},
	groups = {graphical-models},
	issn = {1938-7228},
	langid = {english},
	priority = {prio3},
}

@inproceedings{CombinatorialBayesianOptimization_Oh2019,
	author = {Oh, Changyong and Tomczak, Jakub and Gavves, Efstratios and
	          Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	date = {2019},
	title = {Combinatorial Bayesian Optimization using the Graph Cartesian
	         Product},
	publisher = {Curran Associates, Inc.},
	url = {
	       https://proceedings.neurips.cc/paper/2019/hash/2cb6b10338a7fc4117a80da24b582060-Abstract.html
	       },
	urldate = {2023-02-08},
	volume = {32},
	abstract = {This paper focuses on Bayesian Optimization ({BO}) for
	            objectives on combinatorial search spaces, including ordinal and
	            categorical variables. Despite the abundance of potential
	            applications of Combinatorial {BO}, including chipset
	            configuration search and neural architecture search, only a
	            handful of methods have been pro- posed. We introduce {COMBO}, a
	            new Gaussian Process ({GP}) {BO}. {COMBO} quantifies “smoothness”
	            of functions on combinatorial search spaces by utilizing a
	            combinatorial graph. The vertex set of the combinatorial graph
	            consists of all possible joint assignments of the variables,
	            while edges are constructed using the graph Cartesian product of
	            the sub-graphs that represent the individual variables. On this
	            combinatorial graph, we propose an {ARD} diffusion kernel with
	            which the {GP} is able to model high-order interactions between
	            variables leading to better performance. Moreover, using the
	            Horseshoe prior for the scale parameter in the {ARD} diffusion
	            kernel results in an effective variable selection procedure,
	            making {COMBO} suitable for high dimensional problems.
	            Computationally, in {COMBO} the graph Cartesian product allows
	            the Graph Fourier Transform calculation to scale linearly instead
	            of exponentially.We validate {COMBO} in a wide array of real-
	            istic benchmarks, including weighted maximum satisfiability
	            problems and neural architecture search. {COMBO} outperforms
	            consistently the latest state-of-the-art while maintaining
	            computational and statistical efficiency},
	file = {:CombinatorialBayesianOptimization_Oh2019 - Combinatorial Bayesian
	        Optimization Using the Graph Cartesian Product.pdf:PDF},
	groups = {graphical-models},
	priority = {prio3},
}

@inproceedings{FastKroneckerInference_Flaxman2015,
	author = {Flaxman, Seth and Wilson, Andrew and Neill, Daniel and Nickisch,
	          Hannes and Smola, Alex},
	date = {2015-06-01},
	title = {Fast Kronecker Inference in Gaussian Processes with non-Gaussian
	         Likelihoods},
	eventtitle = {International Conference on Machine Learning},
	pages = {607--616},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v37/flaxman15.html},
	urldate = {2023-02-08},
	abstract = {Gaussian processes ({GPs}) are a flexible class of methods with
	            state of the art performance on spatial statistics applications.
	            However, {GPs} require O(n{\textasciicircum}3) computations and
	            O(n{\textasciicircum}2) storage, and popular {GP} kernels are
	            typically limited to smoothing and interpolation. To address
	            these difficulties, Kronecker methods have been used to exploit
	            structure in the {GP} covariance matrix for scalability, while
	            allowing for expressive kernel learning (Wilson et al., 2014).
	            However, fast Kronecker methods have been confined to Gaussian
	            likelihoods. We propose new scalable Kronecker methods for
	            Gaussian processes with non-Gaussian likelihoods, using a Laplace
	            approximation which involves linear conjugate gradients for
	            inference, and a lower bound on the {GP} marginal likelihood for
	            kernel learning. Our approach has near linear scaling, requiring
	            O(D n{\textasciicircum}(D+1)/D) operations and O(D n{
	            \textasciicircum}2/D) storage, for n training data-points on a
	            dense D {\textgreater} 1 dimensional grid. Moreover, we introduce
	            a log Gaussian Cox process, with highly expressive kernels, for
	            modelling spatiotemporal count processes, and apply it to a point
	            pattern (n = 233,088) of a decade of crime events in Chicago.
	            Using our model, we discover spatially varying multiscale
	            seasonal trends and produce highly accurate long-range local area
	            forecasts.},
	file = {:FastKroneckerInference_Flaxman2015 - Fast Kronecker Inference in
	        Gaussian Processes with Non Gaussian Likelihoods.pdf:PDF},
	groups = {graphical-models},
	issn = {1938-7228},
	langid = {english},
}

@report{KroneckerFactorizationPreventing_McInerney2021,
	author = {{McInerney}, Denis Jered and Kong, Luyang and Arumae, Kristjan and
	          Wallace, Byron and Bhatia, Parminder},
	date = {2021-11-10},
	institution = {{arXiv}},
	title = {Kronecker Factorization for Preventing Catastrophic Forgetting in
	         Large-scale Medical Entity Linking},
	doi = {10.48550/arXiv.2111.06012},
	eprint = {2111.06012},
	eprinttype = {arxiv},
	note = {type: article},
	number = {{arXiv}:2111.06012},
	abstract = {Multi-task learning is useful in {NLP} because it is often
	            practically desirable to have a single model that works across a
	            range of tasks. In the medical domain, sequential training on
	            tasks may sometimes be the only way to train models, either
	            because access to the original (potentially sensitive) data is no
	            longer available, or simply owing to the computational costs
	            inherent to joint retraining. A major issue inherent to
	            sequential learning, however, is catastrophic forgetting, i.e., a
	            substantial drop in accuracy on prior tasks when a model is
	            updated for a new task. Elastic Weight Consolidation is a
	            recently proposed method to address this issue, but scaling this
	            approach to the modern large models used in practice requires
	            making strong independence assumptions about model parameters,
	            limiting its effectiveness. In this work, we apply Kronecker
	            Factorization--a recent approach that relaxes independence
	            assumptions--to prevent catastrophic forgetting in convolutional
	            and Transformer-based neural networks at scale. We show the
	            effectiveness of this technique on the important and illustrative
	            task of medical entity linking across three datasets,
	            demonstrating the capability of the technique to be used to make
	            efficient updates to existing methods as new medical data becomes
	            available. On average, the proposed method reduces catastrophic
	            forgetting by 51\% when using a {BERT}-based model, compared to a
	            27\% reduction using standard Elastic Weight Consolidation, while
	            maintaining spatial complexity proportional to the number of
	            model parameters.},
	file = {:KroneckerFactorizationPreventing_McInerney2021 - Kronecker
	        Factorization for Preventing Catastrophic Forgetting in Large Scale
	        Medical Entity Linking.pdf:PDF},
	groups = {graphical-models},
	keywords = {Computer Science - Computation and Language, Computer Science -
	            Machine Learning},
}

@article{CovarianceEstimationHigh_Tsiligkaridis2013,
	author = {Tsiligkaridis, Theodoros and Hero, Alfred O.},
	date = {2013-11},
	journaltitle = {{IEEE} Transactions on Signal Processing},
	title = {Covariance Estimation in High Dimensions Via Kronecker Product
	         Expansions},
	doi = {10.1109/TSP.2013.2279355},
	issn = {1941-0476},
	number = {21},
	pages = {5347--5360},
	volume = {61},
	abstract = {This paper presents a new method for estimating high dimensional
	            covariance matrices. The method, permuted rank-penalized
	            least-squares ({PRLS}), is based on a Kronecker product series
	            expansion of the true covariance matrix. Assuming an i.i.d.
	            Gaussian random sample, we establish high dimensional rates of
	            convergence to the true covariance as both the number of samples
	            and the number of variables go to infinity. For covariance
	            matrices of low separation rank, our results establish that {PRLS
	            } has significantly faster convergence than the standard sample
	            covariance matrix ({SCM}) estimator. The convergence rate
	            captures a fundamental tradeoff between estimation error and
	            approximation error, thus providing a scalable covariance
	            estimation framework in terms of separation rank, similar to low
	            rank approximation of covariance matrices . The {MSE} convergence
	            rates generalize the high dimensional rates recently obtained for
	            the {ML} Flip-flop algorithm , for Kronecker product covariance
	            estimation. We show that a class of block Toeplitz covariance
	            matrices is approximatable by low separation rank and give bounds
	            on the minimal separation rank r that ensures a given level of
	            bias. Simulations are presented to validate the theoretical
	            bounds. As a real world application, we illustrate the utility of
	            the proposed Kronecker covariance estimator for spatio-temporal
	            linear least squares prediction of multivariate wind speed
	            measurements.},
	eventtitle = {{IEEE} Transactions on Signal Processing},
	file = {:CovarianceEstimationHigh_Tsiligkaridis2013 - Covariance Estimation
	        in High Dimensions Via Kronecker Product Expansions.pdf:PDF},
	groups = {graphical-models},
	keywords = {Covariance matrices, Convergence, Symmetric matrices, Estimation
	            , Least squares approximations, Brain modeling, Standards,
	            Structured covariance estimation, penalized least squares,
	            Kronecker product decompositions, high dimensional convergence
	            rates, mean-square error, multivariate prediction},
}

@misc{CholeskyDecompositionLaplacian_Lee2018,
	author = {Lee, Y. T.},
	date = {2018-01-01},
	title = {Cholesky Decomposition for Laplacian},
	subtitle = {Lecture 13},
	titleaddon = {CSE 599},
	url = {https://yintat.com/teaching/cse599-winter18/13.pdf},
	accessdate = {2023-02-08},
	file = {:CholeskyDecompositionLaplacian_Lee2018 - Cholesky Decomposition for
	        Laplacian.pdf:PDF},
	groups = {laplacian-systems},
}

@article{TropicalKirchhoffsformula_Jukna2021,
	author = {Jukna, Stasys and Seiwert, Hannes},
	date = {2021-01-31},
	journaltitle = {Discrete Applied Mathematics},
	title = {Tropical Kirchhoff's formula and postoptimality in matroid
	         optimization},
	doi = {10.1016/j.dam.2020.09.018},
	issn = {0166-218X},
	note = {{ZSCC}: {NoCitationData}[s0]},
	pages = {12--21},
	url = {https://www.sciencedirect.com/science/article/pii/S0166218X2030439X},
	urldate = {2023-02-15},
	volume = {289},
	abstract = {Given an assignment of real weights to the ground elements of a
	            matroid, the min–max weight of a ground element e is the minimum,
	            over all circuits containing e, of the maximum weight of an
	            element in that circuit with the element e removed. We use this
	            concept to answer the following structural questions for the
	            minimum weight basis problem. Which elements are persistent under
	            a given weighting (belong to all or to none of the optimal
	            bases)? What changes of the weights are allowed while preserving
	            optimality of optimal bases? How does the minimum weight of a
	            basis change when the weight of a single ground element is
	            changed, or when a ground element is contracted or deleted? Our
	            answer to this latter question gives the tropical (min,+,−)
	            analogue of Kirchhoff’s arithmetic (+,×,∕) effective conductance
	            formula for electrical networks.},
	file = {:TropicalKirchhoff’sformula_Jukna2021 - Tropical Kirchhoff’s Formula
	        and Postoptimality in Matroid Optimization.html:URL},
	groups = {graphs},
	keywords = {Weighted matroid, Optimization, Sensitivity, Persistency,
	            Postoptimality},
	langid = {english},
	shortjournal = {Discrete Applied Mathematics},
}

@inproceedings{AlgebraicPathProblem_Sanmartin2022,
	author = {Sanmartı́n, Enrique Fita and Damrich, Sebastian and Hamprecht,
	          Fred},
	date = {2022-06-28},
	title = {The Algebraic Path Problem for Graph Metrics},
	eventtitle = {International Conference on Machine Learning},
	note = {{ZSCC}: {NoCitationData}[s0]},
	pages = {19178--19204},
	publisher = {{PMLR}},
	url = {https://proceedings.mlr.press/v162/sanmarti-n22a.html},
	urldate = {2023-02-15},
	abstract = {Finding paths with optimal properties is a foundational problem
	            in computer science. The notions of shortest paths (minimal sum
	            of edge costs), minimax paths (minimal maximum edge weight),
	            reliability of a path and many others all arise as special cases
	            of the "algebraic path problem" ({APP}). Indeed, the {APP}
	            formalizes the relation between different semirings such as
	            min-plus, min-max and the distances they induce. We here clarify,
	            for the first time, the relation between the potential distance
	            and the log-semiring. We also define a new unifying family of
	            algebraic structures that include all above-mentioned path
	            problems as well as the commute cost and others as special or
	            limiting cases. The family comprises not only semirings but also
	            strong bimonoids (that is, semirings without distributivity). We
	            call this new and very general distance the "log-norm distance".
	            Finally, we derive some sufficient conditions which ensure that
	            the {APP} associated with a semiring defines a metric over an
	            arbitrary graph.},
	file = {:AlgebraicPathProblem_Sanmartin2022 - The Algebraic Path Problem for
	        Graph Metrics.pdf:PDF},
	groups = {graphs},
	issn = {2640-3498},
	langid = {english},
}

@inproceedings{Learningassociativemarkov_Taskar2004,
	author = {Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne},
	booktitle = {Proceedings of the twenty-first international conference on
	             Machine learning},
	date = {2004},
	title = {Learning associative markov networks},
	doi = {10.1145/1015330.1015444},
	pages = {102},
	file = {:Learningassociativemarkov_Taskar2004 - Learning Associative Markov
	        Networks.pdf:PDF},
	groups = {graphical-models},
}

@article{MarkovPerspectiveDevelopment_Mane2011,
	author = {Mane, Muharrem and {DeLaurentis}, Daniel and Frazho, Arthur},
	date = {2011-10-18},
	journaltitle = {Journal of Mechanical Design},
	title = {A Markov Perspective on Development Interdependencies in Networks
	         of Systems},
	doi = {10.1115/1.4004975},
	issn = {1050-0472},
	note = {{ZSCC}: 0000026},
	number = {10},
	urldate = {2023-02-15},
	volume = {133},
	abstract = {The development and acquisition of complex systems remain a
	            challenge, especially in the aerospace/defense sector, due to
	            complexities in both program management and engineering design
	            that often are a result of interdependencies. The
	            interdependencies between constituent systems form networks that,
	            while enabling capabilities that are beyond those of individual
	            systems, also increase risk since disruptions in the development
	            of one system may propagate to other directly or indirectly
	            dependent systems. This paper demonstrates an approach to
	            aggregate the network interdependency characteristics and compare
	            alternatives with respect to the time required to arrest the
	            propagation of development delays in a network. Delay propagation
	            is modeled as a Markov chain, where the states are defined as the
	            constituent systems and the transition probabilities as the
	            dependency strengths between systems. A proof-of-concept
	            application shows the approach can distinguish between alternate
	            networks and indicates its applicability for managing risk in
	            design and development of systems with significant
	            interdependencies.},
	file = {:MarkovPerspectiveDevelopment_Mane2011 - A Markov Perspective on
	        Development Interdependencies in Networks of Systems.pdf:PDF},
	groups = {graphical-models},
	shortjournal = {Journal of Mechanical Design},
}

@article{EvaluationFolksonomyInduction_Strohmaier2012,
	author = {Markus Strohmaier and Denis Helic and Dominik Benz and Christian
	          Körner and Roman Kern},
	date = {2012-09},
	journaltitle = {{ACM} Transactions on Intelligent Systems and Technology},
	title = {Evaluation of Folksonomy Induction Algorithms},
	doi = {10.1145/2337542.2337559},
	number = {4},
	pages = {1--22},
	volume = {3},
	file = {:EvaluationFolksonomyInduction_Strohmaier2012 - Evaluation of
	        Folksonomy Induction Algorithms.pdf:PDF},
	groups = {graphical-models},
	publisher = {Association for Computing Machinery ({ACM})},
}

@report{TemporalPointProcess_Lyu2021,
	author = {Lyu, Yalong and Wang, Huiyuan and Lin, Wei},
	date = {2021-10-21},
	institution = {{arXiv}},
	title = {Temporal Point Process Graphical Models},
	doi = {10.48550/arXiv.2110.11562},
	eprint = {2110.11562},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000000 type: article},
	number = {{arXiv}:2110.11562},
	abstract = {Many real-world objects can be modeled as a stream of events on
	            the nodes of a graph. In this paper, we propose a class of
	            graphical event models named temporal point process graphical
	            models for representing the temporal dependencies among different
	            components of a multivariate point process. In our model, the
	            intensity of an event stream can depend on the historical events
	            in a nonlinear way. We provide a procedure that allows us to
	            estimate the parameters in the model with a convex loss function
	            in the high-dimensional setting. For the approximation error
	            introduced during the implementation, we also establish the error
	            bound for our estimators. We demonstrate the performance of our
	            method with extensive simulations and a spike train data set.},
	file = {:TemporalPointProcess_Lyu2021 - Temporal Point Process Graphical
	        Models.pdf:PDF},
	groups = {graphical-models},
	keywords = {Statistics - Methodology, Mathematics - Statistics Theory,
	            Primary 62M08, secondary 62H08, 60G08},
}

@article{Markovrandomfields_Steck2019,
	author = {Steck, Harald},
	date = {2019},
	journaltitle = {Advances in Neural Information Processing Systems},
	title = {Markov random fields for collaborative filtering},
	url = {
	       https://proceedings.neurips.cc/paper/2019/hash/9087b0efc7c7acd1ef7e153678809c77-Abstract.html
	       },
	volume = {32},
	file = {:Markovrandomfields_Steck2019 - Markov Random Fields for
	        Collaborative Filtering.pdf:PDF},
	groups = {graphical-models},
}

@article{StatisticalAnalysisNon_Besag1975,
	author = {Julian Besag},
	date = {1975-09},
	journaltitle = {The Statistician},
	title = {Statistical Analysis of Non-Lattice Data},
	doi = {10.2307/2987782},
	number = {3},
	pages = {179},
	volume = {24},
	groups = {graphical-models},
	publisher = {{JSTOR}},
}

@article{RegularizedCovarianceMatrix_Coluccia2015,
	author = {Coluccia, Angelo},
	date = {2015-11},
	journaltitle = {{IEEE} Signal Processing Letters},
	title = {Regularized Covariance Matrix Estimation via Empirical Bayes},
	doi = {10.1109/LSP.2015.2462724},
	issn = {1558-2361},
	note = {{ZSCC}: 0000013},
	number = {11},
	pages = {2127--2131},
	volume = {22},
	abstract = {An Empirical Bayes formalization of the regularized covariance
	            estimation problem is proposed for (possibly high-dimensional,
	            low-sample) normal variates. A simple iteration is provided to
	            automatically adjust the shrinkage level, which provably
	            converges to the maximum likelihood hyperparameter estimation for
	            any choice of the starting point. The proposed approach is
	            effective and can outperform both {MSE}-optimized diagonal
	            loading and the Rao-Blackwell Leidot-Wolf estimator in terms of
	            covariance-matrix-specific metrics.},
	eventtitle = {{IEEE} Signal Processing Letters},
	file = {:RegularizedCovarianceMatrix_Coluccia2015 - Regularized Covariance
	        Matrix Estimation Via Empirical Bayes.pdf:PDF},
	groups = {graphical-models},
	keywords = {Covariance matrices, Maximum likelihood estimation, Measurement,
	            Convergence, Loading, Eigenvalues and eigenfunctions, Covariance
	            estimation, diagonal loading, empirical Bayes, minimum mean
	            square error ({MMSE}), regularization, robust estimation,
	            shrinkage},
}

@report{SparseCovarianceSelection_Banerjee2005,
	author = {Banerjee, Onureena and d'Aspremont, Alexandre and Ghaoui, Laurent
	          El},
	date = {2005-06-08},
	institution = {{arXiv}},
	title = {Sparse Covariance Selection via Robust Maximum Likelihood
	         Estimation},
	doi = {10.48550/arXiv.cs/0506023},
	eprint = {cs/0506023},
	eprintclass = {cs},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000011 type: article},
	number = {{arXiv}:cs/0506023},
	abstract = {We address a problem of covariance selection, where we seek a
	            trade-off between a high likelihood against the number of
	            non-zero elements in the inverse covariance matrix. We solve a
	            maximum likelihood problem with a penalty term given by the sum
	            of absolute values of the elements of the inverse covariance
	            matrix, and allow for imposing bounds on the condition number of
	            the solution. The problem is directly amenable to now standard
	            interior-point algorithms for convex optimization, but remains
	            challenging due to its size. We first give some results on the
	            theoretical computational complexity of the problem, by showing
	            that a recent methodology for non-smooth convex optimization due
	            to Nesterov can be applied to this problem, to greatly improve on
	            the complexity estimate given by interior-point algorithms. We
	            then examine two practical algorithms aimed at solving
	            large-scale, noisy (hence dense) instances: one is based on a
	            block-coordinate descent approach, where columns and rows are
	            updated sequentially, another applies a dual version of
	            Nesterov's method.},
	file = {:SparseCovarianceSelection_Banerjee2005 - Sparse Covariance
	        Selection Via Robust Maximum Likelihood Estimation.pdf:PDF},
	groups = {graphical-models},
	keywords = {Computer Science - Computational Engineering, Finance, and
	            Science, Computer Science - Artificial Intelligence, F.2.1, G.1.3
	            , G.1.6, G.3, J.3},
}

@article{SpectralPropertiesHypergraph_Chan2018,
	author = {T.-H. Hubert Chan and Anand Louis and Zhihao Gavin Tang and Chenzi
	          Zhang},
	date = {2018-03},
	journaltitle = {Journal of the {ACM}},
	title = {Spectral Properties of Hypergraph Laplacian and Approximation
	         Algorithms},
	doi = {10.1145/3178123},
	number = {3},
	pages = {1--48},
	volume = {65},
	file = {:SpectralPropertiesHypergraph_Chan2018 - Spectral Properties of
	        Hypergraph Laplacian and Approximation Algorithms.pdf:PDF},
	groups = {incidence structures},
	publisher = {Association for Computing Machinery ({ACM})},
}

@inproceedings{RobustCascadeReconstruction_Xiao2018,
	author = {Xiao, Han and Aslay, Cigdem and Gionis, Aristides},
	booktitle = {2018 {IEEE} International Conference on Data Mining ({ICDM})},
	date = {2018-11},
	title = {Robust Cascade Reconstruction by Steiner Tree Sampling},
	doi = {10.1109/ICDM.2018.00079},
	eventtitle = {2018 {IEEE} International Conference on Data Mining ({ICDM})},
	note = {{ZSCC}: 0000010 {ISSN}: 2374-8486},
	pages = {637--646},
	abstract = {We consider a network where an infection has taken place and a
	            subset of infected nodes has been partially observed. Our goal is
	            to reconstruct the underlying cascade that is likely to have
	            generated these observations. We reduce this
	            cascade-reconstruction problem to computing the marginal
	            probability that a node is infected given the partial
	            observations, which is a \#P-hard problem. To circumvent this
	            issue, we resort to estimating infection probabilities by
	            generating a sample of probable cascades, which span the nodes
	            that have already been observed to be infected, and avoid the
	            nodes that have been observed to be uninfected. The sampling
	            problem corresponds to sampling directed Steiner trees with a
	            given set of terminals, which is a problem of independent
	            interest and has received limited attention in the literature.
	            For the latter problem we propose two novel algorithms with
	            provable guarantees on the sampling distribution of the returned
	            Steiner trees. The resulting method improves over
	            state-of-the-art approaches that often make explicit assumptions
	            about the infection-propagation model, or require additional
	            parameters. Our method provides a more robust approach to the
	            cascade-reconstruction problem, which makes weaker assumptions
	            about the infection model, requires fewer additional parameters,
	            and can be used to estimate node infection probabilities. We
	            experimentally validate the proposed reconstruction algorithm on
	            real-world graphs with both synthetic and real cascades. We show
	            that our method outperforms all other baseline strategies in most
	            cases.},
	file = {:RobustCascadeReconstruction_Xiao2018 - Robust Cascade
	        Reconstruction by Steiner Tree Sampling.pdf:PDF},
	groups = {graphical-models},
	issn = {2374-8486},
	keywords = {Steiner trees, Probabilistic logic, Markov processes, Diffusion
	            processes, Integrated circuit modeling, Task analysis, infection
	            cascades, epidemics, cascade reconstruction, Steiner tree
	            sampling},
}

@article{Sparseinversecovariance_Friedman2008,
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	date = {2008-07},
	journaltitle = {Biostatistics (Oxford, England)},
	title = {Sparse inverse covariance estimation with the graphical lasso},
	doi = {10.1093/biostatistics/kxm045},
	issn = {1465-4644},
	note = {{ZSCC}: 0005870},
	number = {3},
	pages = {432--441},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3019769/},
	urldate = {2023-02-15},
	volume = {9},
	abstract = {We consider the problem of estimating sparse graphs by a lasso
	            penalty applied to the inverse covariance matrix. Using a
	            coordinate descent procedure for the lasso, we develop a simple
	            algorithm—the graphical lasso—that is remarkably fast: It solves
	            a 1000-node problem (∼500000 parameters) in at most a minute and
	            is 30–4000 times faster than competing methods. It also provides
	            a conceptual link between the exact problem and the approximation
	            suggested by Meinshausen and Bühlmann (2006). We illustrate the
	            method on some cell-signaling data from proteomics.},
	file = {:Sparseinversecovariance_Friedman2008 - Sparse Inverse Covariance
	        Estimation with the Graphical
	        Lasso.html:URL;:Sparseinversecovariance_Friedman2008 - Sparse Inverse
	        Covariance Estimation with the Graphical Lasso.pdf:PDF},
	groups = {graphical-models},
	pmcid = {PMC3019769},
	pmid = {18079126},
	shortjournal = {Biostatistics},
}

@article{UniversalHyperbolicGeometry_Wildberger2018,
	author = {Wildberger, Norman J.},
	date = {2018-01},
	journaltitle = {Universe},
	title = {Universal Hyperbolic Geometry, Sydpoints and Finite Fields: A
	         Projective and Algebraic Alternative},
	doi = {10.3390/universe4010003},
	issn = {2218-1997},
	note = {{ZSCC}: 0000001},
	number = {1},
	pages = {3},
	url = {https://www.mdpi.com/2218-1997/4/1/3},
	urldate = {2023-05-04},
	volume = {4},
	abstract = {Universal hyperbolic geometry gives a purely algebraic approach
	            to the subject that connects naturally with Einstein’s special
	            theory of relativity. In this paper, we give an overview of some
	            aspects of this theory relating to triangle geometry and in
	            particular the remarkable new analogues of midpoints called
	            sydpoints. We also discuss how the generality allows us to
	            consider hyperbolic geometry over general fields, in particular
	            over finite fields.},
	file = {:UniversalHyperbolicGeometry_Wildberger2018 - Universal Hyperbolic
	        Geometry, Sydpoints and Finite Fields_ a Projective and Algebraic
	        Alternative.pdf:PDF},
	groups = {hyperbolic trees},
	howpublished = {Article},
	keywords = {rational trigonometry, universal hyperbolic geometry, sydpoints,
	            finite fields},
	langid = {english},
	priority = {prio2},
	publisher = {Multidisciplinary Digital Publishing Institute},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Universal Hyperbolic Geometry, Sydpoints and Finite Fields},
	type = {Article},
}

'
@article{Semisupervisedlearning_Avrachenkov2017,
	author = {Avrachenkov, K. and Chebotarev, P. and Mishenin, A.},
	date = {2017-03-04},
	journaltitle = {Optimization Methods and Software},
	title = {Semi-supervised learning with regularized Laplacian},
	doi = {10.1080/10556788.2016.1193176},
	issn = {1055-6788},
	note = {{ZSCC}: 0000013},
	number = {2},
	pages = {222--236},
	urldate = {2023-05-08},
	volume = {32},
	abstract = {We study a semi-supervised learning method based on the
	            similarity graph and regularized Laplacian. We give convenient
	            optimization formulation of the regularized Laplacian method and
	            establish its various properties. In particular, we show that the
	            kernel of the method can be interpreted in terms of discrete and
	            continuous-time random walks and possesses several important
	            properties of proximity measures. Both optimization and linear
	            algebra methods can be used for efficient computation of the
	            classification functions. We demonstrate on numerical examples
	            that the regularized Laplacian method is robust with respect to
	            the choice of the regularization parameter and outperforms the
	            Laplacian-based heat kernel methods.},
	file = {:Semisupervisedlearning_Avrachenkov2017 - Semi Supervised Learning
	        with Regularized Laplacian.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {semi-supervised learning, graph-based learning, regularized
	            Laplacian, proximity measure, Wikipedia article classification,
	            68T01, 68T05, 15A16, 15A45, 97R40, 05C50},
	priority = {prio1},
	publisher = {Taylor \& Francis},
}

'
@report{MatrixForestTheorem_Chebotarev2006,
	author = {Chebotarev, Pavel and Shamis, Elena},
	date = {2006-02-04},
	institution = {{arXiv}},
	title = {The Matrix-Forest Theorem and Measuring Relations in Small Social
	         Groups},
	doi = {10.48550/arXiv.math/0602070},
	eprint = {math/0602070},
	eprintclass = {math},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000285 type: article},
	number = {{arXiv}:math/0602070},
	abstract = {We propose a family of graph structural indices related to the
	            Matrix-forest theorem. The properties of the basic index that
	            expresses the mutual connectivity of two vertices are studied in
	            detail. The derivative indices that measure "dissociation," "
	            solitariness," and "provinciality" of vertices are also
	            considered. A nonstandard metric on the set of vertices is
	            introduced, which is determined by their connectivity. The
	            application of these indices in sociometry is discussed.},
	file = {:MatrixForestTheorem_Chebotarev2006 - The Matrix Forest Theorem and
	        Measuring Relations in Small Social Groups.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {Mathematics - Combinatorics, Computer Science - Information
	            Retrieval, Mathematics - Algebraic Geometry, 05C50, 05C05, 15A51},
	priority = {prio1},
}

'
@inproceedings{LearningMixedCurvature_Gu2018,
	author = {Gu, Albert and Sala, Frederic and Gunel, Beliz and Ré, Christopher
	          },
	date = {2018-12-21},
	title = {Learning Mixed-Curvature Representations in Product Spaces},
	eventtitle = {International Conference on Learning Representations},
	note = {{ZSCC}: {NoCitationData}[s0]},
	url = {https://openreview.net/forum?id=HJxeWnCcF7},
	urldate = {2023-05-08},
	abstract = {The quality of the representations achieved by embeddings is
	            determined by how well the geometry of the embedding space
	            matches the structure of the data. Euclidean space has been the
	            workhorse for embeddings; recently hyperbolic and spherical
	            spaces have gained popularity due to their ability to better
	            embed new types of structured data---such as hierarchical
	            data---but most data is not structured so uniformly. We address
	            this problem by proposing learning embeddings in a product
	            manifold combining multiple copies of these model spaces
	            (spherical, hyperbolic, Euclidean), providing a space of
	            heterogeneous curvature suitable for a wide variety of
	            structures. We introduce a heuristic to estimate the sectional
	            curvature of graph data and directly determine an appropriate
	            signature---the number of component spaces and their
	            dimensions---of the product manifold. Empirically, we jointly
	            learn the curvature and the embedding in the product space via
	            Riemannian optimization. We discuss how to define and compute
	            intrinsic quantities such as means---a challenging notion for
	            product manifolds---and provably learnable optimization
	            functions. On a range of datasets and reconstruction tasks, our
	            product space embeddings outperform single Euclidean or
	            hyperbolic spaces used in previous works, reducing distortion by
	            32.55\% on a Facebook social network dataset. We learn word
	            embeddings and find that a product of hyperbolic spaces in 50
	            dimensions consistently improves on baseline Euclidean and
	            hyperbolic embeddings, by 2.6 points in Spearman rank correlation
	            on similarity tasks and 3.4 points on analogy accuracy.},
	file = {:LearningMixedCurvature_Gu2018 - Learning Mixed Curvature
	        Representations in Product Spaces.pdf:PDF},
	groups = {hyperbolic trees},
	langid = {english},
}

'
@report{NestedHyperbolicSpaces_Fan2021,
	author = {Fan, Xiran and Yang, Chun-Hao and Vemuri, Baba C.},
	date = {2021-12-02},
	institution = {{arXiv}},
	title = {Nested Hyperbolic Spaces for Dimensionality Reduction and
	         Hyperbolic {NN} Design},
	doi = {10.48550/arXiv.2112.03402},
	eprint = {2112.03402},
	eprinttype = {arxiv},
	note = {{ZSCC}: {NoCitationData}[s0] type: article},
	number = {{arXiv}:2112.03402},
	abstract = {Hyperbolic neural networks have been popular in the recent past
	            due to their ability to represent hierarchical data sets
	            effectively and efficiently. The challenge in developing these
	            networks lies in the nonlinearity of the embedding space namely,
	            the Hyperbolic space. Hyperbolic space is a homogeneous
	            Riemannian manifold of the Lorentz group. Most existing methods
	            (with some exceptions) use local linearization to define a
	            variety of operations paralleling those used in traditional deep
	            neural networks in Euclidean spaces. In this paper, we present a
	            novel fully hyperbolic neural network which uses the concept of
	            projections (embeddings) followed by an intrinsic aggregation and
	            a nonlinearity all within the hyperbolic space. The novelty here
	            lies in the projection which is designed to project data on to a
	            lower-dimensional embedded hyperbolic space and hence leads to a
	            nested hyperbolic space representation independently useful for
	            dimensionality reduction. The main theoretical contribution is
	            that the proposed embedding is proved to be isometric and
	            equivariant under the Lorentz transformations. This projection is
	            computationally efficient since it can be expressed by simple
	            linear operations, and, due to the aforementioned equivariance
	            property, it allows for weight sharing. The nested hyperbolic
	            space representation is the core component of our network and
	            therefore, we first compare this ensuing nested hyperbolic space
	            representation with other dimensionality reduction methods such
	            as tangent {PCA}, principal geodesic analysis ({PGA}) and {
	            HoroPCA}. Based on this equivariant embedding, we develop a novel
	            fully hyperbolic graph convolutional neural network architecture
	            to learn the parameters of the projection. Finally, we present
	            experiments demonstrating comparative performance of our network
	            on several publicly available data sets.},
	file = {:NestedHyperbolicSpaces_Fan2021 - Nested Hyperbolic Spaces for
	        Dimensionality Reduction and Hyperbolic NN Design.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Computer Science - Machine Learning, Computer Science -
	            Artificial Intelligence, Statistics - Machine Learning},
	priority = {prio3},
}

'
@article{unifiedGaussiancopula_Hughes2022,
	author = {Hughes, John},
	date = {2022-09-23},
	journaltitle = {Scientific Reports},
	title = {A unified Gaussian copula methodology for spatial regression
	         analysis},
	doi = {10.1038/s41598-022-20171-1},
	issn = {2045-2322},
	note = {{ZSCC}: 0000000},
	number = {1},
	pages = {15915},
	url = {https://www.nature.com/articles/s41598-022-20171-1},
	urldate = {2023-05-08},
	volume = {12},
	abstract = {Spatially referenced data arise in many fields, including
	            imaging, ecology, public health, and marketing. Although
	            principled smoothing or interpolation is paramount for many
	            practitioners, regression, too, can be an important (or even the
	            only or most important) goal of a spatial analysis. When doing
	            spatial regression it is crucial to accommodate spatial variation
	            in the response variable that cannot be explained by the
	            spatially patterned explanatory variables included in the model.
	            Failure to model both sources of spatial dependence—regression
	            and extra-regression, if you will—can lead to erroneous inference
	            for the regression coefficients. In this article I highlight an
	            under-appreciated spatial regression model, namely, the spatial
	            Gaussian copula regression model ({SGCRM}), and describe said
	            model’s advantages. Then I develop an intuitive, unified, and
	            computationally efficient approach to inference for the {SGCRM}.
	            I demonstrate the efficacy of the proposed methodology by way of
	            an extensive simulation study along with analyses of a well-known
	            dataset from disease mapping.},
	file = {:unifiedGaussiancopula_Hughes2022 - A Unified Gaussian Copula
	        Methodology for Spatial Regression Analysis.pdf:PDF},
	groups = {incidence structures, graphical-models},
	howpublished = {{OriginalPaper}},
	keywords = {Computational science, Gastrointestinal cancer, Statistics},
	langid = {english},
	priority = {prio1},
	publisher = {Nature Publishing Group},
	rights = {2022 The Author(s)},
	shortjournal = {Sci Rep},
	type = {{OriginalPaper}},
}

'
@report{ChangebasisGram_Joot2011,
	author = {Joot, Peeter},
	date = {2011-04-25},
	institution = {{arXiv}},
	title = {Change of basis and Gram-Schmidt orthonormalization in special
	         relativity},
	doi = {10.48550/arXiv.1104.4829},
	eprint = {1104.4829},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000000 type: article},
	number = {{arXiv}:1104.4829},
	abstract = {While an explicit basis is common in the study of Euclidean
	            spaces, it is usually implied in the study of inertial
	            relativistic systems. There are some conceptual advantages to
	            including the basis in the study of special relativistic systems.
	            A Minkowski metric implies a non-orthonormal basis, and to deal
	            with this complexity the concepts of reciprocal basis and the
	            vector dual are introduced. It is shown how the reciprocal basis
	            is related to upper and lower index coordinate extraction, the
	            metric tensor, change of basis, projections in non-orthonormal
	            bases, and finally the Gram-Schmidt procedure. It will be shown
	            that Lorentz transformations can be viewed as change of basis
	            operations. The Lorentz boost in one spatial dimension will be
	            derived using the Gram-Schmidt orthonormalization algorithm, and
	            it will be shown how other Lorentz transformations can be derived
	            using the Gram-Schmidt procedure.},
	file = {:ChangebasisGram_Joot2011 - Change of Basis and Gram Schmidt
	        Orthonormalization in Special Relativity.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Physics - Classical Physics},
	priority = {prio3},
}

'
@article{Projectivegeometryspecial_Delphenich2006,
	author = {Delphenich, D. H.},
	date = {2006-03-01},
	journaltitle = {Annalen der Physik},
	title = {Projective geometry and special relativity},
	doi = {10.1002/andp.2006518030410.1002/andp.200510179},
	issn = {0003-3804},
	note = {{ZSCC}: 0000012 {ADS} Bibcode: 2006AnP...518..216D},
	pages = {216--246},
	url = {https://ui.adsabs.harvard.edu/abs/2006AnP...518..216D},
	urldate = {2023-05-08},
	volume = {518},
	abstract = {Some concepts of real and complex projective geometry are
	            applied to the fundamental physical notions that relate to
	            Minkowski space and the Lorentz group. In particular, it is shown
	            that the transition from an infinite speed of propagation for
	            light waves to a finite one entails the replacement of a
	            hyperplane at infinity with a light cone and the replacement of
	            an affine hyperplane - or rest space - with a proper time
	            hyperboloid. The transition from the metric theory of
	            electromagnetism to the pre-metric theory is discussed in the
	            context of complex projective geometry, and ultimately it is
	            proposed that the geometrical issues are more general than
	            electromagnetism, namely, they pertain to the transition from
	            point mechanics to wave mechanics.},
	file = {:Projectivegeometryspecial_Delphenich2006 - Projective Geometry and
	        Special Relativity.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Projective geometry, special relativity, pre-metric
	            electromagnetism, wave mechanics, General Relativity and Quantum
	            Cosmology},
}

'
@article{EfficientComputationExpectations_Zmigrod2021,
	author = {Zmigrod, Ran and Vieira, Tim and Cotterell, Ryan},
	date = {2021-07-08},
	journaltitle = {Transactions of the Association for Computational
	                Linguistics},
	title = {Efficient Computation of Expectations under Spanning Tree
	         Distributions},
	doi = {10.1162/tacl_a_00391},
	issn = {2307-387X},
	note = {{ZSCC}: 0000004},
	pages = {675--690},
	urldate = {2023-05-08},
	volume = {9},
	abstract = {We give a general framework for inference in spanning tree
	            models. We propose unified algorithms for the important cases of
	            first-order expectations and second-order expectations in
	            edge-factored, non-projective spanning-tree models. Our
	            algorithms exploit a fundamental connection between gradients and
	            expectations, which allows us to derive efficient algorithms.
	            These algorithms are easy to implement with or without automatic
	            differentiation software. We motivate the development of our
	            framework with several cautionary tales of previous research,
	            which has developed numerous inefficient algorithms for computing
	            expectations and their gradients. We demonstrate how our
	            framework efficiently computes several quantities with known
	            algorithms, including the expected attachment score, entropy, and
	            generalized expectation criteria. As a bonus, we give algorithms
	            for quantities that are missing in the literature, including the
	            {KL} divergence. In all cases, our approach matches the
	            efficiency of existing algorithms and, in several cases, reduces
	            the runtime complexity by a factor of the sentence length. We
	            validate the implementation of our framework through runtime
	            experiments. We find our algorithms are up to 15 and 9 times
	            faster than previous algorithms for computing the Shannon entropy
	            and the gradient of the generalized expectation objective,
	            respectively.},
	file = {:EfficientComputationExpectations_Zmigrod2021 - Efficient
	        Computation of Expectations under Spanning Tree Distributions.pdf:PDF
	        },
	groups = {graphical-models},
	priority = {prio1},
	shortjournal = {Transactions of the Association for Computational
	                Linguistics},
}

'
@report{LargeMarginNearest_Nielsen2016,
	author = {Nielsen, Frank and Muzellec, Boris and Nock, Richard},
	date = {2016-09-26},
	institution = {{arXiv}},
	title = {Large Margin Nearest Neighbor Classification using Curved
	         Mahalanobis Distances},
	doi = {10.48550/arXiv.1609.07082},
	eprint = {1609.07082},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000002 type: article},
	number = {{arXiv}:1609.07082},
	abstract = {We consider the supervised classification problem of machine
	            learning in Cayley-Klein projective geometries: We show how to
	            learn a curved Mahalanobis metric distance corresponding to
	            either the hyperbolic geometry or the elliptic geometry using the
	            Large Margin Nearest Neighbor ({LMNN}) framework. We report on
	            our experimental results, and further consider the case of
	            learning a mixed curved Mahalanobis distance. Besides, we show
	            that the Cayley-Klein Voronoi diagrams are affine, and can be
	            built from an equivalent (clipped) power diagrams, and that
	            Cayley-Klein balls have Mahalanobis shapes with displaced
	            centers.},
	file = {:LargeMarginNearest_Nielsen2016 - Large Margin Nearest Neighbor
	        Classification Using Curved Mahalanobis Distances.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Computer Science - Machine Learning, Computer Science -
	            Computational Geometry, Computer Science - Computer Vision and
	            Pattern Recognition},
	priority = {prio1},
}

@inproceedings{FocusContextTechniqueBased_Lamping1995,
	author = {Lamping, John and Rao, Ramana and Pirolli, Peter},
	date = {1995},
	title = {A Focus+Context Technique Based on Hyperbolic Geometry for
	         Visualizing Large Hierarchies},
	doi = {10.1145/223904.223956},
	abstract = {We present a new focus+Context (fisheye) technique for vi­
	            sualizing and manipulating large hierarchies. Our technique
	            assigns more display space to a portion of the hierarchy while
	            still embedding it in the context of the entire hierarchy. The
	            essence of this scheme is to layout the hierarchy in a uniform
	            way on a hyperbolic plane and map this plane onto a circular
	            display region. This supports a smooth blending between fo­ cus
	            and context, as well as continuous redirection of the focus. We
	            have developed effective procedures for manipulating the focus
	            using pointer clicks as well as interactive dragging, and for
	            smoothly animating transitions across such manipulation. A
	            laboratory experiment comparing the hyperbolic browser with a
	            conventional hierarchy browser was conducted.},
	accessdate = {2023-05-08},
	file = {:FocusContextTechniqueBased_Lamping1995 - A Focus+Context Technique
	        Based on Hyperbolic Geometry for Visualizing Large
	        Hierarchies.pdf:PDF},
	groups = {hyperbolic trees},
	keywords = {Hierarchy Display, Information Visualization, Fisheye Display,
	            Focus+Context Technique},
	priority = {prio3},
}

'
@inproceedings{HyperbolicVoronoiDiagrams_Nielsen2010,
	author = {Nielsen, Frank and Nock, Richard},
	booktitle = {2010 International Conference on Computational Science and Its
	             Applications},
	date = {2010-03},
	title = {Hyperbolic Voronoi Diagrams Made Easy},
	doi = {10.1109/ICCSA.2010.37},
	eventtitle = {2010 International Conference on Computational Science and Its
	              Applications},
	note = {{ZSCC}: 0000062},
	pages = {74--80},
	abstract = {We present a simple framework to compute hyperbolic Voronoi
	            diagrams of finite point sets as affine diagrams. We prove that
	            bisectors in Klein's non-conformal disk model are hyperplanes
	            that can be interpreted as power bisectors of Euclidean balls.
	            Therefore our method simply consists in computing an equivalent
	            clipped power diagram followed by a mapping transformation
	            depending on the selected representation of the hyperbolic space
	            (e.g., Poincare conformal disk or upper-plane representations).
	            We discuss on extensions of this approach to weighted and k-order
	            diagrams, and describe their dual triangulations. Finally, we
	            consider two useful primitives on the hyperbolic Voronoi diagrams
	            for designing tailored user interfaces of an image catalog
	            browsing application in the hyperbolic disk: (1) finding nearest
	            neighbors, and (2) computing smallest enclosing balls.},
	file = {:HyperbolicVoronoiDiagrams_Nielsen2010 - Hyperbolic Voronoi Diagrams
	        Made Easy.html:URL},
	groups = {hyperbolic trees},
	keywords = {Information geometry, Gaussian distribution, Application
	            software, Computer science, Laboratories, User interfaces,
	            Nearest neighbor searches, Computer interfaces, Geoscience,
	            Extraterrestrial measurements, Voronoi diagrams,
	            conformal/non-conformal geometries, Klein disk, Poincare disk,
	            Poincare upper-plane},
	priority = {prio2},
}

'
@article{SimilaritiesgraphsKernels_Avrachenkov2019,
	author = {Avrachenkov, Konstantin and Chebotarev, Pavel and Rubanov, Dmytro},
	date = {2019-08-01},
	journaltitle = {European Journal of Combinatorics},
	title = {Similarities on graphs: Kernels versus proximity measures},
	doi = {10.1016/j.ejc.2018.02.002},
	issn = {0195-6698},
	note = {{ZSCC}: 0000018},
	pages = {47--56},
	series = {Special Issue in Memory of Michel Marie Deza},
	url = {https://www.sciencedirect.com/science/article/pii/S0195669818300155},
	urldate = {2023-05-08},
	volume = {80},
	abstract = {We analytically study proximity and distance properties of
	            various kernels and similarity measures on graphs. This helps to
	            understand the mathematical nature of such measures and can
	            potentially be useful for recommending the adoption of specific
	            similarity measures in data analysis.},
	file = {:SimilaritiesgraphsKernels_Avrachenkov2019 - Similarities on Graphs_
	        Kernels Versus Proximity Measures.pdf:PDF},
	groups = {laplacian-systems, thesis},
	langid = {english},
	priority = {prio1},
	shortjournal = {European Journal of Combinatorics},
	shorttitle = {Similarities on graphs},
}

'
@report{OccasionalExactnessDistributional_Hughes2021,
	author = {Hughes, John},
	date = {2021-03-05},
	institution = {{arXiv}},
	title = {On the Occasional Exactness of the Distributional Transform
	         Approximation for Direct Gaussian Copula Models with Discrete
	         Margins},
	doi = {10.48550/arXiv.2103.03688},
	eprint = {2103.03688},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000003 type: article},
	number = {{arXiv}:2103.03688},
	abstract = {The direct Gaussian copula model with discrete marginal
	            distributions is an appealing data-analytic tool but poses
	            difficult computational challenges due to its intractable
	            likelihood. A number of approximations/surrogates for the
	            likelihood have been proposed, including the continuous
	            extension-based approximation ({CE}) and the distributional
	            transform-based approximation ({DT}). The continuous extension
	            approach is exact up to Monte Carlo error but does not scale well
	            computationally. The distributional transform approach permits
	            efficient computation but offers no theoretical guarantee that it
	            is exact. In practice, though, the distributional transform-based
	            approximate likelihood is so very nearly exact for some variants
	            of the model as to permit genuine maximum likelihood or Bayesian
	            inference. We demonstrate the exactness of the distributional
	            transform-based objective function for two interesting variants
	            of the model, and propose a quantity that can be used to assess
	            exactness for experimentally observed datasets. Said diagnostic
	            will permit practitioners to determine whether genuine Bayesian
	            inference or ordinary maximum likelihood inference using the {DT}
	            -based likelihood is possible for a given dataset.},
	file = {:OccasionalExactnessDistributional_Hughes2021 - On the Occasional
	        Exactness of the Distributional Transform Approximation for Direct
	        Gaussian Copula Models with Discrete Margins.pdf:PDF},
	groups = {graphical-models},
	keywords = {Statistics - Methodology},
	priority = {prio2},
}

'
@report{ElectricalFlowsover_Gupta2020,
	author = {Gupta, Swati and Khodabakhsh, Ali and Mortagy, Hassan and Nikolova
	          , Evdokia},
	date = {2020-12-14},
	institution = {{arXiv}},
	title = {Electrical Flows over Spanning Trees},
	doi = {10.48550/arXiv.1909.04759},
	eprint = {1909.04759},
	eprinttype = {arxiv},
	note = {{ZSCC}: {NoCitationData}[s0] type: article},
	number = {{arXiv}:1909.04759},
	abstract = {The network reconfiguration problem seeks to find a rooted tree
	            \$T\$ such that the energy of the (unique) feasible electrical
	            flow over \$T\$ is minimized. The tree requirement on the support
	            of the flow is motivated by operational constraints in
	            electricity distribution networks. The bulk of existing results
	            on convex optimization over vertices of polytopes and on the
	            structure of electrical flows do not easily give guarantees for
	            this problem, while many heuristic methods have been developed in
	            the power systems community as early as 1989. Our main
	            contribution is to give the first provable approximation
	            guarantees for the network reconfiguration problem. We provide
	            novel lower bounds and corresponding approximation factors for
	            various settings ranging from \${\textbackslash}min{
	            \textbackslash}\{O(m-n), O(n){\textbackslash}\}\$ for general
	            graphs, to \$O({\textbackslash}sqrt\{n\})\$ over grids with
	            uniform resistances on edges, and \$O(1)\$ for grids with uniform
	            edge resistances and demands. To obtain the result for general
	            graphs, we propose a new method for (approximate) spectral graph
	            sparsification, which may be of independent interest. Using
	            insights from our theoretical results, we propose a general
	            heuristic for the network reconfiguration problem that is orders
	            of magnitude faster than existing methods in the literature,
	            while obtaining comparable performance.},
	file = {:ElectricalFlowsover_Gupta2020 - Electrical Flows Over Spanning
	        Trees.pdf:PDF},
	groups = {laplacian-systems, graphical-models},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics -
	            Optimization and Control},
	priority = {prio1},
}

'
@report{BayesianSpanningTree_Duan2021,
	author = {Duan, Leo L. and Dunson, David B.},
	date = {2021-06-30},
	institution = {{arXiv}},
	title = {Bayesian Spanning Tree: Estimating the Backbone of the Dependence
	         Graph},
	doi = {10.48550/arXiv.2106.16120},
	eprint = {2106.16120},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000001 type: article},
	number = {{arXiv}:2106.16120},
	abstract = {In multivariate data analysis, it is often important to estimate
	            a graph characterizing dependence among (p) variables. A popular
	            strategy uses the non-zero entries in a (p{\textbackslash}times
	            p) covariance or precision matrix, typically requiring
	            restrictive modeling assumptions for accurate graph recovery. To
	            improve model robustness, we instead focus on estimating the \{{
	            \textbackslash}em backbone\} of the dependence graph. We use a
	            spanning tree likelihood, based on a minimalist graphical model
	            that is purposely overly-simplified. Taking a Bayesian approach,
	            we place a prior on the space of trees and quantify uncertainty
	            in the graphical model. In both theory and experiments, we show
	            that this model does not require the population graph to be a
	            spanning tree or the covariance to satisfy assumptions beyond
	            positive-definiteness. The model accurately recovers the backbone
	            of the population graph at a rate competitive with existing
	            approaches but with better robustness. We show combinatorial
	            properties of the spanning tree, which may be of independent
	            interest, and develop an efficient Gibbs sampler for Bayesian
	            inference. Analyzing electroencephalography data using a Hidden
	            Markov Model with each latent state modeled by a spanning tree,
	            we show that results are much more interpretable compared with
	            popular alternatives.},
	file = {:BayesianSpanningTree_Duan2021 - Bayesian Spanning Tree_ Estimating
	        the Backbone of the Dependence Graph.pdf:PDF},
	groups = {graphical-models, thesis},
	keywords = {Statistics - Methodology},
	priority = {prio1},
	shorttitle = {Bayesian Spanning Tree},
}

@article{ProbabilityAggregationMethods_Allard2012,
	author = {Allard, D. and Comunian, A. and Renard, P.},
	date = {2012-07-01},
	journaltitle = {Mathematical Geosciences},
	title = {Probability Aggregation Methods in Geoscience},
	doi = {10.1007/s11004-012-9396-3},
	issn = {1874-8953},
	note = {{ZSCC}: 0000100},
	number = {5},
	pages = {545--581},
	urldate = {2023-08-08},
	volume = {44},
	abstract = {The need for combining different sources of information in a
	            probabilistic framework is a frequent task in earth sciences.
	            This is a need that can be seen when modeling a reservoir using
	            direct geological observations, geophysics, remote sensing,
	            training images, and more. The probability of occurrence of a
	            certain lithofacies at a certain location for example can easily
	            be computed conditionally on the values observed at each source
	            of information. The problem of aggregating these different
	            conditional probability distributions into a single conditional
	            distribution arises as an approximation to the inaccessible
	            genuine conditional probability given all information. This paper
	            makes a formal review of most aggregation methods proposed so far
	            in the literature with a particular focus on their mathematical
	            properties. Exact relationships relating the different methods is
	            emphasized. The case of events with more than two possible
	            outcomes, never explicitly studied in the literature, is treated
	            in detail. It is shown that in this case, equivalence between
	            different aggregation formulas is lost. The concepts of
	            calibration, sharpness, and reliability, well known in the
	            weather forecasting community for assessing the goodness-of-fit
	            of the aggregation formulas, and a maximum likelihood estimation
	            of the aggregation parameters are introduced. We then prove that
	            parameters of calibrated log-linear pooling formulas are a
	            solution of the maximum likelihood estimation equations. These
	            results are illustrated on simulations from two common stochastic
	            models for earth science: the truncated Gaussian model and the
	            Boolean. It is found that the log-linear pooling provides the
	            best prediction while the linear pooling provides the worst.},
	file = {:ProbabilityAggregationMethods_Allard2012 - Probability Aggregation
	        Methods in Geoscience.pdf:PDF},
	keywords = {Data integration, Conditional probability pooling, Calibration,
	            Sharpness, Log-linear pooling},
	langid = {english},
	priority = {prio2},
	shortjournal = {Math Geosci},
}

@article{NoteResistanceDistances_Sun2022,
	author = {Sun, Wensheng and Yang, Yujun},
	date = {2022-04-01},
	journaltitle = {Frontiers in Physics},
	title = {A Note on Resistance Distances of Graphs},
	doi = {10.3389/fphy.2022.896886},
	note = {{ZSCC}: 0000001 {ADS} Bibcode: 2022FrP....10.6886S},
	pages = {896886},
	url = {https://ui.adsabs.harvard.edu/abs/2022FrP....10.6886S},
	urldate = {2023-08-10},
	volume = {10},
	abstract = {Let G be a connected graph with vertex set V(G). The resistance
	            distance between any two vertices u, v ∈ V(G) is the net
	            effective resistance between them in the electric network
	            constructed from G by replacing each edge with a unit resistor.
	            Let S ⊂ V(G) be a set of vertices such that all the vertices in S
	            have the same neighborhood in G − S, and let G[S] be the subgraph
	            induced by S. In this note, by the \{1\}-inverse of the Laplacian
	            matrix of G, formula for resistance distances between vertices in
	            S is obtained. It turns out that resistance distances between
	            vertices in S could be given in terms of elements in the inverse
	            matrix of an auxiliary matrix of the Laplacian matrix of G[S],
	            which derives the reduction principle obtained in [J. Phys. A:
	            Math. Theor. 41 (2008) 445203] by algebraic method.},
	file = {:NoteResistanceDistances_Sun2022 - A Note on Resistance Distances of
	        Graphs.pdf:PDF},
	groups = {laplacian-systems},
}

@phdthesis{Methodsdeterminingeffective_Vos2016,
	author = {Vos, Vaya Sapobi Samui and others},
	date = {2016},
	institution = {Master’s thesis, 20 December},
	title = {Methods for determining the effective resistance},
	file = {:Methodsdeterminingeffective_Vos2016 - Methods for Determining the
	        Effective Resistance.pdf:PDF},
	groups = {laplacian-systems},
}

@article{GaussianMeanShift_CarreiraPerpinan2007,
	author = {Carreira-Perpinan, Miguel A.},
	date = {2007-05},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine
	                Intelligence},
	title = {Gaussian Mean-Shift Is an {EM} Algorithm},
	doi = {10.1109/TPAMI.2007.1057},
	issn = {1939-3539},
	note = {{ZSCC}: 0000224},
	number = {5},
	pages = {767--776},
	volume = {29},
	abstract = {The mean-shift algorithm, based on ideas proposed by Fukunaga
	            and Hosteller, is a hill-climbing algorithm on the density
	            defined by a finite mixture or a kernel density estimate.
	            Mean-shift can be used as a nonparametric clustering method and
	            has attracted recent attention in computer vision applications
	            such as image segmentation or tracking. We show that, when the
	            kernel is Gaussian, mean-shift is an expectation-maximization ({
	            EM}) algorithm and, when the kernel is non-Gaussian, mean-shift
	            is a generalized {EM} algorithm. This implies that mean-shift
	            converges from almost any starting point and that, in general,
	            its convergence is of linear order. For Gaussian mean-shift, we
	            show: 1) the rate of linear convergence approaches 0 (superlinear
	            convergence) for very narrow or very wide kernels, but is often
	            close to 1 (thus, extremely slow) for intermediate widths and
	            exactly 1 (sublinear convergence) for widths at which modes merge
	            , 2) the iterates approach the mode along the local principal
	            component of the data points from the inside of the convex hull
	            of the data points, and 3) the convergence domains are nonconvex
	            and can be disconnected and show fractal behavior. We suggest
	            ways of accelerating mean-shift based on the {EM} interpretation},
	eventtitle = {{IEEE} Transactions on Pattern Analysis and Machine
	              Intelligence},
	file = {:GaussianMeanShift_CarreiraPerpinan2007 - Gaussian Mean Shift Is an
	        EM Algorithm.pdf:PDF},
	groups = {graphical-models},
	keywords = {Clustering algorithms, Convergence, Kernel, Clustering methods,
	            Computer vision, Application software, Image segmentation, Image
	            converters, Fractals, Acceleration, Mean-shift algorithm,
	            Gaussian mixtures, kernel density estimators, {EM} algorithm,
	            clustering.},
}

@article{GraphLaplacianRegularization_Pang2017,
	author = {Pang, Jiahao and Cheung, Gene},
	date = {2017-04},
	journaltitle = {{IEEE} Transactions on Image Processing},
	title = {Graph Laplacian Regularization for Image Denoising: Analysis in the
	         Continuous Domain},
	doi = {10.1109/TIP.2017.2651400},
	issn = {1941-0042},
	note = {{ZSCC}: 0000231},
	number = {4},
	pages = {1770--1785},
	volume = {26},
	abstract = {Inverse imaging problems are inherently underdetermined, and
	            hence, it is important to employ appropriate image priors for
	            regularization. One recent popular prior-the graph Laplacian
	            regularizer-assumes that the target pixel patch is smooth with
	            respect to an appropriately chosen graph. However, the mechanisms
	            and implications of imposing the graph Laplacian regularizer on
	            the original inverse problem are not well understood. To address
	            this problem, in this paper, we interpret neighborhood graphs of
	            pixel patches as discrete counterparts of Riemannian manifolds
	            and perform analysis in the continuous domain, providing insights
	            into several fundamental aspects of graph Laplacian
	            regularization for image denoising. Specifically, we first show
	            the convergence of the graph Laplacian regularizer to a
	            continuous-domain functional, integrating a norm measured in a
	            locally adaptive metric space. Focusing on image denoising, we
	            derive an optimal metric space assuming non-local self-similarity
	            of pixel patches, leading to an optimal graph Laplacian
	            regularizer for denoising in the discrete domain. We then
	            interpret graph Laplacian regularization as an anisotropic
	            diffusion scheme to explain its behavior during iterations, e.g.,
	            its tendency to promote piecewise smooth signals under certain
	            settings. To verify our analysis, an iterative image denoising
	            algorithm is developed. Experimental results show that our
	            algorithm performs competitively with state-of-the-art denoising
	            methods, such as {BM}3D for natural images, and outperforms them
	            significantly for piecewise smooth images.},
	eventtitle = {{IEEE} Transactions on Image Processing},
	file = {:GraphLaplacianRegularization_Pang2017 - Graph Laplacian
	        Regularization for Image Denoising_ Analysis in the Continuous
	        Domain.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {Laplace equations, Noise reduction, Image denoising, Measurement
	            , Imaging, Anisotropic magnetoresistance, Manifolds, Graph
	            Laplacian regularization, graph signal processing, image
	            denoising},
	priority = {prio2},
	shorttitle = {Graph Laplacian Regularization for Image Denoising},
}

@article{PredictionRiemannianmetrics_Gzyl2022,
	author = {Gzyl, Henryk},
	date = {2022-01-17},
	journaltitle = {Communications in Statistics - Theory and Methods},
	title = {Prediction in Riemannian metrics derived from divergence functions},
	doi = {10.1080/03610926.2020.1752384},
	issn = {0361-0926},
	note = {{ZSCC}: 0000002},
	number = {2},
	pages = {552--568},
	urldate = {2023-08-31},
	volume = {51},
	abstract = {Divergence functions are interesting and widely used discrepancy
	            measures. Even though they are not true distances we can use them
	            to measure how separated two points are. Curiously enough, when
	            they are applied to random variables they lead to a notion of
	            best predictor that coincides with usual best predictor in
	            Euclidean distance. From a divergence function we can derive a
	            Riemannian metric which leads to a true distance between random
	            variables, and the best predictors in this metric do not coincide
	            with their Euclidean counterparts. It is the purpose of this
	            paper to explicitly determine the best predictors in the derived
	            metric, compare it to the estimators in divergence, and to obtain
	            the sample estimators of the best predictors. Along the way we
	            obtain results that relate approximations in divergence to
	            approximations in the metric derived from it.},
	file = {:PredictionRiemannianmetrics_Gzyl2022 - Prediction in Riemannian
	        Metrics Derived from Divergence Functions.pdf:PDF},
	groups = {Research Topics},
	keywords = {Generalized best predictors, prediction in Non-Euclidean metrics
	            , Bregman divergence, hessian metrics, Riemannian distances,
	            60G25, 60G99, 93E24, 62A99},
	publisher = {Taylor \& Francis},
}

@report{Predictionlogarithmicdistance_Gzyl2018,
	author = {Gzyl, Henryk},
	date = {2018-09-19},
	institution = {{arXiv}},
	title = {Prediction in logarithmic distance},
	doi = {10.48550/arXiv.1703.08696},
	eprint = {1703.08696},
	eprinttype = {arxiv},
	note = {{ZSCC}: {NoCitationData}[s0] type: article},
	number = {{arXiv}:1703.08696},
	abstract = {The metric properties of the set in which random variables take
	            their values lead to relevant probabilistic concepts. For example
	            , the mean of a random variable is a best predictor in that it
	            minimizes the standard Euclidean distance or \$L\_2\$ norm in an
	            appropriate class of random variables. Similarly, the median is
	            the same concept but when the distance is measured by the \$L\_1
	            \$ norm. These two predictors stem from the fact that the mean
	            and the median, minimize the distance to a given set of points
	            when distances in \${\textbackslash}mathbb\{R\}\$ or in \${
	            \textbackslash}mathbb\{R\}{\textasciicircum}n\$ are measured in
	            the aforementioned metrics.{\textbackslash}{\textbackslash} It so
	            happens that an interesting \{{\textbackslash}it logarithmic
	            distance\} can be defined on the cone of strictly positive
	            vectors in \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$ in
	            such a way that the minimizer of the distance to a collection of
	            points is their geometric mean.{\textbackslash}{\textbackslash}
	            This distance on the base space leads to an interesting distance
	            on the class of strictly positive random variables, which in turn
	            leads to an interesting class of best predictors predictors and
	            their estimators, as well as a corresponding notion of
	            conditional expectation. The appropriate version of the Law of
	            Large Numbers and the Central Limit Theorem, can also be
	            obtained. We shall see that, for example, the lognormal variables
	            are the analogue of the Gaussian variables for the modified
	            version of the Central Limit Theorem.},
	file = {:Predictionlogarithmicdistance_Gzyl2018 - Prediction in Logarithmic
	        Distance.pdf:PDF},
	groups = {Research Topics},
	keywords = {Mathematics - Probability},
}

@inproceedings{FeatureSelectionKernel_Fung2007,
	author = {Fung, Glenn and Rosales, Romer and Rao, R Bharat},
	booktitle = {IJCAI},
	date = {2007},
	title = {Feature Selection and Kernel Design via Linear Programming.},
	organization = {Citeseer},
	pages = {786--791},
	file = {:FeatureSelectionKernel_Fung2007 - Feature Selection and Kernel
	        Design Via Linear Programming..pdf:PDF},
	groups = {incidence structures},
}

@techreport{optimalassignmentconference_Taylor2008,
	author = {Taylor, Camillo J},
	date = {2008},
	institution = {University of Pennsylvania Department of Computer and
	               Information Science},
	title = {On the optimal assignment of conference papers to reviewers},
	number = {MS-CIS-08-30},
	file = {:optimalassignmentconference_Taylor2008 - On the Optimal Assignment
	        of Conference Papers to Reviewers.pdf:PDF},
	groups = {Research Topics},
}

@inproceedings{Poissonmatrixcompletion_Cao2015,
	author = {Cao, Yang and Xie, Yao},
	booktitle = {2015 {IEEE} International Symposium on Information Theory ({
	             ISIT})},
	date = {2015-06},
	title = {Poisson matrix completion},
	doi = {10.1109/ISIT.2015.7282774},
	eventtitle = {2015 {IEEE} International Symposium on Information Theory ({
	              ISIT})},
	note = {{ZSCC}: 0000063 {ISSN}: 2157-8117},
	pages = {1841--1845},
	abstract = {We extend the theory of matrix completion to the case where we
	            make Poisson observations for a subset of entries of a low-rank
	            matrix. We consider the (now) usual matrix recovery formulation
	            through maximum likelihood with proper constraints on the matrix
	            M of size d1-by-d2, and establish theoretical upper and lower
	            bounds on the recovery error. Our bounds are nearly optimal up to
	            a factor on the order of O(log(d1d2)). These bounds are obtained
	            by adapting the arguments used for one-bit matrix completion [1]
	            (although these two problems are different in nature) and the
	            adaptation requires new techniques exploiting properties of the
	            Poisson likelihood function and tackling the difficulties posed
	            by the locally sub-Gaussian characteristic of the Poisson
	            distribution. Our results highlight a few important distinctions
	            of Poisson matrix completion compared to the prior work in matrix
	            completion including having to impose a minimum signal-to-noise
	            requirement on each observed entry. We also develop an efficient
	            iterative algorithm and demonstrate its good performance in
	            recovering solar flare images.},
	file = {:Poissonmatrixcompletion_Cao2015 - Poisson Matrix Completion.pdf:PDF
	        },
	groups = {Research Topics},
	issn = {2157-8117},
	keywords = {Upper bound, Sparse matrices, Signal to noise ratio, Signal
	            processing algorithms, Random variables, Compressed sensing,
	            matrix completion, Poisson noise, high-dimensional statistics,
	            information theory},
}

@report{Estimatingdistributionthinning_Anevski2018,
	author = {Anevski, Dragi and Pastukhov, Vladimir},
	date = {2018-08-28},
	institution = {{arXiv}},
	title = {Estimating the distribution and thinning parameters of a
	         homogeneous multimode Poisson process},
	doi = {10.48550/arXiv.1808.09448},
	eprint = {1808.09448},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000001 type: article},
	number = {{arXiv}:1808.09448},
	abstract = {In this paper we propose estimators of the distribution of
	            events of different kinds in a multimode Poisson process. We give
	            the explicit solution for the maximum likelihood estimator, and
	            derive its strong consistency and asymptotic normality. We also
	            provide an order restricted estimator and derive its consistency
	            and asymptotic distribution. We discuss the application of the
	            estimator to the detection of neutrons in a novel detector being
	            developed at the European Spallation Source in Lund, Sweden. The
	            inference problem gives rise to Sylvester-Ramanujan system of
	            equations.},
	file = {:Estimatingdistributionthinning_Anevski2018 - Estimating the
	        Distribution and Thinning Parameters of a Homogeneous Multimode
	        Poisson Process.pdf:PDF},
	groups = {Research Topics},
	keywords = {Statistics - Methodology, Statistics - Applications},
}

@inproceedings{PoissonMatRemodelingMatrix_Wang2022,
	author = {Wang, Hao},
	booktitle = {2022 International Conference on Machine Learning and
	             Intelligent Systems Engineering ({MLISE})},
	date = {2022-08},
	title = {{PoissonMat}: Remodeling Matrix Factorization using Poisson
	         Distribution and Solving the Cold Start Problem without Input Data},
	doi = {10.1109/MLISE57402.2022.00055},
	eventtitle = {2022 International Conference on Machine Learning and
	              Intelligent Systems Engineering ({MLISE})},
	note = {{ZSCC}: 0000006},
	pages = {245--249},
	abstract = {Matrix Factorization is one of the most successful recommender
	            system techniques over the past decade. However, the classic
	            probabilistic theory framework for matrix factorization is
	            modeled using normal distributions. To find better probabilistic
	            models, algorithms such as {RankMat}, {ZeroMat} and {DotMat} have
	            been invented in recent years. In this paper, we model the user
	            rating behavior in recommender system as a Poisson process, and
	            design an algorithm that relies on no input data to solve the
	            recommendation problem and the cold start issue at the same time.
	            We prove the superiority of our algorithm in comparison with
	            matrix factorization, random placement, Zipf placement, {ZeroMat}
	            , {DotMat}, etc.},
	file = {:PoissonMatRemodelingMatrix_Wang2022 - PoissonMat_ Remodeling Matrix
	        Factorization Using Poisson Distribution and Solving the Cold Start
	        Problem without Input Data.pdf:PDF},
	groups = {Research Topics},
	keywords = {Machine learning, Gaussian distribution, Probabilistic logic,
	            Data models, Behavioral sciences, Intelligent systems,
	            Recommender systems, recommender system, Poisson process,
	            cold-start problem, {PoissonMat}, {PoissonMat} Hybrid},
	shorttitle = {{PoissonMat}},
}

@article{Foundationalconceptsperson_Greenberg2023,
	author = {Ariel M. Greenberg and Julie L. Marble},
	date = {2023-01},
	journaltitle = {Frontiers in Physics},
	title = {Foundational concepts in person-machine teaming},
	doi = {10.3389/fphy.2022.1080132},
	volume = {10},
	file = {:Foundationalconceptsperson_Greenberg2023 - Foundational Concepts in
	        Person Machine Teaming.pdf:PDF},
	groups = {people},
	publisher = {Frontiers Media {SA}},
}

@article{Intersubjectivityhumanagent_Cassell2007,
	author = {Cassell, Justine and Tartaro, Andrea},
	date = {2007-01-01},
	journaltitle = {Interaction Studies},
	title = {Intersubjectivity in human-agent interaction},
	doi = {10.1075/is.8.3.05cas},
	issn = {1572-0373, 1572-0381},
	note = {{ZSCC}: 0000059},
	number = {3},
	pages = {391--410},
	urldate = {2023-10-11},
	volume = {8},
	abstract = {What is the hallmark of success in human–agent interaction? In
	            animation and robotics, many have concentrated on the looks of
	            the agent — whether the appearance is realistic or lifelike. We
	            present an alternative benchmark that lies in the dyad and not
	            the agent alone: Does the agent’s behavior evoke
	            intersubjectivity from the user? That is, in both conscious and
	            unconscious communication, do users react to behaviorally
	            realistic agents in the same way they react to other humans? Do
	            users appear to attribute similar thoughts and actions? We
	            discuss why we distinguish between appearance and behavior, why
	            we use the benchmark of intersubjectivity, our methodology for
	            applying this benchmark to embodied conversational agents ({ECAs}
	            ), and why we believe this benchmark should be applied to
	            human–robot interaction.},
	file = {:Intersubjectivityhuman–agentinteraction_Cassell2007 -
	        Intersubjectivity in Human–agent Interaction.pdf:PDF},
	groups = {people},
	howpublished = {Text},
	langid = {english},
	publisher = {John Benjamins},
	type = {Text},
}

@article{VygotskysZoneProximal_Shabani2010,
	author = {Shabani, Karim and Khatib, Mohamad and Ebadi, Saman},
	date = {2010},
	journaltitle = {English Language Teaching},
	title = {Vygotsky's Zone of Proximal Development: Instructional Implications
	         and Teachers' Professional Development},
	number = {4},
	url = {https://EconPapers.repec.org/RePEc:ibn:eltjnl:v:3:y:2010:i:4:p:237},
	volume = {3},
	abstract = {The current paper examines the instructional implications of
	            Vygotsky's (1978) seminal notion of Zone of Proximal Development,
	            originally developed to account for the learning potential of
	            children, and investigates ZPD applications to the concept of
	            teacher professional development. Specific attempt has been made
	            to see how a number of assets at the teacher's disposal namely
	            diary writing, peer and mentor collaboration, action research,
	            practicum and TESOL discourse can serve as scaffolders to affect
	            the progression of ZPD in language teachers. The contributions of
	            ZPD to the concepts of scaffolding and dynamic assessment (DA)
	            are explored extensively and the controversial issues are
	            addressed. There is a consensus that the notion of the zone of
	            proximal development and socio-cultural theory of mind based on
	            Vygotskyâ€™s ideas are at the heart of the notion of scaffolding
	            .This study highlights the limitations of the metaphor of
	            scaffolding in interpreting the zone of proximal development. The
	            concept of ZPD, as seen through the approach of DA, offers an
	            operational view of the learnersâ€™ actual level of development
	            and a measure of emerging and imminent development. Utilizing the
	            concept of ZPD, DA unites traditional assessment, instruction,
	            intervention, and remediation. Though the concept of ZPD provides
	            an attractive metaphor for designing instruction and analyzing
	            learning, it poses a real challenge when put into practice. The
	            present research highlights a procedure to provide a more
	            tangible account of ZPD, but research on this area is scanty and
	            further explorations and investigations are needed to reflect the
	            implications of ZPD in instructional context.},
	file = {:VygotskysZoneProximal_Shabani2010 - Vygotsky's Zone of Proximal
	        Development_ Instructional Implications and Teachers' Professional
	        Development.pdf:PDF},
	groups = {people},
}

@article{ScaffoldingHumanChampions_Saetra2022,
	author = {Saetra, Henrik Skaug},
	date = {2022-08-09},
	journaltitle = {Human Arenas},
	title = {Scaffolding Human Champions: {AI} as a More Competent Other},
	doi = {10.1007/s42087-022-00304-8},
	issn = {2522-5804},
	note = {{ZSCC}: 0000003},
	urldate = {2023-10-11},
	abstract = {Artificial intelligence ({AI}) has surpassed humans in a number
	            of specialised intellectual activities—chess and Go being two of
	            many examples. Amongst the many potential consequences of such a
	            development, I focus on how we can utilise cutting edge {AI} to
	            promote human learning. The purpose of this article is to explore
	            how a specialised {AI} can be utilised in a manner that promotes
	            human growth by acting as a tutor to our champions. A framework
	            for using {AI} as a tutor of human champions based on Vygotsky’s
	            theory of human learning is here presented. It is based on a
	            philosophical analysis of {AI} capabilities, key aspects of
	            Vygotsky’s theory of human learning, and existing research on
	            intelligent tutoring systems. The main method employed is the
	            theoretical development of a generalised framework for {AI}
	            powered expert learning systems, using chess and Go as examples.
	            In addition to this, data from public interviews with top
	            professionals in the games of chess and Go are used to examine
	            the feasibility and realism of using {AI} in such a manner.
	            Basing the analysis on Vygotsky’s socio-cultural theory of
	            development, I explain how {AI} operates in the zone of proximal
	            development of our champions and how even non-educational {AI}
	            systems can perform certain scaffolding functions. I then argue
	            that {AI} combined with basic modules from intelligent tutoring
	            systems could perform even more scaffolding functions, but that
	            the most interesting constellation right now is scaffolding by a
	            group consisting of {AI} in combination with human peers and
	            instructors.},
	file = {:ScaffoldingHumanChampions_Saetra2022 - Scaffolding Human Champions_
	        AI As a More Competent Other.pdf:PDF},
	groups = {people},
	keywords = {Artificial intelligence, Zone of proximal development,
	            Scaffolding, Tutoring, Chess},
	langid = {english},
	shortjournal = {Hu Arenas},
	shorttitle = {Scaffolding Human Champions},
}

@article{vicariousvirtualVygotskian_Taber2021,
	author = {Taber, Keith S and Li, Xinyue},
	date = {2021},
	journaltitle = {Advances in psychology research},
	title = {The vicarious and the virtual: A Vygotskian perspective on digital
	         learning resources as tools for scaffolding conceptual development},
	pages = {1--72},
	volume = {143},
	file = {:vicariousvirtualVygotskian_Taber2021 - The Vicarious and the
	        Virtual_ a Vygotskian Perspective on Digital Learning Resources As
	        Tools for Scaffolding Conceptual Development.pdf:PDF},
	groups = {people},
}

@report{InterpretableReinforcementLearning_Hakimzadeh2021,
	author = {Hakimzadeh, Aref and Xue, Yanbo and Setoodeh, Peyman},
	date = {2021-01-31},
	institution = {{arXiv}},
	title = {Interpretable Reinforcement Learning Inspired by Piaget's Theory of
	         Cognitive Development},
	doi = {10.48550/arXiv.2102.00572},
	eprint = {2102.00572},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000006 type: article},
	number = {{arXiv}:2102.00572},
	abstract = {Endeavors for designing robots with human-level cognitive
	            abilities have led to different categories of learning machines.
	            According to Skinner's theory, reinforcement learning ({RL})
	            plays a key role in human intuition and cognition. Majority of
	            the state-of-the-art methods including deep {RL} algorithms are
	            strongly influenced by the connectionist viewpoint. Such
	            algorithms can significantly benefit from theories of mind and
	            learning in other disciplines. This paper entertains the idea
	            that theories such as language of thought hypothesis ({LOTH}),
	            script theory, and Piaget's cognitive development theory provide
	            complementary approaches, which will enrich the {RL} field.
	            Following this line of thinking, a general computational building
	            block is proposed for Piaget's schema theory that supports the
	            notions of productivity, systematicity, and inferential coherence
	            as described by Fodor in contrast with the connectionism theory.
	            Abstraction in the proposed method is completely upon the system
	            itself and is not externally constrained by any predefined
	            architecture. The whole process matches the Neisser's perceptual
	            cycle model. Performed experiments on three typical control
	            problems followed by behavioral analysis confirm the
	            interpretability of the proposed method and its competitiveness
	            compared to the state-of-the-art algorithms. Hence, the proposed
	            framework can be viewed as a step towards achieving human-like
	            cognition in artificial intelligent systems.},
	file = {:InterpretableReinforcementLearning_Hakimzadeh2021 - Interpretable
	        Reinforcement Learning Inspired by Piaget's Theory of Cognitive
	        Development.pdf:PDF},
	groups = {people},
	keywords = {Computer Science - Artificial Intelligence},
}

@report{Bistochasticallynormalized_Cheng2023,
	author = {Cheng, Xiuyuan and Landa, Boris},
	date = {2023-01-26},
	institution = {{arXiv}},
	title = {Bi-stochastically normalized graph Laplacian: convergence to
	         manifold Laplacian and robustness to outlier noise},
	doi = {10.48550/arXiv.2206.11386},
	eprint = {2206.11386},
	eprinttype = {arxiv},
	note = {{ZSCC}: {NoCitationData}[s0] type: article},
	number = {{arXiv}:2206.11386},
	abstract = {Bi-stochastic normalization provides an alternative
	            normalization of graph Laplacians in graph-based data analysis
	            and can be computed efficiently by Sinkhorn-Knopp ({SK})
	            iterations. This paper proves the convergence of
	            bi-stochastically normalized graph Laplacian to manifold
	            (weighted-)Laplacian with rates, when \$n\$ data points are
	            i.i.d. sampled from a general \$d\$-dimensional manifold embedded
	            in a possibly high-dimensional space. Under certain joint limit
	            of \$n {\textbackslash}to {\textbackslash}infty\$ and kernel
	            bandwidth \${\textbackslash}epsilon {\textbackslash}to 0\$, the
	            point-wise convergence rate of the graph Laplacian operator
	            (under 2-norm) is proved to be \$ O( n{\textasciicircum}\{
	            -1/(d/2+3)\})\$ at finite large \$n\$ up to log factors, achieved
	            at the scaling of \${\textbackslash}epsilon {\textbackslash}sim n
	            {\textasciicircum}\{-1/(d/2+3)\} \$. When the manifold data are
	            corrupted by outlier noise, we theoretically prove the graph
	            Laplacian point-wise consistency which matches the rate for clean
	            manifold data plus an additional term proportional to the
	            boundedness of the inner-products of the noise vectors among
	            themselves and with data vectors. Motivated by our analysis,
	            which suggests that not exact bi-stochastic normalization but an
	            approximate one will achieve the same consistency rate, we
	            propose an approximate and constrained matrix scaling problem
	            that can be solved by {SK} iterations with early termination.
	            Numerical experiments support our theoretical results and show
	            the robustness of bi-stochastically normalized graph Laplacian to
	            high-dimensional outlier noise.},
	file = {:Bistochasticallynormalized_Cheng2023 - Bi Stochastically Normalized
	        Graph Laplacian_ Convergence to Manifold Laplacian and Robustness to
	        Outlier Noise.pdf:PDF},
	groups = {laplacian-systems},
	keywords = {Mathematics - Statistics Theory, Computer Science - Machine
	            Learning, Statistics - Machine Learning},
	shorttitle = {Bi-stochastically normalized graph Laplacian},
}

@article{RobustInferenceManifold_Landa2023,
	author = {Landa, Boris and Cheng, Xiuyuan},
	date = {2023-09-30},
	journaltitle = {{SIAM} Journal on Mathematics of Data Science},
	title = {Robust Inference of Manifold Density and Geometry by Doubly
	         Stochastic Scaling},
	doi = {10.1137/22M1516968},
	note = {{ZSCC}: 0000002},
	number = {3},
	pages = {589--614},
	urldate = {2023-10-12},
	volume = {5},
	abstract = {A fundamental step in many data-analysis techniques is the
	            construction of an affinity matrix describing similarities
	            between data points. When the data points reside in Euclidean
	            space, a widespread approach is to form an affinity matrix by the
	            Gaussian kernel with pairwise distances, and to follow with a
	            certain normalization (e.g., the row-stochastic normalization or
	            its symmetric variant). We demonstrate that the doubly stochastic
	            normalization of the Gaussian kernel with zero main diagonal
	            (i.e., no self-loops) is robust to heteroskedastic noise. That is
	            , the doubly stochastic normalization is advantageous in that it
	            automatically accounts for observations with different noise
	            variances. Specifically, we prove that in a suitable
	            high-dimensional setting where heteroskedastic noise does not
	            concentrate too much in any particular direction in space, the
	            resulting (doubly stochastic) noisy affinity matrix converges to
	            its clean counterpart with rate \$m{\textasciicircum}\{-1/2\}\$,
	            where \$m\$ is the ambient dimension. We demonstrate this result
	            numerically and show that, in contrast, the popular
	            row-stochastic and symmetric normalizations behave unfavorably
	            under heteroskedastic noise. Furthermore, we provide examples of
	            simulated and experimental single-cell {RNA} sequence data with
	            intrinsic heteroskedasticity, where the advantage of the doubly
	            stochastic normalization for exploratory analysis is evident.},
	file = {:RobustInferenceManifold_Landa2023 - Robust Inference of Manifold
	        Density and Geometry by Doubly Stochastic Scaling.pdf:PDF},
	groups = {laplacian-systems},
	publisher = {Society for Industrial and Applied Mathematics},
}

@article{NetgraphPublicationquality_Brodersen2023,
	author = {Brodersen, Paul J. n},
	date = {2023-07-19},
	journaltitle = {Journal of Open Source Software},
	title = {Netgraph: Publication-quality Network Visualisations in Python},
	doi = {10.21105/joss.05372},
	issn = {2475-9066},
	note = {{ZSCC}: 0000000},
	number = {87},
	pages = {5372},
	urldate = {2023-10-12},
	volume = {8},
	abstract = {Brodersen, P. J., (2023). Netgraph: Publication-quality Network
	            Visualisations in Python. Journal of Open Source Software, 8(87),
	            5372, https://doi.org/10.21105/joss.05372},
	file = {:NetgraphPublicationquality_Brodersen2023 - Netgraph_ Publication
	        Quality Network Visualisations in Python.pdf:PDF},
	groups = {Tree Editing Interface},
	langid = {english},
	shorttitle = {Netgraph},
}

@article{ReconstructingNetworksUnknown_Peixoto2018,
	author = {Peixoto, Tiago P.},
	date = {2018-10-16},
	journaltitle = {Physical Review X},
	title = {Reconstructing Networks with Unknown and Heterogeneous Errors},
	doi = {10.1103/PhysRevX.8.041011},
	note = {{ZSCC}: 0000099},
	number = {4},
	pages = {041011},
	urldate = {2023-11-02},
	volume = {8},
	abstract = {The vast majority of network data sets contain errors and
	            omissions, although this fact is rarely incorporated in
	            traditional network analysis. Recently, an increasing effort has
	            been made to fill this methodological gap by developing
	            network-reconstruction approaches based on Bayesian inference.
	            These approaches, however, rely on assumptions of uniform error
	            rates and on direct estimations of the existence of each edge via
	            repeated measurements, something that is currently unavailable
	            for the majority of network data. Here, we develop a Bayesian
	            reconstruction approach that lifts these limitations by allowing
	            for not only heterogeneous errors, but also for single edge
	            measurements without direct error estimates. Our approach works
	            by coupling the inference approach with structured generative
	            network models, which enable the correlations between edges to be
	            used as reliable uncertainty estimates. Although our approach is
	            general, we focus on the stochastic block model as the basic
	            generative process, from which efficient nonparametric inference
	            can be performed and yields a principled method to infer
	            hierarchical community structure from noisy data. We demonstrate
	            the efficacy of our approach with a variety of empirical and
	            artificial networks.},
	file = {:ReconstructingNetworksUnknown_Peixoto2018 - Reconstructing Networks
	        with Unknown and Heterogeneous Errors.pdf:PDF;:https\:
	        //journals.aps.org/prx/pdf/10.1103/PhysRevX.8.041011:},
	groups = {graphs, thesis},
	publisher = {American Physical Society},
	shortjournal = {Phys. Rev. X},
}

@article{NetworkReconstructionCommunity_Peixoto2019,
	author = {Peixoto, Tiago P.},
	date = {2019-09-18},
	journaltitle = {Physical Review Letters},
	title = {Network Reconstruction and Community Detection from Dynamics},
	doi = {10.1103/PhysRevLett.123.128301},
	note = {{ZSCC}: 0000115},
	number = {12},
	pages = {128301},
	urldate = {2023-11-02},
	volume = {123},
	abstract = {We present a scalable nonparametric Bayesian method to perform
	            network reconstruction from observed functional behavior that at
	            the same time infers the communities present in the network. We
	            show that the joint reconstruction with community detection has a
	            synergistic effect, where the edge correlations used to inform
	            the existence of communities are also inherently used to improve
	            the accuracy of the reconstruction which, in turn, can better
	            inform the uncovering of communities. We illustrate the use of
	            our method with observations arising from epidemic models and the
	            Ising model, both on synthetic and empirical networks, as well as
	            on data containing only functional information.},
	file = {:NetworkReconstructionCommunity_Peixoto2019 - Network Reconstruction
	        and Community Detection from Dynamics.pdf:PDF},
	groups = {graphs, thesis},
	publisher = {American Physical Society},
	shortjournal = {Phys. Rev. Lett.},
}

@book{atlasaspiringnetwork_Coscia2021,
	author = {Coscia, Michele},
	date = {2021-01},
	title = {The atlas for the aspiring network scientist},
	eprint = {2101.00863},
	eprinttype = {arxiv},
	publisher = {Michele Coscia},
	file = {:atlasaspiringnetwork_Coscia2021 - The Atlas for the Aspiring
	        Network Scientist.pdf:PDF},
	groups = {graphs, thesis},
}

@inproceedings{Quickshiftkernel_Vedaldi2008,
	author = {Vedaldi, Andrea and Soatto, Stefano},
	booktitle = {Computer Vision--ECCV 2008: 10th European Conference on
	             Computer Vision, Marseille, France, October 12-18, 2008,
	             Proceedings, Part IV 10},
	date = {2008},
	title = {Quick shift and kernel methods for mode seeking},
	doi = {10.1007/978-3-540-88693-8_52},
	organization = {Springer},
	pages = {705--718},
	file = {:Quickshiftkernel_Vedaldi2008 - Quick Shift and Kernel Methods for
	        Mode Seeking.pdf:PDF},
	groups = {Research Topics},
}

@inproceedings{ModeseekingMedoidshifts_Sheikh2007,
	author = {Sheikh, Yaser Ajmal and Khan, Erum Arif and Kanade, Takeo},
	booktitle = {2007 {IEEE} 11th International Conference on Computer Vision},
	date = {2007-10},
	title = {Mode-seeking by Medoidshifts},
	doi = {10.1109/ICCV.2007.4408978},
	eventtitle = {2007 {IEEE} 11th International Conference on Computer Vision},
	note = {{ZSCC}: 0000236 {ISSN}: 2380-7504},
	pages = {1--8},
	url = {https://ieeexplore.ieee.org/abstract/document/4408978},
	urldate = {2023-11-07},
	abstract = {We present a nonparametric mode-seeking algorithm, called
	            medoidshift, based on approximating the local gradient using a
	            weighted estimate of medoids. Like meanshift, medoidshift
	            clustering automatically computes the number of clusters and the
	            data does not have to be linearly separable. Unlike meanshift,
	            the proposed algorithm does not require the definition of a mean.
	            This property allows medoidshift to find modes even when only a
	            distance measure between samples is defined. In this sense, the
	            relationship between the medoidshift algorithm and the meanshift
	            algorithm is similar to the relationship between the k-medoids
	            and the k-means algorithms. We show that medoidshifts can also be
	            used for incremental clustering of growing datasets by recycling
	            previous computations. We present experimental results using
	            medoidshift for image segmentation, incremental clustering for
	            shot segmentation and clustering on nonlinearly separable data.},
	file = {:ModeseekingMedoidshifts_Sheikh2007 - Mode Seeking by
	        Medoidshifts.pdf:PDF},
	groups = {Research Topics},
	issn = {2380-7504},
}

@article{Meanshiftmode_Cheng1995,
	author = {Cheng, Yizong},
	date = {1995-08},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine
	                Intelligence},
	title = {Mean shift, mode seeking, and clustering},
	doi = {10.1109/34.400568},
	issn = {1939-3539},
	note = {{ZSCC}: 0005678},
	number = {8},
	pages = {790--799},
	url = {https://ieeexplore.ieee.org/abstract/document/400568},
	urldate = {2023-11-07},
	volume = {17},
	abstract = {Mean shift, a simple interactive procedure that shifts each data
	            point to the average of data points in its neighborhood is
	            generalized and analyzed in the paper. This generalization makes
	            some k-means like clustering algorithms its special cases. It is
	            shown that mean shift is a mode-seeking process on the surface
	            constructed with a "shadow" kernal. For Gaussian kernels, mean
	            shift is a gradient mapping. Convergence is studied for mean
	            shift iterations. Cluster analysis if treated as a deterministic
	            problem of finding a fixed point of mean shift that characterizes
	            the data. Applications in clustering and Hough transform are
	            demonstrated. Mean shift is also considered as an evolutionary
	            strategy that performs multistart global optimization.{\textless}
	            {\textgreater}},
	eventtitle = {{IEEE} Transactions on Pattern Analysis and Machine
	              Intelligence},
	file = {:Meanshiftmode_Cheng1995 - Mean Shift, Mode Seeking, and
	        Clustering.pdf:PDF},
	groups = {Research Topics},
}

@article{BetaProcessesStick_Broderick2012,
	author = {Broderick, Tamara and Jordan, Michael I. and Pitman, Jim},
	date = {2012-06},
	journaltitle = {Bayesian Analysis},
	title = {Beta Processes, Stick-Breaking and Power Laws},
	doi = {10.1214/12-BA715},
	issn = {1936-0975, 1931-6690},
	note = {{ZSCC}: 0000109},
	number = {2},
	pages = {439--476},
	urldate = {2024-01-18},
	volume = {7},
	abstract = {The beta-Bernoulli process provides a Bayesian nonparametric
	            prior for models involving collections of binary-valued features.
	            A draw from the beta process yields an infinite collection of
	            probabilities in the unit interval, and a draw from the Bernoulli
	            process turns these into binary-valued features. Recent work has
	            provided stick-breaking representations for the beta process
	            analogous to the well-known stick-breaking representation for the
	            Dirichlet process. We derive one such stick-breaking
	            representation directly from the characterization of the beta
	            process as a completely random measure. This approach motivates a
	            three-parameter generalization of the beta process, and we study
	            the power laws that can be obtained from this generalized beta
	            process. We present a posterior inference algorithm for the
	            beta-Bernoulli process that exploits the stick-breaking
	            representation, and we present experimental results for a
	            discrete factor-analysis model.},
	file = {:BetaProcessesStick_Broderick2012 - Beta Processes, Stick Breaking
	        and Power Laws.pdf:PDF},
	groups = {incidence structures},
	keywords = {beta process, power law, stick-breaking},
	publisher = {International Society for Bayesian Analysis},
}

@article{Inferringnetworksdiffusion_GomezRodriguez2012,
	author = {Gomez-Rodriguez, Manuel and Leskovec, Jure and Krause, Andreas},
	date = {2012},
	journaltitle = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
	title = {Inferring networks of diffusion and influence},
	doi = {10.1145/2086737.2086741},
	issn = {1556-472X},
	number = {4},
	pages = {1--37},
	volume = {5},
	file = {:Inferringnetworksdiffusion_GomezRodriguez2012 - Inferring Networks
	        of Diffusion and Influence.pdf:PDF},
	groups = {graphs},
	publisher = {ACM New York, NY, USA},
}

@inproceedings{RepresentationLearningInformation_Bourigault2016,
	author = {Bourigault, Simon and Lamprier, Sylvain and Gallinari, Patrick},
	booktitle = {Proceedings of the Ninth ACM International Conference on Web
	             Search and Data Mining},
	date = {2016-02},
	title = {Representation Learning for Information Diffusion through Social
	         Networks: an Embedded Cascade Model},
	doi = {10.1145/2835776.2835817},
	publisher = {ACM},
	series = {WSDM 2016},
	collection = {WSDM 2016},
	groups = {graphs},
}

@article{Humanmemorysearch_Jun2015,
	author = {Jun, Kwang-Sung and Zhu, Jerry and Rogers, Timothy T and Yang,
	          Zhuoran and others},
	date = {2015},
	journaltitle = {Advances in neural information processing systems},
	title = {Human memory search as initial-visit emitting random walk},
	doi = {10.1016/j.physleta.2019.04.060},
	issn = {0375-9601},
	number = {20},
	pages = {2389--2393},
	volume = {28},
	groups = {graphical-models},
	publisher = {Elsevier BV},
}

@article{Statisticalinferencelinks_Peel2022,
	author = {Peel, Leto and Peixoto, Tiago P. and De Domenico, Manlio},
	date = {2022-11},
	journaltitle = {Nature Communications},
	title = {Statistical inference links data and theory in network science},
	doi = {10.1038/s41467-022-34267-9},
	issn = {2041-1723},
	number = {1},
	url = {https://www.nature.com/articles/s41467-022-34267-9},
	volume = {13},
	file = {:Statisticalinferencelinks_Peel2022 - Statistical Inference Links
	        Data and Theory in Network Science.pdf:PDF},
	groups = {graphs},
	publisher = {Springer Science and Business Media LLC},
}

@article{Measurementerrornetwork_Wang2012,
	author = {Wang, Dan J. and Shi, Xiaolin and McFarland, Daniel A. and
	          Leskovec, Jure},
	date = {2012-10},
	journaltitle = {Social Networks},
	title = {Measurement error in network data: A re-classification},
	doi = {10.1016/j.socnet.2012.01.003},
	issn = {0378-8733},
	number = {4},
	pages = {396--409},
	volume = {34},
	file = {:Measurementerrornetwork_Wang2012 - Measurement Error in Network
	        Data_ a Re Classification.pdf:PDF},
	groups = {graphs},
	publisher = {Elsevier BV},
}

@inproceedings{GeneralizedEuclideanmeasure_Coscia2020,
	author = {Coscia, Michele},
	booktitle = {Proceedings of the international AAAI conference on web and
	             social media},
	date = {2020},
	title = {Generalized Euclidean measure to estimate network distances},
	doi = {10.1609/icwsm.v14i1.7284},
	pages = {119--129},
	publisher = {Association for the Advancement of Artificial Intelligence
	             (AAAI)},
	volume = {14},
	groups = {laplacian-systems},
	issn = {2334-0770},
	journaltitle = {Proceedings of the International AAAI Conference on Web and
	                Social Media},
}

@article{Approximatingdiscreteprobability_Chow1968,
	author = {Chow, C. and Liu, C.},
	date = {1968-05},
	journaltitle = {IEEE Transactions on Information Theory},
	title = {Approximating discrete probability distributions with dependence
	         trees},
	doi = {10.1109/tit.1968.1054142},
	issn = {0018-9448},
	number = {3},
	pages = {462--467},
	volume = {14},
	publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
}

@inproceedings{Onlinedictionarylearning_Mairal2009,
	author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro,
	          Guillermo},
	booktitle = {Proceedings of the 26th annual international conference on
	             machine learning},
	date = {2009},
	title = {Online dictionary learning for sparse coding},
	doi = {10.1145/1553374.1553463},
	pages = {689--696},
	publisher = {ACM},
	file = {:Onlinedictionarylearning_Mairal2009 - Online Dictionary Learning
	        for Sparse Coding.pdf:PDF},
	journaltitle = {Proceedings of the 26th Annual International Conference on
	                Machine Learning},
}

@article{Estimatingsemanticnetworks_Zemla2018,
	author = {Zemla, Jeffrey C and Austerweil, Joseph L},
	date = {2018},
	journaltitle = {Computational brain \& behavior},
	title = {Estimating semantic networks of groups and individuals from fluency
	         data},
	doi = {10.31234/osf.io/kg45r},
	pages = {36--58},
	volume = {1},
	file = {:Estimatingsemanticnetworks_Zemla2018 - Estimating Semantic Networks
	        of Groups and Individuals from Fluency Data.pdf:PDF},
	publisher = {Springer},
}

@article{EmbeddinggraphsLorentzian_Clough2017,
	author = {Clough, James R. and Evans, Tim S.},
	date = {2017-11},
	journaltitle = {PLOS ONE},
	title = {Embedding graphs in Lorentzian spacetime},
	doi = {10.1371/journal.pone.0187301},
	editor = {Masuda, Naoki},
	issn = {1932-6203},
	number = {11},
	pages = {e0187301},
	volume = {12},
	file = {:EmbeddinggraphsLorentzian_Clough2017 - Embedding Graphs in
	        Lorentzian Spacetime.pdf:PDF},
	publisher = {Public Library of Science (PLoS)},
}

@inproceedings{LearningHumanSearch_Sexton2016,
	author = {Rachael Sexton and Max Yi Ren},
	booktitle = {Volume 2A: 42nd Design Automation Conference},
	date = {2016-08},
	title = {Learning Human Search Strategies From a Crowdsourcing Game},
	doi = {10.1115/detc2016-59775},
	publisher = {American Society of Mechanical Engineers},
}

@inproceedings{UsingSemanticFluency_Sexton2019,
	author = {Rachael Sexton and Mark Fuge},
	booktitle = {Volume 2A: 45th Design Automation Conference},
	date = {2019-08},
	title = {Using Semantic Fluency Models Improves Network Reconstruction
	         Accuracy of Tacit Engineering Knowledge},
	doi = {10.1115/detc2019-98429},
	publisher = {American Society of Mechanical Engineers},
}

@article{OrganizingTaggedKnowledge_Sexton2020,
	author = {Rachael Sexton and Mark Fuge},
	date = {2020-01},
	journaltitle = {Journal of Mechanical Design},
	title = {Organizing Tagged Knowledge: Similarity Measures and Semantic
	         Fluency in Structure Mining},
	doi = {10.1115/1.4045686},
	issn = {1050-0472},
	number = {3},
	volume = {142},
	groups = {thesis},
	publisher = {{ASME} International},
}

@inproceedings{MSECQuantitativeRetrospective_Sexton2020,
	author = {Rachael Sexton and Michael P. Brundage and Alden Dima and Michael
	          Sharp},
	booktitle = {Volume 2: Manufacturing Processes; Manufacturing Systems;
	             Nano/Micro/Meso Manufacturing; Quality and Reliability},
	date = {2020-09},
	title = {{MSEC}: A Quantitative Retrospective},
	doi = {10.1115/msec2020-8440},
	publisher = {American Society of Mechanical Engineers},
}

@article{Technicallanguageprocessing_Brundage2021,
	author = {Michael P. Brundage and Rachael Sexton and Melinda Hodkiewicz and
	          Alden Dima and Sarah Lukens},
	date = {2021-01},
	journaltitle = {Manufacturing Letters},
	title = {Technical language processing: Unlocking maintenance knowledge},
	doi = {10.1016/j.mfglet.2020.11.001},
	issn = {2213-8463},
	pages = {42--46},
	volume = {27},
	publisher = {Elsevier {BV}},
}

@article{CategorizationErrorsData_Sexton2019,
	author = {Rachael Sexton and Melinda Hodkiewicz and Michael P. Brundage},
	date = {2019-09},
	journaltitle = {Annual Conference of the {PHM} Society},
	title = {Categorization Errors for Data Entry in Maintenance Work-Orders},
	doi = {10.36001/phmconf.2019.v11i1.790},
	issn = {2325-0178},
	number = {1},
	volume = {11},
	groups = {Tree Editing Interface},
	publisher = {{PHM} Society},
}

@inproceedings{DevelopingMaintenanceKey_Brundage2018,
	author = {Michael P. Brundage and K. C. Morris and Rachael Sexton and Sascha
	          Moccozet and Michael Hoffman},
	booktitle = {Volume 3: Manufacturing Equipment and Systems},
	date = {2018-06},
	title = {Developing Maintenance Key Performance Indicators From Maintenance
	         Work Order Data},
	doi = {10.1115/msec2018-6492},
	publisher = {American Society of Mechanical Engineers},
}

@inproceedings{Hybriddataficationmaintenance_Sexton2017,
	author = {Rachael Sexton and Michael P. Brundage and Michael Hoffman and K.
	          C. Morris},
	booktitle = {2017 {IEEE} International Conference on Big Data (Big Data)},
	date = {2017-12},
	title = {Hybrid datafication of maintenance logs from {AI}-assisted human
	         tags},
	doi = {10.1109/bigdata.2017.8258120},
	publisher = {{IEEE}},
}

@inproceedings{RankedTaggingMaintenance_Brundage2020,
	author = {Michael Brundage and Rachael Sexton},
	booktitle = {Proceedings of the 30th European Safety and Reliability
	             Conference and 15th Probabilistic Safety Assessment and
	             Management Conference},
	date = {2020},
	title = {Ranked Tagging of Maintenance Work Order Data using Nestor},
	doi = {10.3850/978-981-14-8593-0_5241-cd},
	publisher = {Research Publishing Services},
}

@article{RethinkingMaintenanceTerminology_Hodkiewicz2021,
	author = {Melinda Hodkiewicz and Sarah Lukens and Michael P. Brundage and
	          Rachael Sexton},
	date = {2021-03},
	journaltitle = {International Journal of Prognostics and Health Management},
	title = {Rethinking Maintenance Terminology for an Industry 4.0 Future},
	doi = {10.36001/ijphm.2021.v12i1.2932},
	issn = {2153-2648},
	number = {1},
	volume = {12},
	publisher = {{PHM} Society},
}

@article{BenchmarkingKeywordExtraction_Sexton2018,
	author = {Rachael Sexton and Melinda Hodkiewicz and Michael P. Brundage and
	          Thomas Smoker},
	date = {2018-09},
	journaltitle = {Annual Conference of the {PHM} Society},
	title = {Benchmarking for Keyword Extraction Methodologies in Maintenance
	         Work Orders},
	doi = {10.36001/phmconf.2018.v10i1.541},
	issn = {2325-0178},
	number = {1},
	volume = {10},
	publisher = {{PHM} Society},
}

@article{DataDrivenFramework_Reslan2021,
	author = {Maya Reslan and Emily M. Hastings and Michael P. Brundage and
	          Rachael Sexton},
	date = {2021-03},
	journaltitle = {International Journal of Prognostics and Health Management},
	title = {Data-Driven Framework for Team Formation for Maintenance Tasks},
	doi = {10.36001/ijphm.2021.v12i1.2930},
	issn = {2153-2648},
	number = {1},
	volume = {12},
	publisher = {{PHM} Society},
}

@article{AgreementBehaviorIsolated_Hastings2019,
	author = {Emily Hastings and Rachael Sexton and Michael P. Brundage and
	          Melinda Hodkiewicz},
	date = {2019-09},
	journaltitle = {Annual Conference of the {PHM} Society},
	title = {Agreement Behavior of Isolated Annotators for Maintenance
	         Work-Order Data Mining},
	doi = {10.36001/phmconf.2019.v11i1.791},
	issn = {2325-0178},
	number = {1},
	volume = {11},
	publisher = {{PHM} Society},
}

@article{StudiesPredictMaintenance_Navinchandran2019,
	author = {Madhusudanan Navinchandran and Michael E. Sharp and Michael P.
	          Brundage and Rachael B. Sexton},
	date = {2019-09},
	journaltitle = {Annual Conference of the {PHM} Society},
	title = {Studies to Predict Maintenance Time Duration and Important Factors
	         From Maintenance Workorder Data},
	doi = {10.36001/phmconf.2019.v11i1.792},
	issn = {2325-0178},
	number = {1},
	volume = {11},
	publisher = {{PHM} Society},
}

@article{NestorToolNatural_Sexton2019,
	author = {Rachael B. Sexton and Michael B. Brundage},
	date = {2019-11},
	journaltitle = {Journal of Research of the National Institute of Standards
	                and Technology},
	title = {Nestor: A Tool for Natural Language Annotation of Short Texts},
	doi = {10.6028/jres.124.029},
	issn = {2165-7254},
	volume = {124},
	publisher = {National Institute of Standards and Technology ({NIST})},
}

@article{MitigatingDisruptionProduction_Sprock2020,
	author = {Timothy Sprock and Michael P. Brundage and William Z. Bernstein
	          and Rachael Sexton and Michael Sharp},
	date = {2020-03},
	journaltitle = {Smart and Sustainable Manufacturing Systems},
	title = {Mitigating Disruption in Production Networks through Dynamic
	         Scheduling Enabled by Integrated Enterprise Data},
	doi = {10.1520/ssms20200051},
	issn = {2520-6478},
	number = {3},
	pages = {20200051},
	volume = {4},
	publisher = {{ASTM} International},
}

@article{PredictiveModelMarkup_Nannapaneni2018,
	author = {Saideep Nannapaneni and Anantha Narayanan and Ronay Ak and David
	          Lechevalier and Rachael Sexton and Sankaran Mahadevan and Yung-Tsun
	          Tina Lee},
	date = {2018-01},
	journaltitle = {Smart and Sustainable Manufacturing Systems},
	title = {Predictive Model Markup Language ({PMML}) Representation of
	         Bayesian Networks: An Application in Manufacturing},
	doi = {10.1520/ssms20180018},
	issn = {2520-6478},
	number = {1},
	pages = {20180018},
	volume = {2},
	publisher = {{ASTM} International},
}

@unpublished{Understanding&Evaluating_Sexton2021,
	author = {Sexton, Rachael},
	date = {2021-08},
	title = {Understanding & Evaluating Informed {NLP} Systems},
	note = {AI Measurement & Evaluation Community of Interest Invited Talk},
	subtitle = {Starting the Road to Technical Language Processing},
	file = {:Understanding&Evaluating_Sexton2021 - Understanding & Evaluating
	        Informed NLP Systems.pdf:PDF},
}

@unpublished{IntroductionNaturalLanguage_Sexton2021,
	author = {Sexton, Rachael},
	date = {2021-06-25},
	title = {Introduction to Natural Language Processing: Theory and Application
	         for Engineering},
	note = {{U.S. NRC} Data Science and {AI} Workshop (invited talk)},
	url = {https://www.nrc.gov/docs/ML2120/ML21201A277.html},
	file = {:IntroductionNaturalLanguage_Sexton2021 - Introduction to Natural
	        Language Processing_ Theory and Application for Engineering.pdf:PDF},
	month = jun,
	year = {2021},
}

@unpublished{MeetingFatigue&_Sexton2021,
	author = {Sexton, Rachael},
	date = {2021-07-08},
	title = {Meeting Fatigue & Measuring Productivity},
	note = {EL Management Council; Diversity, Inclusivity, & Belonging Special
	        Topic},
	subtitle = {When does a researcher have a "good day"?},
	file = {:MeetingFatigue&_Sexton2021 - Meeting Fatigue & Measuring
	        Productivity.pdf:PDF},
}

@unpublished{DataOps_Sexton2021,
	author = {Sexton, Rachael},
	date = {2021-04-16},
	title = {DataOps},
	eventtitle = {TLP CoI Next Steps},
	note = {MBE Summit 2021 TLP Workshop (presentation)},
	titleaddon = {Community Data Operations for Reproducible {TLP}},
	url = {
	       https://www.nist.gov/el/technical-language-processing-community-interest/events/kickoff-event
	       },
}

@unpublished{NestorMachineAugmented_Sexton2021,
	author = {Sexton, Rachael},
	date = {2021-06-14},
	title = {Nestor: Machine-Augmented Annotation for Technical Text},
	note = {IDETC/CIE Tool Showcase (demo)},
}

@inproceedings{ImpactDataQuality_Conte2021,
	author = {Conte, Anna and Bolland, Coline and Phan, Lynn and Brundage,
	          Michael and Sexton, Rachael},
	booktitle = {PHM Society European Conference},
	date = {2021},
	title = {The Impact of Data Quality on Maintenance Work Order Analysis: A
	         Case Study in Historical HVAC Maintenance Work Orders},
	doi = {10.36001/phme.2021.v6i1.2814},
	number = {1},
	pages = {11},
	volume = {6},
}

@article{Adaptingnaturallanguage_Dima2021,
	author = {Alden Dima and Sarah Lukens and Melinda Hodkiewicz and Rachael
	          Sexton and Michael P. Brundage},
	date = {2021-06},
	journaltitle = {Applied {AI} Letters},
	title = {Adapting natural language processing for technical text},
	doi = {10.1002/ail2.33},
	issn = {2689-5595},
	number = {3},
	volume = {2},
	publisher = {Wiley},
}

@inproceedings{VisualAnalyticsApproach_Zhang2021,
	author = {Xiaoyu Zhang and Takanori Fujiwara and Senthil Chandrasegaran and
	          Michael P. Brundage and Rachael Sexton and Alden Dima and Kwan-Liu
	          Ma},
	booktitle = {2021 {IEEE} 14th Pacific Visualization Symposium ({PacificVis})
	             },
	date = {2021-04},
	title = {A Visual Analytics Approach for the Diagnosis of Heterogeneous and
	         Multidimensional Machine Maintenance Data},
	doi = {10.1109/pacificvis52677.2021.00033},
	publisher = {{IEEE}},
}

@software{Nestortoolkitquantifying_Sexton2018,
	author = {Sexton, Rachael and Madhusudanan Navinchandran, Fnu and Bones,
	          Lela and Brundage, Michael and Hoffman, Michael and Moccozet,
	          Sascha},
	date = {2018},
	title = {Nestor: a toolkit for quantifying tacit maintenance knowledge, for
	         investigatory analysis in smart manufacturing},
	doi = {10.18434/T4/1502464},
	language = {en},
	url = {https://github.com/usnistgov/nestor},
	keywords = {CMMS, communication, data cleaning, decision guidance,
	            diagnostics, event sequences, investigations, machine learning,
	            maintenance, manufacturing operations, manufacturing performance,
	            nestor, prognostics, scheduling, smart manufacturin, training,
	            tribal knowledge, visualization, information},
	publisher = {National Institute of Standards and Technology},
}

@software{ToolkitCuratedArchive_Sexton2020,
	author = {Sexton, Rachael},
	date = {2020},
	title = {Toolkit and Curated Archive for COVID-19 Research Challenge Dataset
	         },
	doi = {10.18434/M32201},
	language = {en},
	copyright = {License Information for NIST data},
	keywords = {virus, covid19, natural language processing, nlp, embedding},
	publisher = {National Institute of Standards and Technology},
}

@article{Bipartitenetworkprojection_Zhou2007,
	author = {Zhou, Tao and Ren, Jie and Medo, Mat\'u\ifmmode \check{s}\else \v{
	          s}\fi{} and Zhang, Yi-Cheng},
	date = {2007-10},
	journaltitle = {Phys. Rev. E},
	title = {Bipartite network projection and personal recommendation},
	doi = {10.1103/PhysRevE.76.046115},
	issn = {1550-2376},
	issue = {4},
	number = {4},
	pages = {046115},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.76.046115},
	volume = {76},
	file = {:Bipartitenetworkprojection_Zhou2007 - Bipartite Network Projection
	        and Personal Recommendation.html:URL},
	groups = {incidence structures},
	numpages = {7},
	publisher = {American Physical Society},
}

@article{backbonebipartiteprojections_Neal2014,
	author = {Neal, Zachary},
	date = {2014-10-01},
	journaltitle = {Social Networks},
	title = {The backbone of bipartite projections: Inferring relationships from
	         co-authorship, co-sponsorship, co-attendance and other co-behaviors},
	doi = {10.1016/j.socnet.2014.06.001},
	issn = {0378-8733},
	note = {{ZSCC}: 0000200},
	pages = {84--97},
	url = {https://www.sciencedirect.com/science/article/pii/S0378873314000343},
	urldate = {2024-05-07},
	volume = {39},
	abstract = {The analysis and visualization of weighted networks pose many
	            challenges, which have led to the development of techniques for
	            extracting the network's backbone, a subgraph composed of only
	            the most significant edges. Weighted edges are particularly
	            common in bipartite projections (e.g. networks of co-authorship,
	            co-attendance, co-sponsorship), which are often used as proxies
	            for one-mode networks where direct measurement is impractical or
	            impossible (e.g. networks of collaboration, friendship,
	            alliance). However, extracting the backbone of bipartite
	            projections requires special care. This paper reviews existing
	            methods for extracting the backbone from bipartite projections,
	            and proposes a new method that aims to overcome their
	            limitations. The stochastic degree sequence model ({SDSM})
	            involves the construction of empirical edge weight distributions
	            from random bipartite networks with stochastic marginals, and is
	            demonstrated using data on bill sponsorship in the 108th U.S.
	            Senate. The extracted backbone's validity as a network reflecting
	            political alliances and antagonisms is established through
	            comparisons with data on political party affiliations and
	            political ideologies, which offer an empirical ground-truth. The
	            projection and backbone extraction methods discussed in this
	            paper can be performed using the -onemode- command in Stata.},
	file = {:backbonebipartiteprojections_Neal2014.html:URL},
	groups = {incidence structures},
	keywords = {Bill co-sponsorship, Binarize, Dichotomize, Political network,
	            Projection, Two-mode},
	shortjournal = {Social Networks},
	shorttitle = {The backbone of bipartite projections},
}

@article{Comparingalternativesfixed_Neal2021,
	author = {Neal, Zachary P. and Domagalski, Rachel and Sagan, Bruce},
	date = {2021-12-14},
	journaltitle = {Scientific Reports},
	title = {Comparing alternatives to the fixed degree sequence model for
	         extracting the backbone of bipartite projections},
	doi = {10.1038/s41598-021-03238-3},
	issn = {2045-2322},
	note = {{ZSCC}: 0000023},
	number = {1},
	pages = {23929},
	url = {https://www.nature.com/articles/s41598-021-03238-3},
	urldate = {2024-05-07},
	volume = {11},
	abstract = {Projections of bipartite or two-mode networks capture
	            co-occurrences, and are used in diverse fields (e.g., ecology,
	            economics, bibliometrics, politics) to represent unipartite
	            networks. A key challenge in analyzing such networks is
	            determining whether an observed number of co-occurrences between
	            two nodes is significant, and therefore whether an edge exists
	            between them. One approach, the fixed degree sequence model ({
	            FDSM}), evaluates the significance of an edge’s weight by
	            comparison to a null model in which the degree sequences of the
	            original bipartite network are fixed. Although the {FDSM} is an
	            intuitive null model, it is computationally expensive because it
	            requires Monte Carlo simulation to estimate each edge’s p value,
	            and therefore is impractical for large projections. In this paper
	            , we explore four potential alternatives to {FDSM}: fixed fill
	            model, fixed row model, fixed column model, and stochastic degree
	            sequence model ({SDSM}). We compare these models to {FDSM} in
	            terms of accuracy, speed, statistical power, similarity, and
	            ability to recover known communities. We find that the
	            computationally-fast {SDSM} offers a statistically conservative
	            but close approximation of the computationally-impractical {FDSM}
	            under a wide range of conditions, and that it correctly recovers
	            a known community structure even when the signal is weak.
	            Therefore, although each backbone model may have particular
	            applications, we recommend {SDSM} for extracting the backbone of
	            bipartite projections when {FDSM} is impractical.},
	file = {:Comparingalternativesfixed_Neal2021.pdf:PDF},
	groups = {incidence structures},
	howpublished = {{OriginalPaper}},
	keywords = {Applied mathematics, Ecology, Mathematics and computing, Pure
	            mathematics, Software, Statistics},
	langid = {english},
	publisher = {Nature Publishing Group},
	rights = {2021 The Author(s)},
	shortjournal = {Sci Rep},
	type = {{OriginalPaper}},
}

@article{twostagealgorithm_Slater2009,
	author = {Slater, Paul B.},
	date = {2009-06-30},
	journaltitle = {Proceedings of the National Academy of Sciences},
	title = {A two-stage algorithm for extracting the multiscale backbone of
	         complex weighted networks},
	doi = {10.1073/pnas.0904725106},
	note = {{ZSCC}: 0000040},
	number = {26},
	pages = {E66--E66},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0904725106},
	urldate = {2024-05-07},
	volume = {106},
	file = {Full Text PDF:https\:
	        //www.pnas.org/doi/pdf/10.1073/pnas.0904725106:application/pdf},
	groups = {laplacian-systems},
	publisher = {Proceedings of the National Academy of Sciences},
}

@book{Markovrandomfields_Kindermann1980,
	author = {Kindermann, Ross},
	date = {1980},
	title = {Markov random fields and their applications},
	isbn = {0821850016},
	pages = {142},
	publisher = {American Mathematical Society},
	file = {
	        :Markovrandomfields_Kindermann1980.html:URL;:Markovrandomfields_Kindermann1980.pdf:PDF
	        },
}

 
@article{StabilitySelection_Meinshausen2010,
	author = {Meinshausen, Nicolai and Bühlmann, Peter},
	date = {2010-08},
	journaltitle = {Journal of the Royal Statistical Society Series B:
	                Statistical Methodology},
	title = {Stability Selection},
	doi = {10.1111/j.1467-9868.2010.00740.x},
	issn = {1467-9868},
	number = {4},
	pages = {417--473},
	volume = {72},
	file = {:StabilitySelection_Meinshausen2010.pdf:PDF},
	groups = {graphical-models},
	priority = {prio2},
	publisher = {Oxford University Press (OUP)},
}

@article{statisticalcomparisonMatthews_Chicco2023,
	author = {Chicco, Davide and Jurman, Giuseppe},
	date = {2023-08-01},
	journaltitle = {Journal of Biomedical Informatics},
	title = {A statistical comparison between Matthews correlation coefficient (
	         {MCC}), prevalence threshold, and Fowlkes–Mallows index},
	doi = {10.1016/j.jbi.2023.104426},
	issn = {1532-0464},
	note = {{ZSCC}: 0000018},
	pages = {104426},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046423001478},
	urldate = {2024-09-13},
	volume = {144},
	abstract = {Even if assessing binary classifications is a common task in
	            scientific research, no consensus on a single statistic
	            summarizing the confusion matrix has been reached so far. In
	            recent studies, we demonstrated the advantages of the Matthews
	            correlation coefficient ({MCC}) over other popular rates such as
	            cross-entropy error, F1 score, accuracy, balanced accuracy,
	            bookmaker informedness, diagnostic odds ratio, Brier score, and
	            Cohen’s kappa. In this study, we compared the {MCC} to other two
	            statistics: prevalence threshold ({PT}), frequently used in
	            obstetrics and gynecology, and Fowlkes–Mallows index, a metric
	            employed in fuzzy logic and drug discovery. Through the
	            investigation of the mutual relations among three metrics and the
	            study of some relevant use cases, we show that, when positive
	            data elements and negative data elements have the same importance
	            , the Matthews correlation coefficient can be more informative
	            than its two competitors, even this time.},
	file = {:statisticalcomparisonMatthews_Chicco2023.pdf:PDF},
	groups = {Research Topics},
	keywords = {Matthews correlation coefficient, Prevalence threshold,
	            Fowlkes–Mallows index, Binary classification, Confusion matrix,
	            Supervised machine learning},
	priority = {prio2},
	shortjournal = {Journal of Biomedical Informatics},
}

@misc{MCCF1curve_,
	title = {The MCC-F1 curve: a performance evaluation technique for binary
	         classification - 2006.11278v1},
	url = {https://arxiv.org/pdf/2006.11278},
	accessdate = {2024-09-24},
	file = {:default.pdf:PDF},
	groups = {Research Topics},
}

@article{lineartimealgorithm_Raghavan2007,
	author = {Raghavan, Usha Nandini and Albert, Réka and Kumara, Soundar},
	date = {2007-09-11},
	journaltitle = {Physical Review E},
	title = {Near linear time algorithm to detect community structures in
	         large-scale networks},
	doi = {10.1103/PhysRevE.76.036106},
	note = {{ZSCC}: 0004285},
	number = {3},
	pages = {036106},
	url = {https://link.aps.org/doi/10.1103/PhysRevE.76.036106},
	urldate = {2024-09-24},
	volume = {76},
	abstract = {Community detection and analysis is an important methodology for
	            understanding the organization of various real-world networks and
	            has applications in problems as diverse as consensus formation in
	            social communities or the identification of functional modules in
	            biochemical networks. Currently used algorithms that identify the
	            community structures in large-scale real-world networks require a
	            priori information such as the number and sizes of communities or
	            are computationally expensive. In this paper we investigate a
	            simple label propagation algorithm that uses the network
	            structure alone as its guide and requires neither optimization of
	            a predefined objective function nor prior information about the
	            communities. In our algorithm every node is initialized with a
	            unique label and at every step each node adopts the label that
	            most of its neighbors currently have. In this iterative process
	            densely connected groups of nodes form a consensus on a unique
	            label to form communities. We validate the algorithm by applying
	            it to networks whose community structures are known. We also
	            demonstrate that the algorithm takes an almost linear time and
	            hence it is computationally less expensive than what was possible
	            so far., This article appears in the following collection:},
	file = {:lineartimealgorithm_Raghavan2007.pdf:PDF},
	groups = {graphs},
	publisher = {American Physical Society},
	shortjournal = {Phys. Rev. E},
}

@report{LinearTimeApproximation_CohenAddad2024,
	author = {Cohen-Addad, Vincent and d'Orsi, Tommaso and Mousavifar, Aida},
	date = {2024-06-07},
	institution = {{arXiv}},
	title = {A Near-Linear Time Approximation Algorithm for Beyond-Worst-Case
	         Graph Clustering},
	doi = {10.48550/arXiv.2406.04857},
	eprint = {2406.04857 [cs]},
	eprinttype = {arxiv},
	note = {{ZSCC}: 0000001 type: article},
	number = {{arXiv}:2406.04857},
	url = {http://arxiv.org/abs/2406.04857},
	urldate = {2024-09-24},
	abstract = {We consider the semi-random graph model of [Makarychev,
	            Makarychev and Vijayaraghavan, {STOC}'12], where, given a random
	            bipartite graph with \${\textbackslash}alpha\$ edges and an
	            unknown bipartition \$(A, B)\$ of the vertex set, an adversary
	            can add arbitrary edges inside each community and remove
	            arbitrary edges from the cut \$(A, B)\$ (i.e. all adversarial
	            changes are {\textbackslash}textit\{monotone\} with respect to
	            the bipartition). For this model, a polynomial time algorithm is
	            known to approximate the Balanced Cut problem up to value \$O({
	            \textbackslash}alpha)\$ [{MMV}'12] as long as the cut \$(A, B)\$
	            has size \${\textbackslash}Omega({\textbackslash}alpha)\$.
	            However, it consists of slow subroutines requiring optimal
	            solutions for logarithmically many semidefinite programs. We
	            study the fine-grained complexity of the problem and present the
	            first near-linear time algorithm that achieves similar
	            performances to that of [{MMV}'12]. Our algorithm runs in time \$
	            O({\textbar}V(G){\textbar}{\textasciicircum}\{1+o(1)\} + {
	            \textbar}E(G){\textbar}{\textasciicircum}\{1+o(1)\})\$ and finds
	            a balanced cut of value \$O({\textbackslash}alpha)\$. Our
	            approach appears easily extendible to related problem, such as
	            Sparsest Cut, and also yields an near-linear time \$O(1)\$
	            -approximation to Dagupta's objective function for hierarchical
	            clustering [Dasgupta, {STOC}'16] for the semi-random hierarchical
	            stochastic block model inputs of [Cohen-Addad, Kanade,
	            Mallmann-Trenn, Mathieu, {JACM}'19].},
	file = {:cohen-addad_near-linear_2024.pdf:PDF},
	groups = {graphs},
	keywords = {Computer Science - Data Structures and Algorithms, Computer
	            Science - Machine Learning, F.2, G.3},
}

@unpublished{ParallelEuclideandistance_Angeletti2019,
	author = {Angeletti, M{\'e}lodie and Bonny, J.-M. and Koko, Jonas},
	date = {2019},
	title = {{Parallel Euclidean distance matrix computation on big datasets}},
	note = {working paper or preprint},
	url = {https://hal.science/hal-02047514},
	file = {:ParallelEuclideandistance_Angeletti2019.pdf:PDF},
	groups = {Research Topics},
	hal_id = {hal-02047514},
	hal_version = {v1},
	keywords = {Euclidean distance matrix ; parallelization ; mutlicores ;
	            many-core ; GPU},
	pdf = {https://hal.science/hal-02047514v1/file/pdist.pdf},
	priority = {prio2},
}

@inproceedings{LabelvizierInteractivevalidation_Zhang2023,
	author = {Zhang, Xiaoyu and Xuan, Xiwei and Dima, Alden and Sexton, Thurston
	          and Ma, Kwan-Liu, Sexton, Rachael},
	booktitle = {2023 IEEE 16th Pacific Visualization Symposium (PacificVis)},
	date = {2023},
	title = {Labelvizier: Interactive validation and relabeling for technical
	         text annotations},
	organization = {IEEE},
	pages = {167--176},
}

@misc{infrastructurecuratingquerying_Subrahmanian2023,
	author = {Subrahmanian, Eswaran and Subrahmanian, Eswaran and Amaral,
	          Guillaume Sousa and Bhat, Talapady N and Brady, Kevin G and Brady,
	          Mary C and Collard, Jacob N and Chouder, Sarah and Dessauw,
	          Philippe J and Dima, Alden A and others, Sexton, Rachael},
	date = {2023},
	title = {An infrastructure for curating, querying, and augmenting document
	         data: COVID-19 case study},
	publisher = {US Department of Commerce, National Institute of Standards and
	             Technology},
}

@article{KPIextractionmaintenance_Lutz2023,
	author = {Lutz, Marc-Alexander and Sch{\"a}fermeier, Bastian and Sexton,
	          Rachael and Sharp, Michael and Dima, Alden and Faulstich, Stefan
	          and Aluri, Jagan Mohini},
	date = {2023},
	journaltitle = {Energies},
	title = {KPI extraction from maintenance work orders—a comparison of expert
	         labeling, text classification and AI-assisted tagging for computing
	         failure rates of wind turbines},
	number = {24},
	pages = {7937},
	volume = {16},
	publisher = {MDPI},
}

@article{DiscoveringcriticalKPI_Navinchandran2021,
	author = {Madhusudanan Navinchandran and Michael E. Sharp and Michael P.
	          Brundage and Rachael B. Sexton},
	date = {2021-04},
	journaltitle = {Journal of Intelligent Manufacturing},
	title = {Discovering critical {KPI} factors from natural language in
	         maintenance work orders},
	doi = {10.1007/s10845-021-01772-5},
	issn = {0956-5515},
	pages = {1--19},
	publisher = {Springer Science and Business Media {LLC}},
}

@article{Communitystructuresocial_Girvan2002,
	author = {Girvan, M. and Newman, M. E. J.},
	date = {2002-06},
	journaltitle = {Proceedings of the National Academy of Sciences},
	title = {Community structure in social and biological networks},
	doi = {10.1073/pnas.122653799},
	issn = {1091-6490},
	number = {12},
	pages = {7821--7826},
	volume = {99},
	file = {:Communitystructuresocial_Girvan2002.pdf:PDF:https\:
	        //europepmc.org/articles/pmc122977?pdf=render},
	groups = {graphs},
	publisher = {Proceedings of the National Academy of Sciences},
}

@article{InformationFlowModel_Zachary1977,
	author = {Zachary, Wayne W.},
	date = {1977-12},
	journaltitle = {Journal of Anthropological Research},
	title = {An Information Flow Model for Conflict and Fission in Small Groups},
	doi = {10.1086/jar.33.4.3629752},
	issn = {2153-3806},
	number = {4},
	pages = {452--473},
	volume = {33},
	file = {:InformationFlowModel_Zachary1977.pdf:PDF:http\:
	        //arxiv.org/pdf/1707.03587},
	groups = {graphs},
	publisher = {University of Chicago Press},
}

@article{Scientificcollaborationnetworks._Newman2001,
	author = {Newman, M. E. J.},
	date = {2001-06},
	journaltitle = {Physical Review E},
	title = {Scientific collaboration networks. II. Shortest paths, weighted
	         networks, and centrality},
	doi = {10.1103/physreve.64.016132},
	issn = {1095-3787},
	number = {1},
	pages = {016132},
	volume = {64},
	file = {:Scientificcollaborationnetworks._Newman2001.pdf:PDF:http\:
	        //arxiv.org/pdf/cond-mat/0011144},
	groups = {graphs},
	priority = {prio1},
	publisher = {American Physical Society (APS)},
}
