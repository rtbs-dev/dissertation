% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\RequirePackage[l2tabu,orthodox]{nag}
\documentclass[%
	12pt,
		oneside,
		letterpaper
]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\directlua{pdf.setminorversion(6)}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage[otfmath]{XCharter}
\usepackage[strict]{csquotes}
\usepackage{tikz-network}
% Change contents title to Table of Contents
\addto\captionsenglish{% Replace "english" with the language you use
  \renewcommand{\contentsname}%
    {Table of contents}%
}

% Bibliography
% \usepackage[%
% 	backend=biber,
% 	style=ieee, % pick whatever style you want
% 	sorting=ydnt,
% 	isbn=true,
% ]{biblatex}
% \addbibresource{resources/pubs.bib}

% Line spacing
\usepackage{setspace}

\usepackage{etoolbox}

\usepackage[unicode=false]{hyperref}

% \usepackage[notbib,notindex]{tocbibind}

\usepackage[%
	%textwidth=345pt, % Default textwidth
	% marginpar=4cm, % Make marginal notes wider
	% includemp, % Include marginpar in width
	margin=1in,
	left=1.5in, % Left margin must be at least 1.5 inch
]{geometry}

% Toggle for double spaced or not
% \newbool{doublespaced}
\usepackage{titlesec}
   

\titleformat{\part}[display]{\normalfont\large}{Part \ \thepart}{2em}{}[]
\titleformat{\chapter}[block]{\normalfont\Large}{Chapter \ \thechapter:}{1em}{}[]
\titleformat{name=\chapter,numberless}[block]{\normalfont\Large}{}{1em}{\centering}[]
\titleformat{\section}{\normalfont\large}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\normalsize}{\thesubsection}{1em}{}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage[style=ieee,backend=biber,sorting=ydnt,isbn=true]{biblatex}
\addbibresource{resource/pubs.bib}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Measuring Network Dependencies from Node Activations},
  pdfauthor={Rachael T.B. Sexton},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Measuring Network Dependencies from Node Activations}
\author{Rachael T.B. Sexton}
\date{2024-12-31}

\begin{document}

% \frontmatter
\pagestyle{empty}
\singlespacing

%Abstract Page
\hbox{\ }

\begin{center}
\large{{ABSTRACT}}

\vspace{3em}

\end{center}
\hspace{-.15in}
\begin{tabular}{p{0.35\linewidth}p{0.6\linewidth}}
Title of Dissertation:     & {\large \uppercase{Measuring Network
Dependencies from Node Activations}}\\
                           & {\large  Rachael T.B. Sexton } \\
                           & {\large Doctor of Philosophy, } \\
\                         \\
Dissertation Directed by:  & {\large Professor Mark D. Fuge } \\
                           & {\large Department of Mechanical
Engineering} \\
\end{tabular}

\vspace{3em}

% Use word count on the text file
% \begin{doublespacing}

\renewcommand{\baselinestretch}{2}
\large \normalsize
My abstract for this dissertation.\par
% \end{doublespacing}
\clearpage%Titlepage

\thispagestyle{empty} \hbox{\ } \vspace{1.5in}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\begin{center}

\large{\uppercase{Measuring Network Dependencies from Node
Activations}}\\
\ \\ 
\ \\
\large{by} \\
\ \\
\large{Rachael T.B. Sexton}
\ \\
\ \\
\ \\
\ \\
\normalsize
Dissertation submitted to the Faculty of the Graduate School of the \\
University of Maryland, College Park in partial fulfillment \\
of the requirements for the degree of \\
Doctor of Philosophy \\

\end{center}

\vspace{7.5em}

\noindent Advisory Committee: \\
\hbox{\ }\hspace{.5in}Professor Mark D. Fuge, Chair/Advisor \\
\hbox{\ }\hspace{.5in}Professor Jordan L. Boyd-Graber \\
\hbox{\ }\hspace{.5in}Professor Maria K. Cameron \\
\hbox{\ }\hspace{.5in}Professor Michelle Girvan \\
\hbox{\ }\hspace{.5in}Professor Vincent P. Lyzinski \\
 \doublespacing

% \pagestyle{plain} \pagenumbering{roman} \setcounter{page}{2}

% 

% 
% \cleardoublepage

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

\pagenumbering{roman}

\bookmarksetup{startatroot}

\chapter*{Foreward}\label{foreward}
\addcontentsline{toc}{chapter}{Foreward}

\markboth{Foreward}{Foreward}

\bookmarksetup{startatroot}

\chapter*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

\markboth{Acknowledgements}{Acknowledgements}

\addcontentsline{toc}{chapter}{Table of Contents}
    \renewcommand{\contentsname}{Table of Contents}
\renewcommand{\baselinestretch}{1}
\small\normalsize
\tableofcontents %(required, lower-case Roman)
\newpage

\phantomsection %create the correct anchor for the bookmark
\addcontentsline{toc}{chapter}{List of Tables}
    \renewcommand{\contentsname}{List of Tables}
\listoftables %(if present, lower-case Roman)
\newpage

\phantomsection %create the correct anchor for the bookmark
\addcontentsline{toc}{chapter}{List of Figures}
    \renewcommand{\contentsname}{List of Figures}
\listoffigures %(if present, lower-case Roman)
\newpage

% LIST OF ABBREVIATIONS
\phantomsection %create the correct anchor for the bookmark
% \addcontentsline{toc}{chapter}{List of Abbreviations}
% \include{Abbreviations-supertabular}
% TODO use acronym extension??

\newpage
\setlength{\parskip}{0em}
\renewcommand{\baselinestretch}{2}
\small\normalsize

\pagenumbering{arabic}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

A wide variety of fields show consistent interest in inferring latent
network structure from observed interactions, from human cognition and
social infection networks, to marketing, traffic, finance, and many
others. \autocite{Inferringnetworksdiffusion_GomezRodriguez2012}
However, an increasing number of authors are noting a lack of agreement
in how to approach the metrology of this problem. This includes rampant
disconnects between the theoretical and methodological network analysis
sub-communities\autocite{Statisticalinferencelinks_Peel2022}, treatment
of error as purely aleatory, rather than epistemic
\autocite{Measurementerrornetwork_Wang2012}, or simply ignoring
measurement error in network reconstruction
entirely\autocite{ReconstructingNetworksUnknown_Peixoto2018}.

\section{Ambiguous Metrology}\label{ambiguous-metrology}

Networks in the ``wild'' rarely exist of and by themsleves. Rather, they
are a model of interaction or relation \emph{between} things that were
observed. One of the most beloved examples of a network, the famed
\emph{Zachary's Karate Club}\autocite{InformationFlowModel_Zachary1977},
is in fact reported as a list of pairwise interactions: every time a
club member interacted with another (outside of the club), Zachary
recorded it as two integers (the IDs of the members). The final list of
pairs can be \emph{interpreted} as an ``edge list'', which can be
modeled with a network: a simple graph. This was famously used to show
natural community structure that nicely matches the group separation
that eventually took place when the club split into
two.\autocite{Communitystructuresocial_Girvan2002}

Note, however, that we could have just as easily taken note of the
instigating student for each interaction (i.e.~which student initiated
conversation, or invited the other to socialize, etc.). If that
relational asymmetry is available, our ``edges'' are now
\emph{directed}, and we might be able to ask questions about the rates
that certain students are asked vs.~do the asking, and what that implies
about group cohesion. Additionally, the time span is assumed to be ``for
the duration of observation'' (did the students \emph{ever} interact),
but if observation time was significantly longer, say, multiple years,
we might question the credulity of treating a social interaction 2 years
ago as equally important to an interaction immediately preceding the
split. This is now a ``dynamic'' graph; or, if we only measure relative
to the time of separation, at the very least a ``weighted'' one.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/0-intro_files/figure-latex/codefigs-graphs-fig-karate-club-output-1.pdf}}

}

\caption{\label{fig-karate-club}Zachary's Karate Club, with ambiguously
extant edge 78 highlighted.}

\end{figure}%

\emph{We do not know if any of these are true}. In fact, as illustrated
in Figure~\ref{fig-karate-club}, we do not know if the network being
described from the original edge data even has 77 or 78 edges, due to
ambiguous reporting in the original work. Lacking a precise definition
of what the graph's components (i.e.~it's edges) are, \emph{as
measurable entities}, means we cannot estimate the measurement error in
the graph.

\section{Indirect Network
Measurement}\label{indirect-network-measurement}

While the karate club graph has unquantified edge uncertainty derived
from ambiguous edge measurements, we are fortunate that we \emph{have
edge measurements}. Regardless of how the data was collected, it is de
facto reported as a list of pairs. In many cases, we simply do not have
such luxury. Instead, our edges are only measured \emph{indirectly}, and
instead we are left with lists of node co-ocurrences. Networks
connecting movies as being ``similar'' might be derived from data that
lists sets of movies watched by each user; networks of disease spread
pathways might be implied from patient infection records; famously, we
might build a network of collaboration strength between academic authors
by mining datasets of the papers they co-author together.

Such networks are derived from what we will call \emph{node activation}
data, i.e., records of what entities happened ``together'', whether
contemporaneously, or in some other context or artifact.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/0-intro_files/figure-latex/codefigs-graphs-fig-obs-set-output-1.pdf}}

}

\caption{\label{fig-obs-set}}

\end{figure}%

These are naturally represented as ``bipartite'' networks, having
separate entites for, say, ``papers'' and ``authors'', and connecting
them with edges (paper 1 is ``connected'' to its authors E,H,C, etc.).
But analysts are typically seeking the collaboration network connecting
authors (or papers) themselves! Networks of relationships in this
situation are not directly observed, but which \emph{if recovered} could
provide estimates for community structure, importances of individual
authors (e.g.~as controlling flow of information), and the ``distances''
that separate authors from each other, in their respective domains.
\autocite{Scientificcollaborationnetworks._Newman2001} Common practice
assumes that co-authorship in any paper is sufficient evidence of at
least some level of social ``acquaintance'', where more papers shared
means more ``connected''.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/0-intro_files/figure-latex/codefigs-graphs-fig-collab-output-1.pdf}}

}

\caption{\label{fig-collab}}

\end{figure}%

Thus our social collaboration network is borne out of indirect
measurements: author connection is implied through ``occasions when
co-authorship occurred''. However, authors of papers may recall times
that others were added, not by their choice, but by someone else already
involved. In fact, the final author list of most papers is reasonably a
result of individuals choosing to invite others, not a unanimous,
simultaneous decision by all members. Let's imagine we wished to study
the social network of collaboration more directly: if we had the luxury
of being in situ as, say, a sociologist performing an academic
ethnography, we might have been more strict with our definition of
``connection''. If the goal is a meaningful social network reflecting
the strength of interraction between colleages, perhaps the we prefer
our edges represent ``mutual willingness to collaborate''. Edge
``measurement'', then, could involve records of events that show
willingness to seek or participate in collaboration event, such as:

\begin{itemize}
\tightlist
\item
  \emph{author (g) asked (e), (h), and (c) to co-author a paper, all of
  whom agreed}
\item
  \emph{(i) asked (f) and (j), but (j) wanted to add (b)'s expertise
  before writing one of the sections}
\end{itemize}

and so on. Each time two colleagues had an opportunity to work together
\emph{and it was seized upon} we might conclude that evidence of their
relationship strengthed. With data like this, we could be more confident
in claiming our collaboration network can serve as ``ground truth,'' as
far as empirically confirmed collaborations go. However, even if the
underlying ``activations'' are identical, our new, directly measured
graph looks very different.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/0-intro_files/figure-latex/codefigs-graphs-fig-colleague-output-1.pdf}}

}

\caption{\label{fig-colleague}graph of mutual collaboration
relationships i.e.~the ``ground truth'' social network}

\end{figure}%

Fundamentally, the network in Figure~\ref{fig-colleague} shows which
relationships the authors \emph{depend} on to accomplish their
publishing activity. When causal relations between nodes are being
modeled as edges, we call such a graph a \emph{dependency network}. We
will investigate this idea further later on, but ultimately, if a
network of dependencies is desired (or implied, based on analysis
needs), then the critical problem remaining is \emph{how do we recover
dependency networks from node activations?} Additionally, what goes
wrong when we use co-occurence/activation data to estimate the
dependency network, especially when we wish to use it for metrics like
centrality, shortest path distances, and community belonging?

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/0-intro_files/figure-latex/codefigs-graphs-fig-recover-output-1.pdf}}

}

\caption{\label{fig-recover}Recovering underlying dependency networks
from node-cooccurrences.}

\end{figure}%

Even more practically, networks created directly from bipartite-style
data are notorious for quickly becoming far too dense for useful
analysis, earning them the (not-so-)loving moniker ``hairballs''.
Network ``backboning,'' as it has come to be called tries to find a
subset of edges in this hairball that still captures it's core topology
in a way that's easier to
visualize.\autocite{twostagealgorithm_Slater2009,backbonebipartiteprojections_Neal2014}
Meanwhile, underlying networks of dependencies that \emph{cause} node
activation patterns can provide this: they are almost always more sparse
than their hairballs. Accessing the dependency \emph{backbone} in a
principled way is difficult, but doing so in a rapid, scalable manner is
critical for practitioners to be able to make use of it to trim their
hairballs.

\section{Scope of this work}\label{scope-of-this-work}

The purpose of this thesis is to provide a solid foundation for basic
edge metrology when our data consists of binary node activations, by
framing network analysis as a problem of \emph{inference}, as suggested
by \textcite{Statisticalinferencelinks_Peel2022}. We give special focus
to binary activations that occur due to spreading processes, such as
random walks or cascades on an underlying carrier graph. Recovering the
carrier, or, ``dependency'' network from node activations is of great
interest to the network backboning and causal modeling communities, but
often involves either unspoken sources of epistemic and aleatory error,
or high computation costs (or both). To begin addressing these issues,
we present a guide to current practices, pitfalls, and how common
statistical tools apply to the network recovery problem: a
\emph{Practitioner's Guide to Network Recovery}. We cover what
``measurement'' means in our context, and specifically the ways we
encode observations, operations, and uncertainties numerically.
Clarifying what different versions of what ``relation'' means (whether
proximity or incidence) is critical, since network structure is intended
to encode such relations as mathematical objects, despite common
ambiguities and confusion around what practitioners intend on
communicating through them. Then we use this structure to present a
cohesive framework for selecting a useful network recovery technique,
based on the available data and where in the data processing pipeline is
acceptable to admit either extra modeling assumptions or information
loss.

Next, building on a gap found in the first part, we present a novel
method, \emph{Forest Pursuit}, to extract dependency networks when we
know a \emph{spreading process} causes node activation (e.g.~paper
co-authorship caused by collaboration requests). We create a new
reference dataset to enable community benchmarking of network recovery
techniques, and use it show greatly improved accuracy over many other
widely-used methods. Forest Pursuit in its simplest form scales linearly
with the size of active-node sets, being trivially parallelizable and
streamable over dataset size, and agnostic to network size overall. We
then expand our analysis to re-imagine Forest Pursuit as a Bayesian
probabilistic model, \emph{Latent Forest Allocation}, which has an
easily-implemented Expectation Maximization scheme for posterior
estimation. This significantly improves upon the accuracy results of
Forest Pursuit, at the cost of some speed and scalability, giving
analysts multiple options to adapt to their needs.

Last, we apply Forest Pursuit to several qualitative case-studies,
including a scientific collaboration network, and the verbal fluency
``animals'' network recovery problem, which dramatically change
interpretation under use of our method. We investigate its use as a
low-cost preprocessor for other methods of network recovery,like GLASSO,
improving their stability and interpretability. Finally we discuss the
special case when node activations are reported as an ordered set, where
accounting for cascade-like effects becomes crucial to balance false
positive and false-negative edge prediction. Along with application of
this idea to knowledge-graph creation from technical language in the
form maintenance work-order data, we discuss more broadly the future
needs of network recovery, specifically in the context of embeddings and
gradient-based machine learning toolkits.

\part{A Practitioner's Guide to Network Recovery}

\chapter{Metrology as matrices}\label{metrology-as-matrices}

Where metrology is concerned, the actual unit of observation and how it
is encoded for us is critical to how analysts may proceed with
quantifying, modeling, and measuring uncertainty around observed
phenomena. Experiment and observation tends to be organized as inputs
and outputs, or, independent variables and dependent variables,
specifically. Independent variables are observed, multiple times
(``observations''), and changes in outcome for each can be compared to
the varying values associated with the independent variable input
(``features''). For generality, say a practitioner records their
measurements as scalar values,
i.e.~\(x\in\mathbb{S}\in\{\mathbb{R,Z,N},\cdots\}\). The structure most
often used to record scalar values of \(n\) independent/input variable
features over the course of \(m\) observations is called a design matrix
\(X:\mathbb{S}^{m\times n}\).\footnote{ Not all observations are scalar,
  but they can become so. If individual measurements are
  higher-dimensional (e.g.~images are 2D), X is a tensor, which can be
  transformed through unrolling or embedding into a lower dimensional
  representation before proceeding. There are other techniques for
  dealing with e.g.~categorical data, such as one-hot encoding (where
  the features are binary for each possible category, with boolean
  entries for each observation).}

\section{Observation and feature ``spaces''}\label{sec-matrix-notation}

If we index a set of observations and features, respectively, as
\[ i\in I=\{1,\cdots,m\}, \quad j\in J=\{1,\cdots,n\},\qquad I,J:\mathbb{N}\]
then the design matrix can map the index of an observation and a feature
to the corresponding measurement.
\begin{equation}\phantomsection\label{eq-design-mat}{
x=X(i,j)\qquad X : I\times J \rightarrow \mathbb{S}
}\end{equation} i.e.~the measured value of the \(j\)th independent
variable from the \(i\)th observation.\footnote{ This notation is
  adapted from the sparse linear algebraic treatment of graphs in
  \textcite{GraphAlgorithmsLanguage_Kepner2011} and
  \textcite{MathematicalfoundationsGraphBLAS_Kepner2016}.} In this
scheme, an ``observation'' is a single row vector of features in
\(\mathbb{S}^{n\times 1}\) (or simply \(\mathbb{S}^{n}\)), such that
each observation encodes a position in the space defined by the
features, i.e.~the \emph{feature space}, and extracting a specific
observation vector \(i\) from the entire matrix can be denoted as
\[\mathbf{x}_i=X(i,\cdot),\quad \mathbf{x}:J\rightarrow\mathbb{S}\]
Similarly, every ``feature'' is associated with a single column vector
in \(\mathbb{S}^{1\times m}\), which can likewise be interpreted as a
position in the space of observations (the \emph{data space}):
\[\mathbf{x}_j^*=X(\cdot,j),\quad \mathbf{x}^*:I\rightarrow\mathbb{S}\]
Note that this definition could be swapped without loss of generality.
In other words, \(\mathbf{x}\) and \(\mathbf{x}^*\) being in row and
column spaces is somewhat arbitrary, having more to do with the
logistics of experiment design and data collection. We could have
measured our feature vectors one-at-a-time, measuring their values over
an entire ``population'', in effect treating that as the independent
variable set.\footnote{ In fact, vectors are often said to be in the
  column-space of a matrix, especially when using them as
  transformations in physics or deep learning layers. We generally
  follow a one-observation-per-row rule, unless otherwise stated.}

To illustrate this formalism in a relevant domain, let's take another
look at co-citation networks. For \(m\) papers we might be aware of
\(n\) total authors. For a given paper, we are able to see which authors
are involved, and we say those authors ``activated'' for that paper. It
makes sense that our observations are individual papers, while the
features might be the set of possible authors. However, we are not given
information about which author was invited by which other one, or when
each author signed on. In other words, the measured values are strictly
boolean, and we can structure our dataset as a design matrix
\(X:\mathbb{B}^{m\times n}\). We can then think of the
\(i^{\mathrm{th}}\) paper as being represented by a vector
\(\mathbf{x}_i:\mathbb{B}^n\), and proceed using it in our various
statistical models. If we desired to analyze the set of authors, say, in
order to determine their relative neighborhoods or latent author
communities, we could equally use the feature vectors for each paper,
this time represented in a vector
\(\mathbf{x}^*_j:\mathbb{B}^{1\times m}\).

\section{Models \& linear operators}\label{sec-lin-ops}

Another powerful tool an analyst has is \emph{modeling} the observation
process. This is relevant when the observed data is hypothesized to be
generated by a process we can represent mathematically, but we do not
know the parameter values to best represent the observations (or the
observations are ``noisy'' and we want to find a ``best'' parameters
that account for this noise). This is applicable to much of scientific
inquiry, though one common use-case is the de-blurring of observed
images (or de-noising of signals), since we might have a model for how
blurring ``operated'' on the original image to give us the blurred one.
We call this ``blurring'' a \emph{linear operator} if it can be
represented as a matrix\footnote{in the finite-dimensional case}, and
applying it to a model with \(l\) parameters is called the \emph{forward
map}:
\[\mathbf{x} = F\mathbf{p}\qquad F:\mathbb{R}^{l}\rightarrow \mathbb{R}^n\]
where \(P\) is the space of possible parameter vectors, i.e.~the
\emph{model space}. The forward map takes a modeled vector and predicts
a location in data space.

Of critical importance, then, is our ability to recover some model
parameters from our observed data, e.g.~if our images were blurred
through convolution with a blurring kernel, then we are interested in
\emph{deconvolution}. If \(F\) is invertible, the most direct solution
might be to apply the operator to the data, as the \emph{adjoint map}:
\[ \mathbf{p} = F^H\mathbf{x}\qquad F^H:\mathbb{R}^{n}\rightarrow \mathbb{R}^l\]
which removes the effect of \(F\) from the data \(\mathbf{x}\) to
recover the desired model \(\mathbf{p}\).

Trivially we might have an orthogonal matrix \(F\), so \(F^H=F^{-1}\) is
available directly. In practice, other approaches are used to minimize
the \emph{residual}:
\(\hat{\mathbf{p}}^=\min_{\mathbf{p}} F\mathbf{p}-\mathbf{x}\). Setting
the gradient to 0 yields the normal equation, such that
\[ \hat{\mathbf{p}}=(F^TF)^{-1}F^T\mathbf{x}\] This should be familiar
to readers as equivalent to solving ordinary least-squares (OLS).
However, in that case it is more often shown as having the \emph{design
matrix} \(X\) in place of the operator \(F\).

\emph{This is a critical distinction to make:} OLS as a ``supervised''
learning method treats some of the observed data we represented as a
design matrix previously as a target to be modeled, \(y=X(\cdot,j)\),
and the rest maps parameters into data space, \(F=X(\cdot,J/j)\). With
this paradigm, only the target is being ``modeled'' and the rest of the
data is used to create the operator. In the citation network example, it
would be equivalent to trying to predict one variable, like citation
count or a specific author's participation in every paper, \emph{given}
every other author's participation in them.

For simplicity, most work in the supervised setting treats the reduced
data matrix as X, opting to treat \(y\) as a separate \emph{dependent
variable}. However, our setting will remain \emph{unsupervised}, since
no single target variable is of specific interest---all observations are
``data''. In this, we more closely align with the deconvolution
literature, such that we are seeking a model and an operation on it that
will produce the observed behavior in an ``optimal'' way.

\section{Measurement quantification \& error}\label{sec-smooth-err}

In binary data, such as what we have been considering, it is common to
model observables as so-called ``Bernoulli trials'': events with two
possible outcomes (on, off; yes, no; true, false), and one outcome has
probability \(p\). These can be thought of as weighted coin-flips:
``heads'' with probability \(p\), and ``tails'' \(1-p\). If \(k\) trials
are performed (the ``exposure''), we say the number of successes \(s\)
(the ``count'') is distributed as a binomial distribution
\(s\sim Bin(p,k)\). The empirical estimate for the success probability
is \(\hat{p}=\tfrac{s}{k}\).

Note that this naturally resembles marginal sums on our design matrix
\(X\), if we treat columns (or rows!) as an array of samples from
independent Bernoulli trials:
\(\hat{p}_j = \frac{\sum_{i\in I} X(i,j)}{m}\). Many probability
estimates involving repeated measurements of binary variables (not
simply the row/column variables) have this sort of
\(\frac{\textrm{count}}{\textrm{exposure}}\) structure, as will become
useful in later sections.

However, if we are ``measuring'' a probability, we run into issues when
we need to quantify our uncertainty about it. For instance, an event
might be quite rare, but if in our specific sample we \emph{never} see
it, we still do not generally accept a probability of zero.

\subsection{Additive Smoothing}\label{additive-smoothing}

One approach to dealing with this involves adding \emph{pseudocounts}
that smooth out our estimates for count/exposure, from which we get the
name ``additive smoothing''.{[}CITE?{]}
\[\hat{p} = \frac{s+\alpha}{k+2\alpha} \] Adding 1 success and 1 failure
(\(\alpha=1\)) as pseudocounts to our observations is called
\emph{Laplace's Rule of Succession}, or simply ``Laplace
smoothing,''\footnote{derived when Laplace desired estimates of
  probability for unobserved phenomena, such as the sun (not) rising
  tomorrow.} while adding \(\alpha=0.5\) successes and failures is
called using \emph{Jeffrey's Prior}. It's so-called because this
pseudocount turns out to be a special case of selecting a Bayesian prior
on the binomial probability (a.k.a. a \emph{Beta-Binomial} distribution)
\(p\sim \textrm{Beta}(\alpha, \beta)\), such that the posterior
distribution after our success/failure counts is
\(\textrm{Beta}(s+\alpha, k-s+\beta)\), which has the mean
\[E[p|s,k]=\frac{s+\alpha}{k+\alpha+\beta}\] which exactly recovers
additive smoothing with a Jeffrey's prior for
\(\alpha=\beta=0.5\)\footnote{ A useful comparison of the two priors (1,
  0.5) is to ask, given all of the trials we have seen so far, whether
  we believe we are near the ``end'' of an average run of trials
  (\(\alpha=1\), nearly all evidence has been collected), or about
  halfway through an average-length run (\(\alpha=0.5\), only half of
  expected evidence has been observed).\\
} This generalization allows us to be more flexible, and specify our
prior expectations on counts or exposure with more precision. Such
models provide both an estimate of the aleatory uncertainty (via the
posterior distribution), and a form of ``shrinkage'' that prevents
sampling noise from unduly affecting parameter estimates (via the prior
distribution). Despite being a simple foundation, this treatment of
``counts'' and ``exposure'' can be built upon in many ways.

\subsection{Conditional Probabilities \&
Contingencies}\label{conditional-probabilities-contingencies}

In dependency/structure recovery, since our goal involves estimating (at
least) pairwise relationships, the independence assumption required to
estimate node occurrences as Beta-Binomial is clearly
violated\footnote{ In fact, a recent method from
  \autocite{MultivariateBernoullidistribution_Dai2013} models
  probabilistic binary observations, \emph{with dependencies}, by
  generalizing the mechanics overviewed here to a fully multivariate
  Bernoulli distribution, capable of including 3rd- and higher-order
  interractions, not just pairwise.\\
}.

However, it's natural to make use of joint (\(P(A\cup B)\), how often
does A happen with B, out of all data?) and conditional (\(P(A|B)\) how
often A given B; or \(P(B|A)\), how often B given A) probabilities
between nodes, while trying to estimate dependencies. Once again, we can
estimate the base probabilities for each node from marginal sums, but
the joint and conditional probabilities can instead be estimated using
matrix multiplication using the Gram matrix, discussed below. It encodes
pair-wise co-occurrence counts, such that
\(G(i,j):\mathbb{Z}^{n\times n}\) has the co-occurrence count for node
``i'' with ``j''.

The co-occurrence probability for each pair can be approximated with the
beta-binomial scheme mentioned above, but care must be taken not to
confuse this with the edge strength connecting two nodes. First, nodes
that rarely activate (low node probability) may nonetheless reliably
connect to others when they do occur (high edge probability). In fact,
without direct observation of edges, we are not able to estimate their
count, \emph{or} their exposure, which can be a source of systemic error
from \emph{epistemic uncertainty}. We don't know when edges are used,
directly, and we also don't have a reliable way to estimate the
opportunities each edge had to activate (their exposure), either. This
is especially true when we wish to know whether an edge even \emph{can}
be traversed, i.e.~the edge \emph{support}. Support, as used in this
sense, is the set of inputs for which we expect a non-zero output.
Intuitively, this idea captures the sense that we might care more about
\emph{whether} an edge/dependency exists, not \emph{how important} it
is. For that, we have to re-assess our simple model: even if we could
count the number of times an edge might have been traversed, how do we
estimate the opportunities it had to be available for traversal (it's
``exposure'')?

Assuming this kind of epistemic uncertainty can be adequately addressed
through modeling---attempts at which will be discussed in more detail in
Chapter~\ref{sec-lit-review}---conditional probability/contingency
tables will again be useful for validation. When comparing estimated
edge probability to some known ``true'' edge existence (if we have
that), we can count the number of correct predictions, as well as type I
(false positive) and type II (false negative) errors. We can do this at
\emph{every probability/weight threshold value}, as well, and we will
return to ways to aggregate all of these values into useful scoring
metrics in Section~\ref{sec-FP-experiments}.

\section{Proximity vs.~Incidence}\label{sec-products}

As alluded to in the previous section, co-occurrence seems to have a
deep connection to a Gram matrix, which is a

\subsection{Kernels \& distances}\label{kernels-distances}

Importantly for the use of linear algebra, these values assigned for
each feature are assumed to exist in a field (or, more generally, a
semiring) \(R\), equipped with operators analogous to addition
(\(\oplus\)) and multiplication (\(\otimes\)) that allow for values to
be aggregated through an inner product. The matrix of all pairs of
inner-products found by matrix multiplication (contracting over the
feature space) is given by:

\begin{equation}\phantomsection\label{eq-gen-matmul}{
G(\mathbf{x}_i, \mathbf{x}_j,R) = G_{ij} = \bigoplus_{k=1}^{n} x_{ik} \otimes x_{kj}
}\end{equation}

such that real-valued entries and a traditional ``plus-times'' inner
product recovers the Gram matrix

\[
G_{ij}= X^TX\sum_{k=1}^{n} x_{ik}x_{kj}
\]

How ``close'' or ``far away'' things are\ldots. Avrachenkov et al.~

Important: these measurements often assume distance is defined in terms
of the measurements/objects/data, but for \emph{inverse problems},
structure learning, etc., they are more often applied in terms of the
features/operators.

Example with doc-term matrices

The inner product between two papers will yield a ``true'' only if two
papers share at least one author in common. This is called a
\emph{bipartite projection}{[}CITE{]}, specifically the ``papers''
projection.

Similarly, if our goal is to determine a network of ``whether two
authors ever coauthored'', we could perform a bipartite projection using
the boolean inner product in the observation space i.e.~the ``authors''
projection. It is this second projection, for determining a structure
between features embedded into the ``observation'' space, that we are
primarily concerned with in this work, since it is the view that most
closely resembles the concept of covariance or correlation between
independent variables (features) in statistics more generally.

\subsection{Incidence structures \&
dependency}\label{incidence-structures-dependency}

foundational model of graph theory and incidence structures more
broadly. More to come, but get the terminology down.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\[
%X(\{1,2,3,4,\cdots\})=\\
\begin{array}{c c}
& \begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\ \end{array} \\
\begin{array}{c c } x_1\\x_2\\x_3\\x_4\\ \vdots \end{array} &
\left[
\begin{array}{c c c c c c c c c c}
  0  &  0  &  1  &  0  &  1  &  0  &  1  &  1  &  0  &  0 \\
  1  &  0  &  0  &  0  &  1  &  1  &  0  &  0  &  0  &  0 \\
  0  &  1  &  0  &  0  &  0  &  1  &  0  &  0  &  1  &  1 \\
  0  &  0  &  0  &  1  &  1  &  0  &  0  &  1  &  0  &  0 \\
  &&&& \vdots &&&&&
\end{array}
\right]
\end{array}
\]

}

\subcaption{\label{fig-biadj-mat}}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part1/1-01-matrix-meas_files/figure-latex/..-codefigs-graphs-fig-bipartite-output-1.pdf}}

}

\subcaption{\label{fig-bipartite}Bipartite representation of node
``activation'' data}

\end{minipage}%

\end{figure}%

\begin{itemize}
\tightlist
\item
  Spring example, road example, etc.
\item
  partial correlations
\end{itemize}

\subsection{Implications for networks}\label{implications-for-networks}

Usually dependencies are taken as causing or enabling proximity. E.g.
shortest paths, vs.~edges.

\begin{itemize}
\tightlist
\item
  Discuss Complex Systems and their representation.
\end{itemize}

The approach taken by researchers/investigators\ldots do they assume a
level of interchangeability between the two kinds of ``relation''? Do
they define Or do they

\chapter{Incidence through vector
representation}\label{incidence-through-vector-representation}

To provide a sufficiently rigorous foundation for network recovery from
binary occurrence, we will need a rigorous way to represent networks and
occurrences that lends itself to building structured ways both connect
to each other. We build on the incidence structure and matrix product
formalism from the previous chapter, introducing various ways to build
graphs as incidence structures that have direct representations as
matrices. This can be extended to representing occurrences as matrices
of \emph{hyperedge vectors}. This view allows us to interpret different
representations of graphs (or hypergraphs) as connected by simple matrix
operations.

Traditionally\autocite{MathematicalfoundationsGraphBLAS_Kepner2016,WhyHowWhen_Torres2021},
we might say a graph on nodes (or, ``vertices'')
\(v\in V=\{1,\cdots,n\}\) and ``edges'' \(E\) is a tuple: \[
G=(V,E) \quad \textrm{s.t.} \quad E\subseteq V \times V
\]

The \emph{adjacency matrix} \(A\) of \(G\), degree matrix \(D\), and
graph/discrete Laplacian \(L\) are then defined as:\footnote{ The
  \emph{indicator function} \(\mathbf{1}_A(x)\) is 1 for values of \(x\)
  in the set \(A\), and 0 otherwise.} \[
\begin{aligned}
A(u,v) & =\mathbf{1}_E((u,v)) \quad &A : V\times V\rightarrow \mathbb{B} \\
D(u,v) & =\mathrm{diag}({\textstyle\sum}_V A(u,v))\quad &D : V\times V\rightarrow \mathbb{N} \\
L(u,v) & = D(u,v) - A(u,v) \quad &L : V\times V\rightarrow \mathbb{Z} 
\end{aligned}
\]

However, if edges and their recovery is so important to us, defining
them explicitly as pairs of nodes can be problematic when we wish to
estimate their existence (or not) when pairs of nodes co-occur.
Additionally, we have to be very careful to distinguish whether our
graph is \emph{(un)directed}, \emph{weighted}, \emph{simple}, etc., and
hope that the edge set has been filtered to a subset of \(N\times N\)
for each case. Instead, we propose a less ambiguous framework for
vectorizing graphs, based on their underlying incidence structure.

\section{Graphs as incidence
structures}\label{graphs-as-incidence-structures}

Instead, we give edges their own set of identifiers,
\(e\in E=\{1,\cdots \omega\}\). Now, define graphs as incidence
structures between nodes and edges such that every edge is incident to
either two nodes, or none:

\begin{equation}\phantomsection\label{eq-simple-incidence}{
G = (V,E,\mathcal{I}) \quad s.t. \quad \mathcal{I} \subseteq E\times V
}\end{equation}

Variations on graphs can often be conveniently defined as constraints on
\(\mathcal{I}\):

\begin{itemize}
\tightlist
\item
  Self loops can be prohibited by only allowing unique flags for a given
  relation\footnote{ never two flags with the same pair,
    i.e.~\(\mathcal{I}\) is a set, not a multiset.}
\item
  Multigraphs are similarly described by whether we allow pairs of
  vertices to appear with more than one edge\footnote{ in the set of
    flags containing nodes \(u\) or \(v\), only one \(e\) may be
    incident to both of them.}
\end{itemize}

Together, these constraints define ``simple'' graphs. Similarly, we can
equip Equation~\ref{eq-simple-incidence} with a function \(B\) that
allows \(\mathcal{I}\) to encode information about the specific kinds of
incidence relations under discussion. We give \(B\) a range of possible
flag values \(S\):

\begin{equation}\phantomsection\label{eq-map-incidence}{
G = (V,E,\mathcal{I},B) \quad s.t. \quad \mathcal{I} \subseteq E\times V\quad B:\mathcal{I}\rightarrow S
}\end{equation}

\begin{itemize}
\tightlist
\item
  Undirected, unweighted graphs only need single elements: ``incidence
  exists'' i.e.\(S=\{1\}\)
\item
  Directed graphs can use two elements e.g.~a ``spin'' for
  \(S=\{-1,1\}\)
\item
  Weighted, undirected graphs are supported on positive scalars
  e.g.~\(S=\mathbb{R}^+\)
\item
  Weighted, directed graphs are supported on any scalar
  e.g.~\(S=\mathbb{R}_{\neq0}\)
\end{itemize}

If the ``absence'' of incidence needs to be modeled explicitly, a
``null'' stand-in (0,False) can be added to each \(S\), which is useful
for representing each structure as arrays for use with linear algebra
(i.e.~\(\{0,1\}\),\(\{-1,0,-1\}\),\(\mathbb{R}^+_0\), and
\(\mathbb{R}\), respectively). By doing so, we can also place an exact
limit on the maximum possible size of \(\omega=\|E\|\) in the simple
graph case, and indicate edges by their unique ID, such that
\(\mathcal{I}= E\times V\) is no longer a subset relation for
\(E=\{1,\cdots,{n\choose2} \}\). Instead of existence in
\(\mathcal{I}\), we explicitly use incidence relation \(S\) to tell us
whether each possible edge ``exists'' or not, simplifying our graph
definition further\footnote{ if we allow multi-edges, then}:

\begin{equation}\phantomsection\label{eq-incidence-graph}{
\begin{aligned}
G  = (V,E,B) \quad s.t. \quad & B : E\times V \rightarrow S\\
& v \in V = \{1,\cdots, n\}\quad \\
& e \in E = \left\{1,\cdots, {n\choose 2} \right\}
\end{aligned}
}\end{equation}

\subsection{Embedding in vector space}\label{embedding-in-vector-space}

The representation of \(B\) in Equation~\ref{eq-incidence-graph} bears a
remarkable similarity to our original description of design matrices in
Equation~\ref{eq-design-mat}. In fact, as a matrix, \(B(e,v)\) is called
the \emph{incidence} matrix: every row has two non-zero entries, with
every column containing a number of non-zero entries equal to that
corresponding node's degree in \(G\). Traditionally, we use an
\emph{oriented} incidence matrix, such that each row has exactly one
positive (non-zero) value, and one negative (non-zero) value.\footnote{
  In fact, this would make B\^{}*(v,e) equivalent to a \emph{graphical
  matroid}, another common formalism that generalizes graph-like
  structures to vector space representations.} Even for undirected
graphs, the selection of \emph{which entry} is positive or negative is
left to be ambiguous, since much of the math used later is symmetric
w.r.t. direction\footnote{though not always!}.

However, we can be more precise by selecting each row(edge) vector, and
partitioning it into two: one for each non-zero column (node) that edge
is incident to. This makes every incidence be embedded as standard basis
vector \(\mathbf{e}\) in the feature space of \(B\), scaled by the
corresponding value of \(S\). Let \(V_e\) be the set of nodes with
(non-zero) incidence to edge \(e\). Then the incidence vectors are:\\
\[\delta_e(v) = B(e,v)\mathbf{e}_v \quad \forall v\in V_e\] The
unoriented incidence matrix is easily defined as having rows that are
sums over the incidence vectors for each edge:
\(\mathbf{b}^+_e = \sum_{v\in V_e} \delta_e(v)\)

To build the oriented incidence matrix, one could define undirected
graphs as equivalent to multidigraphs, where each edge is really two
directed edges, in opposing directions. This does allow the matrix \(B\)
to have the correct range for its entries in this formalism (the
directed graph range, \(S=\{-1,0,1\}\)), and the edge definition based
on sums would hold. The resulting set of incidences would have twice the
number of edges than our combinatoric limit for simple graphs, however.
Plus, it would necessitate averaging of weights over different edge ID's
to be prior to use of inner products, and many other implementation
difficulties.

Instead we would like to allow for systematic differences between the
incidence vectors, without ambiguity. But, now that we have removed the
information on ``which nodes an edge connects'' from our definition
(since every edge is a scalar ID), how do we construct \(V_e\) without a
costly search over all incidences?\\
Because of our unique identification of edges up to the combinatoric
limit, we can still actually provide a unique ordering of the nodes it
each edge connects. Using an identity from
\textcite{ParallelEuclideandistance_Angeletti2019}, we have a
closed-form equation both to retrieve the IDs of nodes \(u,v\) (given an
edge \(e\)), and an edge \(e\) (given two nodes \(u,v\)), for any simple
graph with \(n\) vertices:
\begin{equation}\phantomsection\label{eq-sq-id}{
\begin{aligned}
    u_n(e) & = n - 2 - \left\lfloor\frac{\sqrt{-8e + 4n(n - 1) - 7}-1}{2}\right\rfloor\\
    v_n(e) & = e + u_n(e) + 1 -\frac{1}{2} \left(n (n - 1) + (n - u_n(e))^2 - n+u_n(e)\right)\\
    e_n(u,v) & = \frac{1}{2}\left(n(n - 1) - ((n - u)^2 - n+u)\right) + v - u - 1
\end{aligned}
}\end{equation}

Using this, we can unambiguously define a \emph{partition} between
incidences on \(e\), without needing other metadata or flags to
distinguish \(u\) from \(v\), since those are defined through
Equation~\ref{eq-sq-id} unambiguously w.r.t. \(e\).
\[\mathbf{b}_e = \delta_e(u)-\delta_e(v)\]

This could be called an ``edge-centric'' view of simple graphs, since
the nodes involved in a graph are now actually derived w.r.t. the edges
and our mapping \(B\), uniquely.

\subsection{\texorpdfstring{Inner products on
\(B\)}{Inner products on B}}\label{inner-products-on-b}

Laplacian as inner product on incidence observations. Associated objects
(degree vector, o)

Rescaling to achieve normaalization.

Use to define kernels (and application e.g.~soft-cosine measure)

\ldots{}

\subsection{Metrological Considerations: Interaction
Vectors}\label{metrological-considerations-interaction-vectors}

Strictly speaking, we can't say that nodes are directly observed in this
space\ldots{} edges are. Collections of nodes are measured two-at-a-time
(one-per-edge being traversed).

Another way to approach is to view inner products as a sum of outer
products. A each edge uniquely corresponds to 2 nodes (in a simple
graph). Use triangle unfolding for closed form bijection.

Unrolling 3D tensor of subgraphs along eads to a secondary
representation of graphs as an \emph{edgelist}, having binary activation
vectors on edges rather than nodes. Then each observation in this model
is necessarily a set of activated edges. The non-zero (visited) nodes
are found using the incidence matrix as an operator.

\section{Graphs and Node Occurences}\label{graphs-and-node-occurences}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{content/part1/../images/hypergraph-observations.png}}

}

\caption{Hyperedge Relation Observational Model}

\end{figure}%

\subsection{Hyperedges as Vectors of
Nodes}\label{hyperedges-as-vectors-of-nodes}

\subsection{Inner product on
Hyperedges}\label{inner-product-on-hyperedges}

Roundabout way of describing binary/occurrence data. Inner product is
co-occurrences.

Leads to correlation/covariance, etc.

\subsection{Combining Occurence \&
Dependence}\label{combining-occurence-dependence}

\begin{itemize}
\tightlist
\item
  soft cosine
\item
  kernels on graphs (incl.~coscia euclidean)
\item
  Retrieving one from the other is hard.
\end{itemize}

\chapter{Roads to Network Recovery}\label{sec-lit-review}

\section{Choosing a structure recovery
method}\label{choosing-a-structure-recovery-method}

\begin{quote}
Takeaway: a way to organize existing algorithms, AND highlight unique
set of problems we set out to solve
\end{quote}

\section{Organizing Recovery Methods}\label{organizing-recovery-methods}

i.e.~Network Recovery as an Inverse Problem, and what information is had
at each point.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{content/part1/../images/adjoint-cheatsheet.png}}

}

\caption{Relating Graphs and Hypergraph/bipartite structures as adjoint
operators}

\end{figure}%

\subsection{Observing Nodes vs Edges}\label{observing-nodes-vs-edges}

\subsection{Embeddings, Inner Products, \&
Preprocessing}\label{embeddings-inner-products-preprocessing}

\section{Tracing Information Loss
Paths}\label{tracing-information-loss-paths}

\subsection{Table of Existing
Approaches}\label{table-of-existing-approaches}

\begin{itemize}
\tightlist
\item
  Observation-level loss (starting with the inner product or kernel)
\item
  Non-generative model loss (no projection of data into model space)
\item
  no uncertainty quantification
\end{itemize}

\subsection{A Path Forward}\label{a-path-forward}

Sorting algorithms\ldots{} \emph{none address all three!}

i.e.~MOTIVATES FOREST PURSUIT

\part{Nonparametric Network Recovery With Random Spanning Forests}

\chapter{Desire Paths and Spanning
Forests}\label{desire-paths-and-spanning-forests}

Addressing gaps discussed in the previous section to reach a generative
model for network recovery requires careful attention to the generation
mechanism for node activations. While there are many ways we might
imagine bipartite data to be be generated, presuming the existence of a
dependency graph that \emph{causes} activation patterns will give us
useful ways to narrow down the generative specification.

First, we will investigate the common assumption that pairwise
co-occurrences can serve as proxies for measuring relatedness, and how
this ``gambit of the group'' is, in fact, a strong bias toward dense,
clique-filled recovered networks. Because we wish to model our node
activations as being \emph{caused} by other nodes (that they depend on),
we draw a connection to a class of models for \emph{spreading}, or,
\emph{diffusive processes}. We outline how random-walks are related to
these diffusive models of graph traversal, enabled by an investigation
of the graph's ``regularized laplacian'' from
\textcite{Semisupervisedlearning_Avrachenkov2017}. Then we use the
implicit causal dependency tree structure of each observation, together
with the Matrix Forest Theorem
\autocite{MatrixForestTheorem_Chebotarev2006,Countingrootedforests_Knill2013}
to more generally define our generative node activation model. This
leads to a generative model for binary activation data as rooted random
spanning forests on the underlying dependency graph.

\section{The Gambit of the Inner
Product}\label{the-gambit-of-the-inner-product}

As we saw repeatedly in Chapter~\ref{sec-lit-review}, networks are
regularly assumed to arise from co-occurrences, whether directly as
counts or weighted in some way. This assumption can be a significant a
source of bias in the measurement of edges. \emph{Why} a flat count of
co-occurrence leads to ``hairballs'' and bias for dense clusters can be
related to the use of inner products on node activation vectors.

\subsection{Gambit of the Group}\label{gambit-of-the-group}

It seems reasonable, when interactions are unobserved, to assume some
evidence for all possible interactions is implied by co-occurrence.
However, similar to the use of uniform priors in other types of
inference, if we don't have a good reason to employ a fully-connected
co-occurrence prior on interaction dependencies, we are adding
systematic bias to our inference. Whether co-occurrence observations can
be used to infer interaction networks directly was discussed at length
in \textcite{Techniquesanalyzingvertebrate_Whitehead1999}, where
Whitehead and Dufault call this the \emph{gambit of the group}.

\begin{quote}
So, consiously or unconsciously, many ethnologists studying social
organization makr what might be called the ``gambit of the group'': the
assume that animals which are clustered {[}\ldots{]} are interacting
with one another and then use membership of the same cluster
{[}\ldots{]} to define association.
\end{quote}

This work was rediscovered in the context of measuring assortativity for
social networks,\footnote{Assortativity is, roughly, the correlation
  between node degree and the degrees of its neighbors.} where the
author of \textcite{PerceivedAssortativitySocial_Fisher2017} advises
that ``group-based methods'' can introduce sampling bias to the
calculation of assortativity, namely, systematic overestimation when the
sample count is low.

The general problems with failing to specify a model of what ``edges''
actually \emph{are} get analyzed in more depth in
\textcite{Statisticalinferencelinks_Peel2022}. They include a warning
not to naively use correlational measures with a threshold, since even
simple 3-node systems will easily yield false positives edges. Still, it
would be helpful for practitioners to have a more explicit mental model
of \emph{why} co-occurence-based models yield systematic bias,

\subsection{Inner Products as Sums of
Cliques}\label{inner-products-as-sums-of-cliques}

Underlying correlation and co-occurrence models for edge strength is a
reliance on inner products between node occurrence vectors. They all use
gram matrices (or centered/scaled versions of them), which were brought
up in Section~\ref{sec-products}. The matrix multiplication performed
represents inner products between all pairs of feature vectors. For
\(X(i,j)\in\mathbb{B}\), these inner products sum together the times in
each observation that two nodes were activated together.

However, another (equivalent) way to view matrix multiplication is as a
sum of outer products
\[ G(i,j) = X^T X = \sum_{i=1}^k X(i,k)X(j,k)= \sum_{i=1}^k \mathbf{x}_k\mathbf{x}_k^T \]
Those outer products of binary vectors create \(m\times m\) matrices
that have a 1 in every \(i,j\) entry where nodes \(i,j\) both
occurred---i.e., they can be seen as adjacency matrices of the clique on
nodes activated in the \(k\)th observation In this sense, any method
that involves transforming or re-weighting a gram matrix, is implicitly
believing that the \(k\)th observation was a \emph{complete graph}. This
is illustrated in Figure~\ref{fig-stacked-graphs}.

\begin{figure}

\begin{minipage}{0.67\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-01-rand-sf_files/figure-latex/..-codefigs-graphs-fig-stack-bow-output-1.pdf}}

}

\subcaption{\label{fig-stack-bow}Edge Measurements with Group Gambit
(BoW) assumption}

\end{minipage}%
%
\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-01-rand-sf_files/figure-latex/..-codefigs-graphs-fig-bipartite-output-1.pdf}}

}

\subcaption{\label{fig-bipartite}Bipartite representation of node
``activation'' data}

\end{minipage}%

\caption{\label{fig-stacked-graphs}Inner-product projections imply
observations of complete graphs, summed.}

\end{figure}%

For many modeling scenarios, this paradigm allows practitioners to make
a more straight-forward intuition-check: do clique observations
\emph{make sense} here? When a list of authors for a paper is finished,
does that imply that all authors mutually interacted with all others
directly to equally arrive at the decision to publish? This would be
similar to assuming the authors might simultaneously enter a room, look
at a number of others (who all look exclusively at each other, as well),
and at once decide to publish together. In our introduction, we
described a more likely scenario we could expect from an observer on the
ground: a researcher asks a colleague or two to collaborate, who might
know a couple more with relevant expertise, and so on.

\section{Networks as Desire Path Density
Estimates}\label{networks-as-desire-path-density-estimates}

Unfortunately, methods based on inner-product thresholding are still
incredibly common, in no small part due to how \emph{easy} it is to
create them from occurrence data. What we need is a way to retain the
ease of use of inner-product network creation, with a more
domain-appropriate graph at the observation level. Of course there are
many classes of graphs we might believe local interactions occur on:
path-graphs, trees, or any number of graphs that reflect the topolgy or
mechanism of local interactions in our domain of interest. Authors have
proposed classes of graphs that mirror human perception of set shapes
{[}RELATIVE NEIGHBORHOOD{]}\footnote{ e.g.~for dependencies based on
  perception, such as human decision making tendencies, or causes based
  on color names.}, or graphs whose modeled dependencies are strictly
planar {[}planar maximally filtered graps{]}\footnote{ e.g.~when
  interactions are limited to planar dependencies, like inferring
  ancient geographic borders.}. Alternatively, the interactions might be
scale free, small-world, or samples from stochastic block
models.{[}CITE{]}

In any case, these assumptions provide an explicit way to describe the
set of \emph{possible} interaction graphs we believe our individual
observations are sampled from. In other words, we limit our possible
interaction graph to a set of all possible graphs in our class
\(\mathcal{C}\), and model the interactions allowed to be inferred from
the activated nodes as \(c\in\mathcal{C}\). With an associated
probability measure \(\mu_{\mathcal{C}}(c)\) defined on subgraphs of the
complete graph on our node set V\footnote{Assuming we can even find one,
  something we must return to shortly}, we are able to say our
interactions are sampled from a distribution over graphs in the class.

For simplicity, since unreported/hidden-but-activated nodes is outide
the scope of our work, we narrow the distribution to be only on the
induced subgraph \(C(S_k) \in G[S_k])\), where \(S_k\) is the set of
activated nodes in \(\mathbf{x}_k\). More specifically, we say that an
observation of \(S\subset V=\{s_1,\cdots, s_t\},\quad t=|S|\) activated
nodes implies a distribution over edge vectors we could hav observed,
determined by the chosen class,

\[\mathbf{x}^E_k \in \]

Once an analyst has provided epistemic justification for a \emph{class
of graphs} to model We propose that the computationally-efficient
inner-product networks can still be used, but could be made far more
effective by counting edge observation counts with something more
appropriate than cliques.

The class of diffusive processes we focus on ``spread'' from one node to
another. If a node is activated, it is able to activate other nodes it
is connected to, directly encoding our need for the graph edges to
represent that nodes ``depend'' on others to be activated. In this case,
a node activates when another node it depends on spreads their state to
it. These single-cause activations are equivalent to imagining a
random-walk on the dependency graph, where visiting a node activates it.

\subsection{Random Walk Activations}\label{random-walk-activations}

Random walks are regularly employed to model spreading and diffusive
processes on networks. If a network consists of locations, states,
agents, etc. as ``nodes'', and relationships between nodes as ``edges'',
then random walks consist of a stochastic process that ``visits'' nodes
by randomly ``walking'' between them along connecting edges.
Epidemiological models, cognitive search in semantic networks, stock
price influences, website traffic routing, social and information
cascades, and many other domains also involving complex systems, have
used the statistical framework of random walks to describe, alter, and
predict their behaviors. {[}CITE\ldots lots?{]}

When network structure is known, the dynamics of random-walks are used
to capture the network structure via sampling {[}LITTLEBALLOFFUR,
etc{]}, estimate node importance's{[}PAGERANK{]}, or predict
phase-changes in node states (e.g.~infected vs.~uninfected){[}SIR I
think{]} In our case, Since we have been encoding the activations as
binary activation vectors, the ``jump'' information is
lost---activations are ``emitted'' for observation only upon the random
walker's initial visit. {[}CITE INVITE{]} In many cases, however, the
existence of relationships is not known already, and analysts might
\emph{assume} their data was generated by random-walk-like processes,
and want to use that knowledge to estimate the underlying structure of
the relationships between nodes.

\begin{itemize}
\tightlist
\item
  useful tool for analysis of our data: reg laplacian
\item
  interpretations
\end{itemize}

\subsection{Dependencies as Trees}\label{dependencies-as-trees}

The whole graph isn't a tree\ldots.Every data point is.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-01-rand-sf_files/figure-latex/..-codefigs-graphs-fig-stack-tree-output-1.pdf}}

}

\caption{\label{fig-stack-tree}Edge Measurements with true (tree)
dependencies known}

\end{figure}%

{[}GRAPHIC 1 - my data{]}

{[}GRAPHIC 2 - infection vector from meta node{]}

\subsection{Matrix Tree and Forest
Theorems}\label{matrix-tree-and-forest-theorems}

\begin{itemize}
\tightlist
\item
  one from kirchoff
\item
  one from Chebotarv
\end{itemize}

\section{Generative Model
Specification}\label{generative-model-specification}

\pandocbounded{\includegraphics[keepaspectratio]{content/part2/../images/random-spanning-forests.png}}
- hierarchical model - marginalize over the root node.

\chapter{Forest Pursuit: Approximate Recovery in Near-linear
Time}\label{forest-pursuit-approximate-recovery-in-near-linear-time}

\begin{quote}
filling the gap we saw in the literature
\end{quote}

\section{Sparse Dictionary Learning}\label{sparse-dictionary-learning}

\subsection{Problem Specification}\label{problem-specification}

\subsection{Matching Pursuit}\label{matching-pursuit}

\subsection{Space of Spanning Forests}\label{space-of-spanning-forests}

\section{Forest Pursuit: Approximate Recovery in Near-linear
Time}\label{forest-pursuit-approximate-recovery-in-near-linear-time-1}

I.e. the PLOS paper (modified basis-pursuit via MSTs) \#\#\# Algorithm
Summary

\subsection{Uncertainty Estimation}\label{uncertainty-estimation}

\subsection{Approximate Complexity}\label{approximate-complexity}

\section{Simulation Study}\label{sec-FP-experiments}

\subsection{Method}\label{method}

\subsection{Results - Scoring}\label{results---scoring}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-02-forest-pursuit_files/figure-latex/..-codefigs-results-fig-fp-overall-output-1.pdf}}

}

\caption{\label{fig-fp-overall}Comparison of MENDR recovery scores}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-02-forest-pursuit_files/figure-latex/..-codefigs-results-fig-fp-compare-output-1.pdf}}

}

\caption{\label{fig-fp-compare}Comparison of MENDR Recovery Scores by
Graph Type}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-02-forest-pursuit_files/figure-latex/..-codefigs-results-fig-partials-mcc-output-1.pdf}}

}

\caption{\label{fig-partials-mcc}Partial Residuals (regression on
E{[}MCC{]})}

\end{figure}%

\subsection{Results - Performance}\label{results---performance}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{content/part2/2-02-forest-pursuit_files/figure-latex/..-codefigs-results-fig-runtime-output-1.png}}

}

\caption{\label{fig-runtime}Runtime Scaling (Forest-Pursuit vs GLASSO)}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-02-forest-pursuit_files/figure-latex/..-codefigs-results-fig-partials-runtime-output-1.pdf}}

}

\caption{\label{fig-partials-runtime}Partial Residuals (regression on
computation time)}

\end{figure}%

\section{Discussion}\label{discussion}

\subsection{Interaction Probability}\label{interaction-probability}

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/../images/PR.pdf}}

\chapter{LFA: Latent Forest
Allocation}\label{lfa-latent-forest-allocation}

\section{Radom Spanning Trees}\label{radom-spanning-trees}

\begin{itemize}
\tightlist
\item
  Methods for sampling i.e.~wilson's and Duan's (other? Energy paper?)
\item
  Tree Likelihoods, other facts
\end{itemize}

\section{Bayesian Estimation by Gibbs
Sampling}\label{bayesian-estimation-by-gibbs-sampling}

\begin{itemize}
\tightlist
\item
  comparison with LDA
\item
  Simplifying Assumptions (conditional prob IS prob for this)
\end{itemize}

I.e. the unwritten paper, modifying technique by
\textcite{BayesianSpanningTree_Duan2021} for RSF instead of RSTs

\section{Simulation Study}\label{simulation-study}

\subsection{Score Improvement}\label{score-improvement}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-03-latent-forest-alloc_files/figure-latex/..-codefigs-results-fig-efm-mcc-output-1.pdf}}

}

\caption{\label{fig-efm-mcc}Change in Expected MCC (EFM vs FP)}

\end{figure}%

\subsection{Odds of Individual Edge
Improvement}\label{odds-of-individual-edge-improvement}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-03-latent-forest-alloc_files/figure-latex/..-codefigs-results-fig-efm-logits-output-1.pdf}}

}

\caption{\label{fig-efm-logits}Logistic Regression Coef. (EFM - FP)
vs.~(Ground Truth)}

\end{figure}%

\part{Applications \& Extentions}

\chapter{Qualitative Application of Relationship
Recovery}\label{qualitative-application-of-relationship-recovery}

\section{Network Science Collaboration
Network}\label{network-science-collaboration-network}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-netsci-cooc-output-1.pdf}}

}

\caption{\label{fig-netsci-cooc}134 Network scientists from
{[}NEWMAN;BOCCALETTI;SNEPPEN{]}, connected by co-authorship}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-netsci-tree-output-1.pdf}}

}

\caption{\label{fig-netsci-tree}Max. likelihood tree dependency
structure to explain co-authorships}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-netsci-fp-output-1.pdf}}

}

\caption{\label{fig-netsci-fp}Forest Pursuit estimate of NetSci
collaborator dependency relationships}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-netsci-degree-output-1.pdf}}

}

\caption{\label{fig-netsci-degree}}

\end{figure}%

\section{Les Miserables Character
Network}\label{les-miserables-character-network}

\subsection{Backboning}\label{backboning}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-lesmis-cooc-output-1.pdf}}

}

\caption{\label{fig-lesmis-cooc}}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-lesmis-fp-output-1.pdf}}

}

\caption{\label{fig-lesmis-fp}}

\end{figure}%

\subsection{Character Importance
Estimation}\label{character-importance-estimation}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-06-qualitative_files/figure-latex/..-codefigs-qualitative-fig-lesmis-centrality-output-1.pdf}}

}

\caption{\label{fig-lesmis-centrality}}

\end{figure}%

\chapter{Recovery from Partial
Orders}\label{recovery-from-partial-orders}

Like before, but with the added twist of \emph{knowing} our nodes were
activated with a particular partial order.

\section{Technical Language
Processing}\label{technical-language-processing}

\emph{insert from
\autocite{OrganizingTaggedKnowledge_Sexton2020,UsingSemanticFluency_Sexton2019}}

\section{Verbal Fluency Animal
Network}\label{verbal-fluency-animal-network}

\subsection{Edge Connective Effiency and
Diversity}\label{edge-connective-effiency-and-diversity}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-dsmin-output-1.pdf}}

}

\caption{\label{fig-fluency-dsmin}}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-tree-output-1.pdf}}

}

\caption{\label{fig-fluency-tree}}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-glassomin-output-1.pdf}}

}

\caption{\label{fig-fluency-glassomin}}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-fpmin-output-1.pdf}}

}

\caption{\label{fig-fluency-fpmin}Comparison of backboning/dependency
recovery methods tested vs.~Forest Pursuit}

\end{figure}%

\subsection{Thresholded Structure
Preservation}\label{thresholded-structure-preservation}

Differences in structural preservation with increased thresholding.

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-preservation-output-1.pdf}}

}

\subcaption{\label{fig-fluency-preservation-1}co-occurrence methods will
retain local communities at the cost of global structure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-preservation-output-2.pdf}}

}

\subcaption{\label{fig-fluency-preservation-2}dependency network drops
rarer nodes from the preserved central structure at higher uncetainty
cutoffs}

}

\caption{\label{fig-fluency-preservation}When only retaining the top 2\%
of edge strengths, blah}

\end{figure}%

\subsection{Forest Pursuit as
Preprocessing}\label{forest-pursuit-as-preprocessing}

Differences in structural preservation with increased thresholding.

Retaining the top 2\% of edges, co-occurrence retains local communities

at the cost of global structure.

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-preprocess-output-1.pdf}}

}

\subcaption{\label{fig-fluency-preprocess-1}Islands of local structure
remain (doubly-stochastic)}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-ordered_files/figure-latex/..-codefigs-qualitative-fig-fluency-preprocess-output-2.pdf}}

}

\subcaption{\label{fig-fluency-preprocess-2}Intact global structure with
isolates}

}

\caption{\label{fig-fluency-preprocess}We might prefer to drop
low-certainty/rare nodes from a preserved central structure.}

\end{figure}%


\printbibliography



\end{document}
