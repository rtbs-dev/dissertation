% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\RequirePackage[l2tabu,orthodox]{nag}
\documentclass[%
	12pt,
		oneside,
		letterpaper
]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

\directlua{pdf.setminorversion(6)}
% \usepackage[T1]{fontenc}
% \usepackage[utf8]{inputenc}
\usepackage{pdfpages}
\usepackage[english]{babel}
\usepackage[otfmath]{XCharter}
\DeclareMathAlphabet{\mathcal}{OMS}{cmsy}{m}{n}

\usepackage[strict]{csquotes}
\usepackage{tikz-network}
% Change contents title to Table of Contents
\addto\captionsenglish{% Replace "english" with the language you use
  \renewcommand{\contentsname}%
    {Table of contents}%
}

% Bibliography
% \usepackage[%
% 	backend=biber,
% 	style=ieee, % pick whatever style you want
% 	sorting=ydnt,
% 	isbn=true,
% ]{biblatex}
% \addbibresource{resources/pubs.bib}

% Line spacing
\usepackage{setspace}

\usepackage{etoolbox}

\usepackage[unicode=false]{hyperref}
\usepackage{algpseudocode}
% \usepackage[notbib,notindex]{tocbibind}

\usepackage[%
	%textwidth=345pt, % Default textwidth
	% marginpar=4cm, % Make marginal notes wider
	% includemp, % Include marginpar in width
	margin=1in,
	left=1in, % Left margin must be at least 1 inch
]{geometry}

% Toggle for double spaced or not
% \newbool{doublespaced}
\usepackage{titlesec}
   

\titleformat{\part}[display]{\normalfont\large}{Part \ \thepart}{2em}{}[]
\titleformat{\chapter}[block]{\normalfont\large}{Chapter \ \thechapter}{1em}{}[]
\titleformat{name=\chapter,numberless}[block]{\normalfont\large}{}{1em}{\centering}[]
\titleformat{\section}{\bfseries\normalsize}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\normalsize}{\thesubsection}{1em}{}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{algorithm}{}{\usepackage{algorithm}}
\makeatother
\makeatletter
\@ifpackageloaded{algpseudocode}{}{\usepackage{algpseudocode}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\makeatother
\newcounter{quartocalloutnteno}
\newcommand{\quartocalloutnte}[1]{\refstepcounter{quartocalloutnteno}\label{#1}}

\usepackage[style=ieee,backend=biber,sorting=ydnt,isbn=true]{biblatex}
\addbibresource{resource/pubs.bib}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Measuring Network Dependencies from Node Activations},
  pdfauthor={Rachael T.B. Sexton},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Measuring Network Dependencies from Node Activations}
\author{Rachael T.B. Sexton}
\date{2025-06-12}

\begin{document}

% \frontmatter
\pagestyle{empty}
\singlespacing

%Abstract Page
\hbox{\ }

\begin{center}
\large{{ABSTRACT}}

\vspace{3em}

\end{center}
\hspace{-.15in}
\begin{tabular}{p{0.35\linewidth}p{0.6\linewidth}}
Title of Dissertation:     & {\large \uppercase{Measuring Network Dependencies from Node Activations}}\\
                           & {\large  Rachael T.B. Sexton } \\
                           & {\large Doctor of Philosophy, } \\
\                         \\
Dissertation Directed by:  & {\large Professor Mark D. Fuge } \\
                           & {\large Department of Mechanical Engineering} \\
\end{tabular}

\vspace{3em}

% Use word count on the text file
% \begin{doublespacing}

% \renewcommand{\baselinestretch}{2}
\doublespacing
\large \normalsize
This thesis develops a method for inferring dependency networks from binary data by describing and accounting for sources of systematic bias.
Node ``activation'' data is a common source of complex networks, such as scientific collaboration networks from publications, to keyword and tag co-occurrence graphs from documents, epidemiological networks from infection records, and customer product recommendations from buyer carts, to name only a few.
However,the way these networks get created often lead to dense edge ``hairballs'' and heavy post-processing needs.
To more accurately reconstruct dependency networks, we propose a flexible technique that generalizes co-occurrence counting, and allows domain-knowledge to inform \emph{why} nodes activate (and how to use that for inference).

Using our technique, we obtain ``Desire Path Density'' estimates of latent networks: families of possible networks based on probability distributions over individual edge activations, rather than node co-occurrences.
We show that co-occurrence counting is a special case of this estimatation technique---and that it assumes node activations arise exclusively from cliques.
This assumption leads to ``clique-bias'' in the recovered graphs, which results in the unwanted hairball effect.
Our method, called \emph{Forest Pursuit}, is a different application of desire path density estimation that attempts to address this bias.
It assumes node activations are generated by sampling from distributions over dependency trees.

\emph{Forest Pursuit} outperforms other algorithms at accurate network recovery, is scalable to very large networks sizes, and can be executed in a streaming or parallel manner over observations.
It approximates Steiner trees for each set of activated nodes before aggregating them to estimate the latent global network.
We additionally extend \emph{Forest Pursuit} to be a probabilistic model whose parameters may be inferred through Expectation Maximization.
This modification shows improved performance over one-shot \emph{Forest Pursuit}, at the cost of computation time.

Our introductory chapters provide a unified framework to analyze node activation data and edge (dependency) occurrences in related vector spaces, and provide a taxonomy of network recovery assumption types so that practitioners can find and describe sources of systematic bias from those assumptions.
Then, along with derivations for Desire Path Density estimation and Forest Pursuit, we verify our method and compare its performance against others.
We created a dataset of synthetic experiments (MENDR), made publicly available as a reference and test-bench for the community.
Finally, we use several case studies to demonstrate the impacts of using \emph{Forest Pursuit} for network analysis in a realistic setting,with insights into the topological differences between networks inferred by other methods and ours.
By using \emph{Forest Pursuit}, practitioners can correct for clique-bias and better use their latent networks to uncover important behavior of complex systems.\par
% \end{doublespacing}
\clearpage%Titlepage

\thispagestyle{empty} \hbox{\ } \vspace{1.5in}
% \renewcommand{\baselinestretch}{1}
\singlespacing
\small\normalsize
\begin{center}

\large{\uppercase{Measuring Network Dependencies from Node Activations}}\\
\ \\ 
\ \\
\large{by} \\
\ \\
\large{Rachael T.B. Sexton}
\ \\
\ \\
\ \\
\ \\
\normalsize
Dissertation submitted to the Faculty of the Graduate School of the \\
University of Maryland, College Park in partial fulfillment \\
of the requirements for the degree of \\
Doctor of Philosophy \\

\end{center}

\vspace{7.5em}

\noindent Advisory Committee: \\
\hbox{\ }\hspace{.5in}Professor Mark D. Fuge, Chair/Advisor \\
\hbox{\ }\hspace{.5in}Professor Jordan L. Boyd-Graber, (Dean's Representative) \\
\hbox{\ }\hspace{.5in}Professor Maria K. Cameron \\
\hbox{\ }\hspace{.5in}Professor Michelle Girvan \\
\hbox{\ }\hspace{.5in}Professor Vincent P. Lyzinski \\
 \doublespacing

% \pagestyle{plain} \pagenumbering{roman} \setcounter{page}{2}

% 

% 
% \cleardoublepage
\floatname{algorithm}{Algorithm}

\numberwithin{algorithm}{chapter}


\bookmarksetup{startatroot}

\chapter*{Foreward}\label{foreward}
\addcontentsline{toc}{chapter}{Foreward}

\markboth{Foreward}{Foreward}

\pagenumbering{roman}

\begin{quote}
DISCLAIMER FOR PRIOR WORK
\end{quote}

Portions of Chapter~\ref{sec-desirepath}, Chapter~\ref{sec-fp}, Chapter~\ref{sec-extend}, along with network figures from Chapter~\ref{sec-qual} and Chapter~\ref{sec-ordered}, are planned for submission to PLOS \emph{Complex Systems} later this year.

Chapter~\ref{sec-fp} references \texttt{MENDR}, a standard reference dataset and test-bench for network recovery algorithms created in the course of completing this thesis.
It is awaiting DOI assignment through NIST internal review processes.
Likewise for \texttt{affinis}, a python library for dependency inference from binary activations containing our reference implementations of various algorithms (including \emph{Forest Pursuit}).

Portions of Chapter~\ref{sec-ordered} involving INVITE and recovery from partially ordered node sets are taken from \textcite{OrganizingTaggedKnowledge_Sexton2020} in the Journal of Mechanical Design, and its predecessor \textcite{UsingSemanticFluency_Sexton2019} from the 45th ASME Design Automation Conference.

\includepdf[pages=-]{resource/prev-pub-letter.pdf}

\thispagestyle{empty}
\singlespacing
\small\normalsize
\addcontentsline{toc}{chapter}{Table of Contents}
    \renewcommand{\contentsname}{Table of Contents}
\tableofcontents %(required, lower-case Roman)
\clearpage


\phantomsection %create the correct anchor for the bookmark
\addcontentsline{toc}{chapter}{List of Tables}
    \renewcommand{\contentsname}{List of Tables}
\listoftables %(if present, lower-case Roman)
\clearpage

\phantomsection %create the correct anchor for the bookmark
\addcontentsline{toc}{chapter}{List of Figures}
    \renewcommand{\contentsname}{List of Figures}
\listoffigures %(if present, lower-case Roman)
\clearpage

% LIST OF ABBREVIATIONS
%\phantomsection %create the correct anchor for the bookmark
% \addcontentsline{toc}{chapter}{List of Abbreviations}
% \include{Abbreviations-supertabular}
% TODO use acronym extension??

%\newpage
\setlength{\parskip}{0em}
\doublespacing
\small\normalsize

\pagenumbering{arabic}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

A wide variety of fields show consistent interest in inferring latent network structure from observed interactions, from human cognition and social infection networks, to marketing, traffic, finance, and many others. \autocite{Inferringnetworksdiffusion_GomezRodriguez2012}
However, an increasing number of authors are noting a lack of agreement in how to approach the metrology of this problem.
This includes rampant disconnects between the theoretical and methodological network analysis sub-communities\autocite{Statisticalinferencelinks_Peel2022}, treatment of error as purely aleatory, rather than epistemic \autocite{Measurementerrornetwork_Wang2012}, or simply ignoring measurement error in network reconstruction entirely\autocite{ReconstructingNetworksUnknown_Peixoto2018}.
This thesis builds on recent methodological recommendations for increased focus on how \emph{dependencies} should play a central role in network analysis \autocite{WhyHowWhen_Torres2021}, and facilitating a paradigm shift toward network analysis as \emph{inference} of an inverse problem \autocite{Statisticalinferencelinks_Peel2022}.

\section{Ambiguous Metrology}\label{ambiguous-metrology}

Networks in the ``wild'' rarely exist of and by themselves.
Rather, they are a model of interaction or relation \emph{between} things that were observed.
One of the most beloved examples of a network, the famed \emph{Zachary's Karate Club}\autocite{InformationFlowModel_Zachary1977}, is in fact reported as a list of pairwise interactions: every time a club member interacted with another (outside of the club), Zachary recorded it as two integers (the IDs of the members).
The final list of pairs can be \emph{interpreted} as an ``edge list'', which can be modeled with a network: a simple graph.
This was famously used to show natural community structure (Figure~\ref{fig-karate-club}) that nicely matches the group separation that eventually took place when the club split into two.\autocite{Communitystructuresocial_Girvan2002}

Note, however, that we could have just as easily taken note of the instigating student for each interaction (\emph{i.e.}, which student initiated conversation, or invited the other to socialize, etc.).
If that relational asymmetry is available, our ``edges'' are now \emph{directed}, and we might be able to ask questions about the rates that certain students are asked vs.~do the asking, and what that implies about group cohesion.
Additionally, the time span is assumed to be ``for the duration of observation'' (did the students \emph{ever} interact), but if observation time was significantly longer, say, multiple years, we might question the credulity of treating a social interaction 2 years ago as equally important to an interaction immediately preceding the split.
This is now a ``dynamic'' graph; or, if we only measure relative to the time of separation, at the very least a ``weighted'' one.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/00-intro_files/figure-latex/-content-codefigs-graphs-fig-karate-club-output-1.pdf}}

}

\caption{\label{fig-karate-club}Zachary's Karate Club, with ambiguously extant edge 78 highlighted.}

\end{figure}%

This observation raises an interesting metrology problem: \emph{We do not know if any of these are true}.
``Metrology'' is not limited to physical units, like ``meters'' and ``grams'', but more generally is concerned with systematic quantification with uncertainty.
Units provide a natural framework to describe what a metrologist is usually after: not just ``how much'', but ``how \emph{accurately} and \emph{precisely} that much'', as well.
When we use ``metrology'' in the context of network analysis, we are specifically referring to the need to:

\begin{itemize}
\tightlist
\item
  Quantify a network
\item
  Consider the trueness of that quantification
\item
  Consider the precision of that quantification.
\end{itemize}

The difference between trueness and precision is a crucial, often overlooked distinction: how close a set of measurements are to a reference value vs.~how repeatable/reproducible a measurement is\autocite{Accuracytruenessprecision_ISO1994}.

The metrological questions we posed above are ones of \_trueness\_we have no way to tell if Zachary's network model is specified correctly, because the reference network ``type'' is under-defined and we have no networks to compare it with.
We simply have to take the network as a reference unto itself; it is a calibration artefact, much like a physical ``meter rod''.
However, even with an assumed perfect ``trueness'', precision is often an issue as well!
In fact, as illustrated in Figure~\ref{fig-karate-club}, we do not know if the network being described from the original edge data even has 77 or 78 edges, due to ambiguous reporting in the original work.
Lacking a precise definition of what the graph's components (\emph{i.e.}, it's edges) are, \emph{as measurable entities}, means we cannot estimate the accuracy of the graph, whether for trueness or precision.

\section{Indirect Network Measurement}\label{indirect-network-measurement}

While the karate club graph has unquantified edge uncertainty derived from ambiguous edge measurements, we are fortunate that we \emph{have edge measurements}.
Regardless of how the data was collected, it is de facto reported as a list of pairs, which lends itself to treatment as a reference artefact.
In many cases, we simply do not have such luxury.

Instead, edges are often measured \emph{indirectly}, and instead we are given lists of node co-occurrences.
Networks connecting movies as being ``similar'' might be derived from data that lists sets of movies watched by each user;
networks of disease spread pathways might be implied from patient infection records;
famously, we might build a network of collaboration strength between academic authors by mining datasets of the papers they co-author together.

Such networks are derived from what we will call \emph{node activation} data, \emph{i.e.}, records of what entities happened ``together'', whether contemporaneously, or in some other context or artifact.
For this class, \emph{precision} might be easy to assess, having oft-repeated activations.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/00-intro_files/figure-latex/-content-codefigs-graphs-fig-obs-set-output-1.pdf}}

}

\caption{\label{fig-obs-set}Observations as activation sets}

\end{figure}%

These networkx are naturally represented as ``bipartite'' networks, having separate entites for, say, ``papers'' and ``authors'', and connecting them with edges (paper 1 is ``connected'' to its authors E,H,C, etc.).
But analysts are typically seeking the collaboration network connecting authors (or papers) themselves!
Networks of relationships in this situation are not directly observed, but which \emph{if recovered} could provide estimates for community structure, importances of individual authors (e.g.~as controlling flow of information), and the ``distances'' that separate authors from each other, in their respective domains. \autocite{Scientificcollaborationnetworks._Newman2001}
Common practice assumes that co-authorship in any paper is sufficient evidence of at least some level of social ``acquaintance'', where more papers shared means more ``connected''.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/00-intro_files/figure-latex/-content-codefigs-graphs-fig-collab-output-1.pdf}}

}

\subcaption{\label{fig-collab}Network based solely on co-authorship observations}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/00-intro_files/figure-latex/-content-codefigs-graphs-fig-colleague-output-1.pdf}}

}

\subcaption{\label{fig-colleague}graph of mutual collaboration relationships i.e.~the ``ground truth'' social network}

\end{minipage}%

\caption{\label{fig-toy}Co-authorship vs.~collaborator network}

\end{figure}%

Thus our social collaboration network in Figure~\ref{fig-collab} is borne out of indirect measurements: author connection is implied through ``occasions when co-authorship occurred''.
However, authors of papers may recall times that others were added, not by their choice, but by someone else already involved.
In fact, the final author list of most papers is reasonably a result of individuals choosing to invite others, not a unanimous, simultaneous decision by all members.
Let's imagine we wished to study the social network of collaboration more directly: if we had the luxury of being in situ as, say, a sociologist performing an academic ethnography, we might have been more strict with our definition of ``connection''.
If the goal is a meaningful social network reflecting the strength of interraction between colleagues, perhaps we prefer that our edges represent ``mutual willingness to collaborate''.
Edge ``measurement'', then, could involve records of events that show willingness to seek or participate in collaboration event, such as:

\begin{itemize}
\tightlist
\item
  \emph{Author (g) asked (e), (h), and (c) to co-author a paper, all of whom agreed}
\item
  \emph{(i) asked (f) and (j), but (j) wanted to add (b)'s expertise before writing one of the sections}
\item
  etc.
\end{itemize}

Each time two colleagues had an opportunity to work together \emph{and it was seized upon} we might conclude that evidence of their relationship strengthened.
With data like this, we could be more confident in claiming our collaboration network can serve as ``ground truth,'' as far as empirically confirmed collaborations go.
However, even if the underlying ``activations'' are identical, our new, directly measured graph looks very different.

Fundamentally, the network in Figure~\ref{fig-colleague} shows which relationships the authors \emph{depend} on to accomplish their publishing activity.
When causal relations between nodes are being modeled as edges, we call such a graph a \emph{dependency network}.
We will investigate this idea further later on, but ultimately, if a network of dependencies is desired (or implied, based on analysis needs), then the critical problem remaining is \emph{how do we recover dependency networks from node activations?}
What is missing, once again, is any sense of \emph{reference value} to base our assessment of \emph{trueness} on.
This thesis is primarily concerned with a metrological need within the network analysis community to have terms and techniques for describing and dealing with this problem.
What goes wrong when we use co-occurrence/activation data to estimate the dependency network?
What goes wrong when we wish to use co-occurrences for metrics like centrality and assortativity, or for exploratory analyses like building relationship type inventories?

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/00-intro_files/figure-latex/-content-codefigs-graphs-fig-recover-output-1.pdf}}

}

\caption{\label{fig-recover}Recovering underlying dependency networks from node-cooccurrences.}

\end{figure}%

Even more practically, networks created directly from bipartite-style data are notorious for quickly becoming far too dense for useful analysis, earning them the (not-so-)loving moniker ``hairballs''.
Network ``backboning,'' as it has come to be called tries to find a subset of edges in this hairball that still captures its core topology in a way that's easier to visualize.\autocite{twostagealgorithm_Slater2009,backbonebipartiteprojections_Neal2014}
Meanwhile, underlying networks of dependencies that \emph{cause} node activation patterns can provide this: they are almost always more sparse than their hairballs.
Accessing the dependency \emph{backbone} in a principled way is difficult, but doing so in a rapid, scalable manner is critical for practitioners to be able to make use of it to trim their hairballs.

\section{Scope of this work}\label{scope-of-this-work}

The purpose of this thesis is to provide a solid foundation for edge metrology when our data consists of binary node activations, by framing network analysis as a problem of \emph{inference}, as suggested by \textcite{Statisticalinferencelinks_Peel2022}.
We give special focus to binary activations that occur due to spreading processes, such as random walks or cascades on an underlying carrier graph.
Recovering the carrier, or, ``dependency'' network from node activations is of great interest to the network backboning and causal modeling communities, but often involves either unspoken sources of epistemic and aleatory error, or high computation costs (or both).
To begin addressing these issues, Part I of this thesis presents a guide and review of current practices, some of their pitfalls, and how common statistical tools apply to the network recovery problem: a \emph{Practitioner's Guide to Network Recovery}.
We will cover what ``measurement'' means in our context, and specifically the ways we encode observations, operations, and uncertainties numerically.
Clarifying what different versions of what ``relation'' means (whether proximity or incidence) is critical, since network structure is intended to encode such relations as mathematical objects (despite common ambiguities and confusion around what practitioners intend on communicating through them).
Then we organize a literature review to present a cohesive framework for assessing network recovery techniques, based on the assumptions and compromises being made to make the network inference problem tractable.

Next, building on a gap found in the first part, Part II presents a novel method, \emph{Forest Pursuit}, to extract dependency networks when we know a \emph{spreading process} causes node activation (e.g.~paper co-authorship caused by collaboration requests).
We formalize a generic foundation for representing inferred networks as unions of observed subgraphs (Chapter~\ref{sec-desirepath}), which we term \emph{Desire Path Density} estimates.
A closed form for Desire Path tree densities leads to the \emph{Forest Pursuit} algorithm, (Chapter~\ref{sec-fp}) which scales linearly with the size of active-node sets, is trivially parallelizable and streamable over dataset size, and is agnostic to network size.
We create a new reference dataset to enable community benchmarking of network recovery techniques, and use it show greatly improved accuracy over many other widely-used methods.
We then extend Forest Pursuit (Chapter~\ref{sec-extend}) as a Bayesian probabilistic model, for which we present an Expectation Maximization scheme for posterior estimation.
This improves upon the accuracy results of Forest Pursuit, at the cost of some speed and scalability, giving analysts multiple options to adapt to their needs.

Last, in Part III we apply Forest Pursuit to several qualitative case-studies.
We reconstruct scientific collaboration dataset to re-assess properties of the inferred network, and again with the classic \emph{Les Miserables} character co-occurrences (Chapter~\ref{sec-qual}).
Then, we investigate network dependency recovery when partial-order information for co-occurrences are available, such as with text analysis (Chapter~\ref{sec-ordered}), and test Forest Pursuit on a classic verbal fluency ``animals'' network recovery problem.
Finally, we discuss more broadly the future needs of network recovery with Forest Pursuit (Chapter~\ref{sec-conclusion}), specifically in the context of human-in-the-loop relationship annotation, hyperbolic graph embeddings, and gradient-based machine learning toolkits.

\subsection{Overview}\label{overview}

In summary, the remainder of this thesis provides the following:

\begin{itemize}
\tightlist
\item
  Chapter~\ref{sec-mat-met} and Chapter~\ref{sec-vec} defines common operations on binary observations, and builds a unified representation of both dependencies and co-occurrences as incidence structures and vectors.
\item
  Chapter~\ref{sec-lit-review} reviews current literature, and organizes it into a useful framework for needs assessment and future work.
\item
  Chapter~\ref{sec-desirepath} generalizes co-occurrence estimation to incorporate a priori domain information as Desire Path Density estimates of networks.\\
\item
  Chapter~\ref{sec-fp} presents a new scalable algorithm for dependency recovery and validates it on a new testbench for network recovery problems
\item
  Chapter~\ref{sec-extend} builds on Forest Pursuit to improve its performance on certain metrics, and re-formulates it as a probabilistic model.
\item
  Chapter~\ref{sec-qual} and Chapter~\ref{sec-ordered} present case studies for applying forest pursuit to network analysis problems without available ground-truth networks.
\end{itemize}

\part{A Practitioner's Guide to Network Recovery}

\chapter{Metrology with matrices}\label{sec-mat-met}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``Matrices act. They don't just sit there.''}

\hfill -- Gilbert Strang\\
\end{quote}

\doublespacing

\end{minipage}

\end{flushright}

Where metrology is concerned, the actual unit of observation and how it is encoded for us is critical to how analysts may proceed with quantifying, modeling, and measuring uncertainty around observed phenomena.
Experiment and observation tends to be organized as inputs and outputs, or, independent variables and dependent variables, specifically.
Independent variables are observed, multiple times (``observations''), and changes in outcome for each can be compared to the varying values associated with the independent variable input (``features'').
For generality, say a practitioner records their measurements as scalar values, \emph{i.e.}, \(x\in\mathbb{S}\in\{\mathbb{R,Z,N},\cdots\}\).
The structure most often used to record scalar values of \(n\) independent/input variable features over the course of \(m\) observations is called a design matrix \(X:\mathbb{S}^{m\times n}\).\footnote{
  Not all observations are scalar, but they can become so.
  If individual measurements are higher-dimensional (\emph{e.g.}, images are 2D), X is a tensor, which can be transformed through unrolling or embedding into a lower dimensional representation before proceeding.
  There are other techniques for dealing with \emph{e.g.}, categorical data, such as one-hot encoding (where the features are binary for each possible category, with boolean entries for each observation).}

\section{Observation and feature ``spaces''}\label{sec-matrix-notation}

If we index a set of observations and features, respectively, as
\[ i\in I=\{1,\cdots,m\}, \quad j\in J=\{1,\cdots,n\},\qquad I,J:\mathbb{N}\]
then the design matrix can map the index of an observation and a feature to the corresponding measurement.
\begin{equation}\phantomsection\label{eq-design-mat}{
x=X(i,j)\qquad X : I\times J \rightarrow \mathbb{S}
}\end{equation}
\emph{i.e.}, the measured value of the \(j\)th independent variable from the \(i\)th observation.\footnote{
  This notation is adapted from the sparse linear algebraic treatment of graphs in \textcite{GraphAlgorithmsLanguage_Kepner2011} and \textcite{MathematicalfoundationsGraphBLAS_Kepner2016}.}
In this scheme, an ``observation'' is a single row vector of features in \(\mathbb{S}^{n\times 1}\) (or simply \(\mathbb{S}^{n}\)), such that each observation encodes a position in the space defined by the features, \emph{i.e.}, the \emph{feature space}, and extracting a specific observation vector \(i\) from the entire matrix can be denoted as
\[\mathbf{x}_i=X(i,\cdot),\quad \mathbf{x}:J\rightarrow\mathbb{S}\]
Similarly, every ``feature'' is associated with a single column vector in \(\mathbb{S}^{1\times m}\), which can likewise be interpreted as a position in the space of observations (the \emph{data space}):
\[\mathbf{x}_j'=X(\cdot,j),\quad \mathbf{x}':I\rightarrow\mathbb{S}\]
Note that this definition could be swapped without loss of generality.
In other words, \(\mathbf{x}\) and \(\mathbf{x}'\) being in row and column spaces is somewhat arbitrary, having more to do with the logistics of experiment design and data collection.
We could have measured our feature vectors one-at-a-time, measuring their values over an entire ``population'', in effect treating that as the independent variable set.\footnote{
  In fact, vectors are often said to be in the column-space of a matrix, especially when using them as transformations in physics or deep learning layers.
  We generally follow a one-observation-per-row rule, unless otherwise stated.}

To illustrate this formalism in a relevant domain, let's take another look at co-citation networks.
For \(m\) papers we might be aware of \(n\) total authors.
For a given paper, we are able to see which authors are involved, and we say those authors ``activated'' for that paper.
It makes sense that our observations are individual papers, while the features might be the set of possible authors.
However, we are not given information about which author was invited by which other one, or when each author signed on.
In other words, the measured values are strictly boolean, and we can structure our dataset as a design matrix \(X:\mathbb{B}^{m\times n}\).
We can then think of the \(i^{\mathrm{th}}\) paper as being represented by a vector \(\mathbf{x}_i:\mathbb{B}^n\), and proceed using it in our various statistical models.
If we desired to analyze the set of authors, say, in order to determine their relative neighborhoods or latent author communities, we could equally use the feature vectors for each paper, this time represented in a vector \(\mathbf{x}'_j:\mathbb{B}^{1\times m}\).

\section{Models \& linear operators}\label{sec-lin-ops}

Another powerful tool an analyst has is \emph{modeling} the observation process.
This is relevant when the observed data is hypothesized to be generated by a process we can represent mathematically, but we do not know the parameter values to best represent the observations (or the observations are ``noisy'' and we want to find a ``best'' parameters that account for this noise).
This is applicable to much of scientific inquiry, though one common use-case is the de-blurring of observed images (or de-noising of signals), since we might have a model for how blurring ``operated'' on the original image to give us the blurred one.
We call this ``blurring'' a \emph{linear operator} if it can be represented as a matrix\footnote{in the finite-dimensional case}, and applying it to a model with \(l\) parameters is called the \emph{forward map}:
\[\mathbf{x} = F\mathbf{p}\qquad F:\mathbb{R}^{l}\rightarrow \mathbb{R}^n\]
where \(P\) is the space of possible parameter vectors, \emph{i.e.}, the \emph{model space}.
The forward map takes a modeled vector and predicts a location in data space.

Of critical importance, then, is our ability to recover some model parameters from our observed data, \emph{e.g.}, if our images were blurred through convolution with a blurring kernel, then we are interested in \emph{deconvolution}.
If \(F\) is invertible, the most direct solution might be to apply the operator to the data, as the \emph{adjoint map}:
\[ \mathbf{p} = F^H\mathbf{x}\qquad F^H:\mathbb{R}^{n}\rightarrow \mathbb{R}^l\]
which removes the effect of \(F\) from the data \(\mathbf{x}\) to recover the desired model \(\mathbf{p}\).

Trivially we might have an orthogonal matrix \(F\), so \(F^H=F^{-1}\) is available directly.
In practice, other approaches are used to minimize the \emph{residual}: \(\hat{\mathbf{p}}^=\min_{\mathbf{p}} F\mathbf{p}-\mathbf{x}\).
Setting the gradient to 0 yields the normal equation, such that
\[ \hat{\mathbf{p}}=(F^TF)^{-1}F^T\mathbf{x}\]
This should be familiar to readers as equivalent to solving ordinary least-squares (OLS).
However, in that case it is more often shown as having the \emph{design matrix} \(X\) in place of the operator \(F\).

\emph{This is a critical distinction to make:} OLS as a ``supervised'' learning method treats some of the observed data we represented as a design matrix previously as a target to be modeled, \(y=X(\cdot,j)\), and the rest maps parameters into data space, \(F=X(\cdot,J/j)\).
With this paradigm, only the target is being ``modeled'' and the rest of the data is used to create the operator.
In the citation network example, it would be equivalent to trying to predict one variable, like citation count or a specific author's participation in every paper, \emph{given} every other author's participation in them.

For simplicity, most work in the supervised setting treats the reduced data matrix as X, opting to treat \(y\) as a separate \emph{dependent variable}.
However, our setting will remain \emph{unsupervised}, since no single target variable is of specific interest---all observations are ``data''.
In this, we more closely align with the deconvolution literature, such that we are seeking a model and an operation on it that will produce the observed behavior in an ``optimal'' way.

\section{Measurement quantification \& error}\label{sec-smooth-err}

In binary data, such as what we have been considering, it is common to model observables as so-called ``Bernoulli trials'': events with two possible outcomes (on, off; yes, no; true, false), and one outcome has probability \(p\).
These can be thought of as weighted coin-flips: ``heads'' with probability \(p\), and ``tails'' \(1-p\).
If \(k\) trials are performed (the ``exposure''), we say the number of successes \(s\) (the ``count'') is distributed as a binomial distribution \(s\sim Bin(p,k)\).
The empirical estimate for the success probability is \(\hat{p}=\tfrac{s}{k}\).

Note that this naturally resembles marginal sums on our design matrix \(X\), if we treat columns (or rows!) as an array of samples from independent Bernoulli trials: \(\hat{p}_j = \frac{\sum_{i\in I} X(i,j)}{m}\).
Many probability estimates involving repeated measurements of binary variables (not simply the row/column variables) have this sort of \(\frac{\textrm{count}}{\textrm{exposure}}\) structure, as will become useful in later sections.

However, if we are ``measuring'' a probability, we run into issues when we need to quantify our uncertainty about it.
For instance, an event might be quite rare, but if in our specific sample we \emph{never} see it, we still do not generally accept a probability of zero.

\subsection{Additive Smoothing}\label{sec-counting}

One approach to dealing with this involves adding \emph{pseudocounts} that smooth out our estimates for count/exposure, from which we get the name ``smoothing'' \autocite[45-53]{SpeechLanguageProcessing_Jurafsky2025}.
\[\hat{p} = \frac{s+\alpha}{k+2\alpha} \]
Adding 1 success and 1 failure (\(\alpha=1\)) as pseudocounts to our observations is called \emph{Laplace's Rule of Succession}, or simply ``Laplace smoothing,''\footnote{
  derived when Laplace desired estimates of probability for unobserved phenomena, such as the sun (not) rising tomorrow.} while adding \(\alpha=0.5\) successes and failures is called using \emph{Jeffrey's Prior}.
It's so-called because this pseudocount turns out to be a special case of selecting a Beta prior on the Bernoulli probability \(p\sim \textrm{Beta}(\alpha, \beta)\), such that the posterior distribution for \(p\) after our observations is \(\textrm{Beta}(s+\alpha, k-s+\beta)\), which has the mean:
\begin{equation}\phantomsection\label{eq-beta-binomial}{
E[p|s,k]=\frac{s+\alpha}{k-\alpha+\beta}
}\end{equation}

This exactly recovers additive smoothing with a Jeffrey's prior for \(\alpha=\beta=0.5\).\footnote{
  A useful comparison of the two priors (1, 0.5) is to ask, given all of the trials we have seen so far, whether we believe we are near the ``end'' or ``middle'' of an average run of trials.
  For \(\alpha=1\), we believe nearly all evidence has been collected, but for \(\alpha=0.5\), only half of expected evidence has been observed.\\
}
This generalization allows us to be more flexible, and specify our prior expectations on counts or exposure with more precision.
Such models provide both an estimate of the aleatory uncertainty (via the posterior distribution), and a form of ``shrinkage'' that prevents sampling noise from unduly affecting parameter estimates (via the prior distribution).
Despite being a simple foundation, this treatment of ``counts'' and ``exposure'' can be built upon in many ways.

\subsection{Conditional Probabilities \& Contingencies}\label{conditional-probabilities-contingencies}

In dependency/structure recovery, since our goal involves estimating (at least) pairwise relationships, the independence assumption required to estimate node occurrences as Beta-Binomial is clearly violated.\footnote{
  In fact, a recent method from \autocite{MultivariateBernoullidistribution_Dai2013} models probabilistic binary observations, \emph{with dependencies}, by generalizing the mechanics overviewed here to a fully multivariate Bernoulli distribution, capable of including 3rd- and higher-order interactions (not just pairwise).\\
}

However, it's common to estimate how similar two random variables \(A,B\) are, \emph{e.g.}, if samples of each correspond to columns of binary \(X\).
For instance, the joint probabilities \(P(A\cap B)\) answer ``how often does A happen with B, out of all data?''
Conditional probabilities \(P(A|B)=\frac{P(A\cap B)}{P(B)}\) measure how often A occurs given B happened.
Once again, we can estimate the base probabilities \(P(A)\) and \(P(B)\) with methods like Equation~\ref{eq-beta-binomial} for each marginal sums \(X(\cdot,A)\) or \(X(\cdot,B)\), but the joint and conditional probabilities can instead be estimated using matrix multiplication using the Gram matrix, discussed below.
It encodes pair-wise co-occurrence counts, such that \(G(i,i'):\mathbb{Z}^{n\times n}\) has the co-occurrence count for node \(i\) with \(i'\).

For binary data, we typically create association meatures from values on a \(2\times2\) contingency table, with counts and marginals.
A shorthand notation for these values is:

\begin{equation}\phantomsection\label{eq-contingency}{
\begin{array}{c|cc|c}
      & B=1         & B=0         & \sum_B \\
\hline 
A=1   & p_{11}      & p_{10}      & p_{1\bullet} \\
A=0   & p_{01}      & p_{00}      & p_{0\bullet} \\
\hline 
\sum_A   & p_{\bullet 1} & p_{\bullet 0} & p_\bullet \\
\end{array}
}\end{equation}

The co-occurrence probability \(p_{11}=P(A\cap B)\) for each pair can also be approximated with the beta-binomial scheme mentioned above, but care must be taken not to confuse this with the edge strength connecting two nodes.
First, nodes that rarely activate (low node probability) may nonetheless reliably connect to others when they do occur (high edge probability).
In fact, without direct observation of edges, we are not able to estimate their count, \emph{or} their exposure, which can be a source of systemic error from \emph{epistemic uncertainty}.
We don't know when edges are used, directly, and we also don't have a reliable way to estimate the opportunities each edge had to activate (their exposure), either.
This is especially true when we wish to know whether an edge even \emph{can} be traversed, \emph{i.e.}, the edge \emph{support}.
Support, as used in this sense, is the set of inputs for which we expect a non-zero output.
Intuitively, this idea captures the sense that we might care more about \emph{whether} an edge/dependency exists, not \emph{how important} it is.
For that, we have to re-assess our simple model: even if we could count the number of times an edge might have been traversed, how do we estimate the opportunities it had to be available for traversal (it's ``exposure'')?

Assuming this kind of epistemic uncertainty can be adequately addressed through modeling---attempts at which will be discussed in more detail in Chapter~\ref{sec-lit-review}---conditional probability/contingency tables will again be useful for validation.
When comparing estimated edge probability to some known ``true'' edge existence (if we have that), we can count the number of correct predictions, as well as type I (false positive) and type II (false negative) errors.
We can do this at \emph{every probability/weight threshold value}, as well, and we will return to ways to aggregate all of these values into useful scoring metrics in Section~\ref{sec-FP-experiments}.

\section{Distance vs.~Incidence}\label{sec-products}

As we have already seen, operations from linear algebra make many counting and combinatoric tasks easier, while unifying disparate concepts to a common set of mechanics.
In addition to having a map from integer indices to sets of interest, these design matrices/vectors are implicitly assumed to have entries that exist in a field \(F=(\mathbb{S},\oplus,\otimes)\).
equipped with operators analogous to addition (\(\oplus\)) and multiplication (\(\otimes\)).\footnote{
  Or, more generally, a semiring if inverse operations for \(\oplus,\otimes\) don't exist.}
With this, we are able to define generalized inner products that take pairs vectors in a vector space \(\mathbf{x}\in V\), such that \(\langle\cdot,\cdot\rangle_F:\mathbb{S}^n\times \mathbb{S}^n\rightarrow \mathbb{S}\).
\[
\langle\mathbf{x}_a,\mathbf{x}_b\rangle_{F} = \bigoplus_{j=1}^n \mathbf{x}_a(j)\otimes\mathbf{x}_b(j)
\]

We can use this to perform ``contractions'' along any matching dimensions of matrices as well, since the sum index is well-defined.
\[
\begin{aligned}
X\in\mathbb{S}^{m\times n}\quad Y\in\mathbb{S}^{n\times m} \\
Z(i,j)=X\oplus,\otimes Y = \bigoplus_{j=1}^{n} X(i,j) \otimes Y(j,k) = XY
\end{aligned}
\]
For ease-of-use, we will assume the standard field for any given set \((\mathbb{S},+,\times)\) if not specified otherwise, which recovers standard inner products \(\langle\cdot,\cdot\rangle\).
Still, the idea that matrix products can be generalized will allow us to ``stay'' in the correct (binary) domain under contraction, which will be an issue dealt with again in Section~\ref{sec-fp-problem}.
It might be of interest to a reader to refer to an entirely linear-algebraic formulation of graph theory in \textcite{MathematicalfoundationsGraphBLAS_Kepner2016}, which also illustrates the usefulness of various fields (or semirings).
They allow linear-algebraic representation of many graph operations, such as shortest paths through inner products over \((\mathbb{R}\cup -\infty,\textrm{min}, +)\).
This works because discrete/boolean edge weights will not accumulate extra strength beyond 1 under contraction over observations.

\subsection{Kernels \& distances}\label{kernels-distances}

As alluded to in the previous section, co-occurrence have a deep connection to a Gram matrix, which is a matrix of all pairwise inner products over a set of vectors.

\begin{equation}\phantomsection\label{eq-gram-mat}{
X^TX=G(j,j')=\langle\mathbf{x'}_j,\mathbf{x}_{j'}'\rangle = \sum_{i=1}^{m} X(i,j)X(i,j')
}\end{equation}

Matrices that can be decomposed into another matrix and its transpose are symmetric, and positive semidefinite (PSD), making every gram matrix PSD.
They are directly related to (squared) euclidean distances through the polarization identity:\footnote{\textbf{Important}: these definitions are all using the \(\mathbf{x}'\) notation to indicate that these measurements are almost exclusively being done in the \emph{data space}, \emph{i.e.}, on column vectors.
  While most definitions work on distances in terms of the measurements/objects/data, for \emph{inverse problems} (like network recovery, structure learning, etc.) they are more often applied in terms of the features (here, the nodes.
  This can be seen in statistics as well, where covariance and correlation matrices (which are related to the gram and distance matrix definitions above), are defined as relationships between features/dimensions, not individual samples.}
\begin{equation}\phantomsection\label{eq-sq-distance}{
d^2(j,j') = \|\mathbf{x}_{j'}'-\mathbf{x}_{j'}'\|^2 = G(j,j) - 2G(j,j') + G(j',j') 
}\end{equation}

In our example from before, the gram matrix will have entries showing the number of papers shared by two authors (or total papers by each, on the diagonal).
This is because an inner product between two author (column) vectors will add 1 for each paper in the sum only if it has both authors in common.
This is called a \emph{bipartite projection}\autocite{Bipartitenetworkprojection_Zhou2007,atlasaspiringnetwork_Coscia2021} into the authors ``mode'', and is illustrated visually in Figure~\ref{fig-collab}.

Due to \autocite{Metricspacespositive_Schoenberg1938}, we can generalize Equation~\ref{eq-sq-distance} such that \emph{any} function ``kernel'' function \(\kappa(x,y)\) that creates PSD matrix \(K(j,j')\in\mathbb{S}^{n\times n}\).
It says that such a PSD matrix can always be decomposed into a form \(K=R^TR\) for any matrix \(R(i,j)\in \mathbb{S}^{m\times n}\), thus letting us use the polarization identity to create arbitrary distance metrics. on \(\mathbb{S}^n\) \autocite{SimilaritiesgraphsKernels_Avrachenkov2019}\footnote{
  Distance metric, here, means that \(d(x,y)\) satisfies the triangle inequality for all \(x,y\).}
\begin{equation}\phantomsection\label{eq-dist-kernel}{
d_K(j,j') = \tfrac{1}{2}\left(K(j,j)+K(j',j'))\right)-K(j,j')
}\end{equation}

This ability to create valid distance measures from arbitrary kernel functions is the core of a vast area of machine learning and statistics that employs the so-called \emph{kernel trick}. \autocite{Patternrecognition_Theodoridis2010}
Different kernels yield different properties useful for distinguishing points having specific properties.
One class of kernels are normalized to the range \([0,1]\), such that we ensure that equality along any one dimension is given a weight of \(\tilde{K}(j,j)=1\).
Such a kernel matrix can be derived from any other kernel, and is often combined with a logarithmic similarity measure \(\tilde{\kappa}(x,y)=\log{s(x,y)}\).
\begin{equation}\phantomsection\label{eq-norm-diag}{
\begin{aligned}
\tilde{K}(j,j') &= \frac{K(j,j')}{\sqrt{K(j,j)^2K(j',j')^2}}\\
d_{\tilde{K}}(j,j') &= -\log{\tilde{K}(j,j')}
\end{aligned}
}\end{equation}
Since this is equivalent to applying Equation~\ref{eq-dist-kernel} to \(s\) directly.
This normalization should be familiar as the way cosine similarities and correlation matrices are made as well (also having 1s along their diagonal), and illustrates how non-metric similarities can be potentially made into (pseudo)metrics.

\subsection{Incidence structures \& dependency}\label{incidence-structures-dependency}

Rather than how ``close'' or ``far'' to points are in vector space, which is described with the kernels and distances above, whether something ``touches''---or, is incident to---something else is usually described abstractly as an \emph{incidence structure}.
This is an abstract way to describe how things ``touch'', such as when a set of points lie on a line/plane, or nodes touch an edge.
We say an incidence structure is a triple of sets called (for historical reasons) \emph{points} \(\mathfrak{P}\), \emph{lines} \(\mathfrak{L}\), and \emph{flags} \(\mathfrak{I}\).\autocite{Incidencegeometry_Moorhouse2007}
\[(\mathfrak{P,L,I})\quad \mathfrak{I}\subseteq \mathfrak{P}\times\mathfrak{L}\]

Representing these as matrices will be further explored in Section~\ref{sec-incidence-vec}.
But, the discrete nature of these incidence sets makes it clear that estimating the size and elements of \(\mathfrak{I}\), is a very different question from estimating the similarity/distance between two entities in a vector space.

In statistics, such discrete structures usually arise when we wish to distinguish direct dependence from indirect.
Take as an example a set of masses connected together by springs.
If we shake one mass, all masses will also shake some amount, depending on the spring constants of the springs each mass is ``transmitted'' force through, and the losses due to friction or air resistance.
While the amount of movement over time depends on how ``close'' in this spring network two masses are, the movement itself can only be transmitted through springs that the masses are \emph{incident} to.
Movement could be modeled through similarity/distance measurements like correlation, since none of the masses are independent (all move when any do), but incidence in terms of spring force transmission is modeled in terms of conditional (in)dependence.
If we hold all but two masses still, and moving one doesn't move the other, then we know they are conditionally independent: no spring connects them!

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part1/1-01-matrix-meas_files/figure-latex/-content-codefigs-graphs-fig-springs-output-1.pdf}}

}

\caption{\label{fig-springs}Spring system as a network of conditional dependencies}

\end{figure}%

This idea gets formalized as probabilistic graphical models, which are networks that define ``incidence'' between two variables as conditional dependence.
Letting the random variables in the column-space of \(X\) be \(A,B\) and the remaining columns be \(C=X(\cdot,J\setminus \{A,B\})\), then
\begin{equation}\phantomsection\label{eq-cond-indep}{
P(A\cap B |C ) = P(A|C)P(B|C)\implies (A\perp B|C)\implies (A,B)\notin \mathfrak{I}
}\end{equation}
for a set of incidences \(\mathfrak{I}\) defining a PGM that was sampled as \(X\).

\chapter{Vector representations of incidence}\label{sec-vec}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``By stripping away all the extra baggage of distance, length, angle, continuity, betweenness, etc. and retaining only the notion of incidence, we find that what remains is still quite fascinating and highly nontrivial.''}

\hfill -- G. Eric Moorehouse\\
\doublespacing 
\end{quote}

\end{minipage}

\end{flushright}

To provide a sufficiently rigorous foundation for network recovery from binary occurrence, we will need a rigorous way to represent networks and occurrences that lends itself to building structured ways both connect to each other.
We build on the incidence structure and matrix product formalism from the previous chapter, introducing various ways to build graphs as incidence structures that have direct representations as matrices.
This can be extended to representing occurrences as matrices of \emph{hyperedge vectors}.
This view allows us to interpret different representations of graphs (or hypergraphs) as connected by simple matrix operations.

Traditionally\autocite{MathematicalfoundationsGraphBLAS_Kepner2016,WhyHowWhen_Torres2021}, we might say a graph on nodes (or, ``vertices'') \(v\in V=\{1,\cdots,n\}\) and ``edges'' \(E\) is a tuple:
\[
G=(V,E) \quad \textrm{s.t.} \quad E\subseteq V \times V
\]

In incidence geometry terms, this would be similar to making two duplicate sets of the same nodes, and defining a graph as the set of incidences between nodes.
The \emph{adjacency matrix} \(A\) of \(G\), degree matrix \(D\), and graph/discrete Laplacian \(L\) are then defined as:\footnote{
  The \emph{indicator function} \(\mathbf{1}_A(x)\) is 1 for values of \(x\) in the set \(A\), and 0 otherwise.}
\[
\begin{aligned}
A(u,v) & =\mathbf{1}_E((u,v)) \quad &A : V\times V\rightarrow \mathbb{B} \\
D(u,v) & =\mathrm{diag}({\textstyle\sum}_V A(u,v))\quad &D : V\times V\rightarrow \mathbb{N} \\
L(u,v) & = D(u,v) - A(u,v) \quad &L : V\times V\rightarrow \mathbb{Z} 
\end{aligned}
\]

However, if edges and their recovery is so important to us, defining them explicitly as nodes-node incidences can be problematic when we wish to estimate edge existence (or not), given noisy pairs of node co-occurrences.
Additionally, we have to be very careful to distinguish whether our graph is \emph{(un)directed}, \emph{weighted}, \emph{simple}, etc., and hope that the edge set has been filtered to a subset of \(N\times N\) for each case.
Instead, we propose a less ambiguous framework for vectorizing graphs, based on their underlying incidence structure.

\section{Graphs as incidence structures}\label{sec-incidence-vec}

Instead, we give edges their own set of identifiers, \(e\in E=\{1,\cdots \omega\}\).
Now, define graphs as incidence structures between nodes and edges such that every edge is incident to either two nodes, or none:

\begin{equation}\phantomsection\label{eq-simple-incidence}{
G = (V,E,\mathcal{I}) \quad s.t. \quad \mathcal{I} \subseteq E\times V
}\end{equation}

Variations on graphs can often be conveniently defined as constraints on \(\mathcal{I}\):

\begin{itemize}
\tightlist
\item
  Self loops can be prohibited by only allowing unique flags for a given relation\footnote{
    never two flags with the same pair, \emph{i.e.}, \(\mathcal{I}\) is a set, not a multiset.}
\item
  Multigraphs are similarly described by whether we allow pairs of vertices to appear with more than one edge\footnote{
    in the set of flags containing nodes \(u\) or \(v\), only one \(e\) may be incident to both of them.}
\end{itemize}

Together, these constraints define ``simple'' graphs.
Similarly, we can equip Equation~\ref{eq-simple-incidence} with a function \(B\) that allows \(\mathcal{I}\) to encode information about the specific kinds of incidence relations under discussion.
We give \(B\) a range of possible flag values \(S\):

\begin{equation}\phantomsection\label{eq-map-incidence}{
G = (V,E,\mathcal{I},B) \quad s.t. \quad \mathcal{I} \subseteq E\times V\quad B:\mathcal{I}\rightarrow S
}\end{equation}

\begin{itemize}
\tightlist
\item
  Undirected, unweighted graphs only need single elements: ``incidence exists'' \emph{i.e.}, \(S=\{1\}\)
\item
  Directed graphs can use two elements \emph{e.g.}, a ``spin'' for \(S=\{-1,1\}\)
\item
  Weighted, undirected graphs are supported on positive scalars \emph{e.g.}, \(S=\mathbb{R}^+\)
\item
  Weighted, directed graphs are supported on any scalar \emph{e.g.}, \(S=\mathbb{R}_{\neq0}\)
\end{itemize}

If the ``absence'' of incidence needs to be modeled explicitly, a ``null'' stand-in (0,False) can be added to each \(S\), which is useful for representing each structure as arrays for use with linear algebra (\emph{i.e.}, \(\{0,1\}\),\(\{-1,0,-1\}\),\(\mathbb{R}^+_0\), and \(\mathbb{R}\), respectively).
By doing so, we can also place an exact limit on the maximum possible size of \(\omega=\|E\|\) in the simple graph case, and indicate edges by their unique ID, such that \(\mathcal{I}= E\times V\) is no longer a subset relation for \(E=\{1,\cdots,{n\choose2} \}\).
Instead of existence in \(\mathcal{I}\), we explicitly use incidence relation \(S\) to tell us whether each possible edge ``exists'' or not, simplifying our graph definition further\footnote{
  if we allomulti-edges
  , then}:

\begin{equation}\phantomsection\label{eq-incidence-graph}{
\begin{gathered}
G  = (V,E,B) \quad s.t. \quad B : E\times V \rightarrow S\\
v \in V = \{1,\cdots, n\}\quad \\
e \in E = \left\{1,\cdots, {n\choose 2} \right\}
\end{gathered}
}\end{equation}

The representation of \(B\) in Equation~\ref{eq-incidence-graph} bears a remarkable similarity to our original description of design matrices in Equation~\ref{eq-design-mat}.
In fact, as a matrix, \(B(e,v)\) is called the \emph{incidence} matrix: every row has two non-zero entries, with every column containing a number of non-zero entries equal to that corresponding node's degree in \(G\).
Traditionally, we use an \emph{oriented} incidence matrix, such that each row has exactly one positive (non-zero) value, and one negative (non-zero) value.\footnote{
  In fact, this would make B\^{}*(v,e) equivalent to a \emph{graphical matroid}, another common formalism that generalizes graph-like structures to vector space representations.}
Even for undirected graphs, the selection of \emph{which entry} is positive or negative is left to be ambiguous, since much of the math used later is symmetric w.r.t. direction\footnote{though not always!}.

\subsection{Embedding incidences in vector space}\label{embedding-incidences-in-vector-space}

A formalism for graphs that starts with incidence matrices would benefit from a \emph{canonical} oriented incidence matrix, rather than the family that is ambiguous w.r.t. edge orientation.
To start, we can be more precise by selecting each row(edge) vector, and partitioning it into two: one for each non-zero column (node) that edge is incident to.
Every incidence can be represented individually as standard basis vector \(\mathbf{e}\) in the feature space of \(B\), scaled by the corresponding value of \(S\).

Let \(V_e\) be the set of nodes with (non-zero) incidence to edge \(e\).
Then the incidence vectors are:\\
\begin{equation}\phantomsection\label{eq-incidence-vec}{
\delta_e(v) = B(e,v)\mathbf{e}_v \quad \forall v\in V_e
}\end{equation}
And the (unoriented) incidence matrix vectors are recovered as sums over the incidence vectors for each edge:
\begin{equation}\phantomsection\label{eq-incidence-edge-sum}{
\mathbf{b}_e = \sum_{v\in V_e} \delta_e(v)
}\end{equation}

A traditional approach might then define undirected graphs as equivalent, in some sense, to multidigraphs, where each edge is really two directed edges, in opposing directions.
This does allow the matrix \(B\) to have the correct range for its entries in this formalism (the directed graph range, \(S=\{-1,0,1\}\)), and the edge identity based on sums would hold.
However, the resulting set of incidences would have twice the number of edges than our combinatoric limit for simple graphs, and prevent the more elegant definition of graph types through the set \(\mathbf{S}\).
Plus, it would necessitate averaging of weights over different edge ID's to arrive at a single undirected ``edge weight'', and many other implementation details that make keeping track of specifics difficult for practitioners.

Instead, we would like a canonical oriented distance matrix, which can be derived from the vectorized incidences in the undirected range of \(B\) (the standard basis vectors).
Without loss of generality, let \(u_e,v_e\in V_e\) be nodes such that \(u<v\).\footnote{the inequality is strict because self-loops are not allowed.}
Using this, we can unambiguously define a \emph{partition} \(B(e,\cdot)=B(e,u_e) + B(e,v_e)\) between incidences on \(e\), along with a new derived incidence, \(B_o\), which has oriented rows like:
\[B_o(e,\cdot)=\mathbf{b}^o_e = \delta_e(u)-\delta_e(v)\]
In other words, while the unoriented incidence matrix is the ``foundational'' representation for graphs in our formalism, the (canonical) oriented one can be derived, even if negative incidence values are not in \(\mathbb{S}\).\footnote{
  This works as long as we are in at least a ring, since semirings in general do not need to define additive inverse operations.
  In this case we would limit ourselves to the oriented incidence.}

\begin{figure}

\centering{

\centering{

\[
\small
B^{\omega \times n} =
\begin{array}{c c}
& \begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\ \end{array} \\
\begin{array}{c c } e_1\\e_2\\e_3\\e_4\\e_5\\e_6\\e_7\\e_8\\e_9\\e_{10}\\e_{11}\\ \vdots \\e_\omega\\\end{array} &
\left[
\begin{array}{cccccccccc}
  1  &  .  &  .  &  .  &  .  &  1  &  .  &  .  &  .  &  . \\
  1  &  .  &  .  &  .  &  1  &  .  &  .  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  1  &  1  &  .  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  .  &  1  &  .  &  .  &  1  &  . \\
  .  &  .  &  .  &  .  &  1  &  .  &  1  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  1  &  .  &  .  &  1  &  .  &  . \\
  .  &  1  &  .  &  .  &  .  &  .  &  .  &  .  &  .  &  1 \\
  .  &  .  &  .  &  .  &  .  &  .  &  .  &  .  &  1  &  1 \\
  .  &  .  &  1  &  .  &  .  &  .  &  1  &  .  &  .  &  . \\
  .  &  .  &  .  &  .  &  .  &  .  &  1  &  1  &  .  &  . \\
  .  &  .  &  .  &  1  &  .  &  .  &  .  &  1  &  .  &  . \\
  &&&& \vdots &&&&&
  \\
  &&&& \vdots &&&&&
   \end{array}
\right]
\end{array}
\]

}

\subcaption{\label{fig-incidence-mat}: Network \(G\) as an \emph{incidence matrix} \(B\).}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part1/1-02-graph-obs_files/figure-latex/-content-codefigs-graphs-fig-colleague-output-1.pdf}}

}

\subcaption{\label{fig-colleague}graph of mutual collaboration relationships i.e.~the ``ground truth'' social network}

}

\caption{\label{fig-incidence-graph}: Incidence matrix representation of a graph}

\end{figure}%

But, now that we have removed the information on ``which nodes an edge connects'' from our definition of edges (since every edge is a scalar ID), how do we construct \(V_e\) without a circular dependency on \(B\) to find non-zero entries?
Because of our unique identification of edges up to the combinatoric limit, we can still actually provide a unique ordering of the nodes in \(V_e\), without searching over the entirety of \(B\)'s domain.
Using an identity from \textcite{ParallelEuclideandistance_Angeletti2019}, we have a closed-form equation both to retrieve the IDs of nodes \(u,v\) (given an edge \(e\)), and an edge \(e\) (given two nodes \(u,v\)), for any simple graph with \(n\) vertices.
\begin{equation}\phantomsection\label{eq-sq-id}{
\begin{gathered}
    u_n(e) = n - 2 - \left\lfloor\frac{\sqrt{-8e + 4n(n - 1) - 7}-1}{2}\right\rfloor\\
    v_n(e) = e + u_n(e) + 1 -\frac{1}{2} \left(n (n - 1) + (n - u_n(e))^2 - n+u_n(e)\right)\\
    e_n(u,v) = \frac{1}{2}\left(n(n - 1) - ((n - u)^2 - n+u)\right) + v - u - 1
\end{gathered}
}\end{equation}
Our ease-of-calculation lets us drop some of the excess notation and refer to our (un)oriented incidence matrices in terms of the incidences of each edge on their \(u\) or \(v\), directly.
\[
B = B_u + B_e \qquad B_o \equiv B_u - B_v
\]

\subsection{\texorpdfstring{Inner products on \(B\)}{Inner products on B}}\label{inner-products-on-b}

With all of this background, the other representations of graphs can be seen as derivations from the canonical incidence matrices.
The Laplacian, which is usually introduced either in terms of ajacency/degree, or as the gram matrix for oriented edge vectors, is also related to the gram matrix between all pairs of incidences on \((u,v)\).
The other identities are simply consequences of the polarization identity:
\begin{equation}\phantomsection\label{eq-laplacian}{
\begin{split}
L & = B_o^TB_o\\
  & = \|B_u - B_v \|^2 \\
  & = 2\|B_u\|^2 +2\|B_v\|^2 - \|B_u + B_v \|^2 \\
  & = 2D - B^TB = D-A
\end{split}
}\end{equation}

The Laplacian is often used in defining random-walks and markov chains, such that the degree of each node should be normalized to 1, which can be accomplished either by row- or column-normalizing it: \(L^{\textrm{rw}}=D^{-1}L\) or \(LD^{-1}\), respectively.
If this degree-normalization is desired without de-symmetrizing \(L\), we can still perform an operation similar to normed kernels in Equation~\ref{eq-norm-diag}, called the symmetric normalized Laplacian.
\begin{equation}\phantomsection\label{eq-norm-laplacian}{
\tilde{L} = D^{-\tfrac{1}{2}}LD^{-\tfrac{1}{2}} = \frac{L(u,v)}{\sqrt{L(u,u)^2L(v,v)^2}}
}\end{equation}

\subsection{Edge Metrology, Edge Vectors}\label{edge-metrology-edge-vectors}

Implicitly in the use of \(B\) with design matrix mechanics from the previous chapter is a treatment of edges as ``observations'' (the data space), and nodes as features.
If an edge \emph{is} an observation, then unfortunately we cannot really quantify uncertainty over repeated measurements of edges with the simple mechanics from Section~\ref{sec-counting} (because that edge \emph{is} that corresponding observation, and IDs are not duplicated).

So far we have seen two ways of representing the entire graph in matrix form: Incidence matrix \(B\) (or \(B_o\)), and the inner-product matrices derived from it (\(L\), \(A\)).
Since we can recover node IDs from edge IDs by Equation~\ref{eq-sq-id}, we can use a single vector to represent an entire graph structure by it's edges alone.
Then a data dimension for vectors in ``edge space'' can once again represent observations, with nodes implied by Equation~\ref{eq-sq-id}.
This is either done by contracting \(B\) along the nodes (columns) dimension, or by \emph{unrolling} the upper triangle of \(L\) or \(A\) according to Equation~\ref{eq-sq-id}.\footnote{
  Equation~\ref{eq-edge-vectors} uses an averaging operation to accomplish the contraction, but any reduction over the two nodes shared by an edge would accomplish the same, especially since we rarely see separate values for edge weight per-node, the way incidences can.}
If each vector represents a value of \(B\) associated with a corresponding edge, then \(m\) of these vectors would be equivalent to \(m\) observations of \({n \choose 2}\) edges on the same set of \(n\) nodes.
Formally, for the \(i\)th observed structure on \(n\) nodes:
\begin{equation}\phantomsection\label{eq-edge-vectors}{
\begin{gathered}
R(i,e) = \min(\{B_i(e, u_n(e)),B_i(e,v_n(e))\}) \\
\quad \textrm{s.t.} \quad R:I\times E \rightarrow \mathbb{S}\\
i\in I = \{1,\cdots,m\} \qquad e \in E=\left\{1,\cdots,\omega\right\}\\
n=\tfrac{1}{2}(1+\sqrt{8\omega+1})
\end{gathered}
}\end{equation}
This representation formalizes what practitioners call ``edgelists'' into a data structure that can unambiguously distinguish directed, undirected, and weighted graphs.
In addition, it allows for repeated measurements of edges over the same set of nodes, while flexibly growing when new nodes arrive.\footnote{
  For instance, say observations are stored as sparse entries via \(R\), and a new node arrives.
  First, the participating nodes can be recovered in a vectorized manner through Equation~\ref{eq-sq-id}.
  Then, a new node id increases \(n\), followed by reassignment of the edge IDs with \(e_n(u,v)\).}
We are now able to encode the kinds of observations our hypothetical social scientist would be making of author collaboration interactions \emph{as vectors}, shown in Figure~\ref{fig-incidence-rep}.

\begin{figure}

\centering{

\centering{

\[
\small
%X(\{1,2,3,4,\cdots\})=\\
R^{n\times\omega}=
\begin{array}{c c}\small
 & \begin{array}{ccccccccccc} e_1 & e_2 & e_3 & e_4 & e_5 & e_6 & e_7 & e_8 & e_9 & e_{10} & e_{11}\\ \end{array} \\
\begin{array}{c c } x_1\\x_2\\ \vdots \end{array} &
\left[
\begin{array}{ccccccccccc}
 \  .\; &  \;. \; &  \;. \; &  \;. \; &  \;1 \; &  \;. \; &  \;. \; &  \;. \; &  \;1 \; &  \;1 \; &  . \ \\
 \  .\; &  \;. \; &  \;. \; &  \;1 \; &  \;. \; &  \;. \; &  \;1 \; &  \;1 \; &  \;. \; &  \;. \; &  . \ \\
  &&&& \vdots &&&&&
\end{array}
\right]
\end{array}
\]

}

\subcaption{\label{fig-edge-mat}: Observations embedded in ``edge space''}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part1/1-02-graph-obs_files/figure-latex/-content-codefigs-graphs-fig-socsci-output-1.pdf}}

}

\subcaption{\label{fig-socsci}(hypothetical) edge-based observations}

}

\caption{\label{fig-incidence-rep}: Possible edge-based embedding of observations.}

\end{figure}%

\section{Node activation, bipartite graphs, and hypergraphs}\label{node-activation-bipartite-graphs-and-hypergraphs}

What if an incidence structure allows for more than two incidences for the ``line'' set?
In our binary design matrix, we might consider each observation its own ``line'', such that it is incident to all the activated nodes.
This is no longer a graph of edges and nodes, but rather a more general object.

Every incidence structure can be seen as an incidence matrix, which additionally can be thought of as a \emph{bipartite} graph.
In this sense, the incidence matrix is thought of as a bi-adjacency matrix, which is a subset of a larger adjacency having two off-diagonal non-zero blocks

\[
A_{BP} = 
\begin{pmatrix}
0_{n,n} & X^T \\
X & 0_{m,m}
\end{pmatrix}
\]

The graph having this adjacency structure has two sets of nodes that do not intraconnect (ergo, ``bipartite'').
The resulting structure for our toy example is shown next to the incidence matrix in Figure~\ref{fig-incidence-struct}.

\begin{figure}

\begin{minipage}{0.50\linewidth}

\centering{

\[
\small
X^{m\times n}=
%X(\{1,2,3,4,\cdots\})=\\
\begin{array}{c c}
& \begin{array}{cccccccccc} a & b & c & d & e & f & g & h & i & j\\ \end{array} \\
\begin{array}{c c } x_1\\x_2\\x_3\\x_4\\ \vdots \\ x_m\end{array} &
\left[
\begin{array}{c c c c c c c c c c}
  .  & .   &  1  & .   &  1  & .   &  1  &  1  & .   & .  \\
  1  & .   & .   & .   &  1  &  1  & .   & .   & .   & .  \\
  .  &  1  & .   & .   & .   &  1  & .   & .   &  1  &  1 \\
  .  & .   & .   &  1  &  1  & .   & .   &  1  & .   & .  \\
  &&&& \vdots &&&&&
  \\
  &&&& \vdots &&&&&
\end{array}
\right]
\end{array}
\]

}

\subcaption{\label{fig-biadj-mat}: \(X\) as a (bi-adjacency) matrix.}

\end{minipage}%
%
\begin{minipage}{0.50\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part1/1-02-graph-obs_files/figure-latex/-content-codefigs-graphs-fig-bipartite-output-1.pdf}}

}

\subcaption{\label{fig-bipartite}Bipartite representation of node ``activation'' data}

\end{minipage}%

\caption{\label{fig-incidence-struct}: Bipartite representation of binary design matrix}

\end{figure}%

When the set of lines is a family of subsets of points, incidences on points and lines form a \emph{hypergraph}.
Hypergraphs are usually thought of as graphs where edges can connect more than two nodes, which again can be made into an incidence structure in the same way our graphs are.
This isomorphism lets many of the familiar ideas on graphs (\emph{e.g.}, walks, paths, laplacians, etc.) be reformulated in terms of hypergraphs.
For a more in-depth elaboration on algebraic graph theory on graphs, see \textcite{orientedhypergraphicapproach_Reff2012}.

\subsection{Inner product on Hyperedges}\label{inner-product-on-hyperedges}

Rather than go into similar detail about hypergraphs, we want to focus on the implication of performing inner product calculations with vectors of the hypergraph.
\(\mathbf{x}_i^T\mathbf{x}_{i'}\) will not necessarily result in a binary value, but a sum over all shared nodes in each edge.
As mentioned previously, the Gram matrix for \(X\) will count co-occurrences in the off-diagonals, with marginal counts on the diagonal.
This means that the relation between edge-vector entries and node-activations is not so straight forward: each entry has a ``magnitude'', so forcing a binary edge activation from the adjacency or laplacian forms (using Equation~\ref{eq-sq-id}) will necessarily lose information.
This information loss is one of the key precautions practitioners are advised to consider in \textcite{WhyHowWhen_Torres2021}, since transformation between any of the representations for complex systems will change the information encoded by them.

\subsection{Combining Occurrence \& Dependence}\label{combining-occurrence-dependence}

As an aside, there are several ways that known graph structures can be exploited to regularize distance or association measures for hypergraphic data.
Graphs of dependency information can provide useful kernels \emph{on the graph}.\footnote{
  Note that a ``kernel on a graph'' works by defining distances between weighted vectors of nodes.
  A ``graph kernel,'' on the other hand, describes a kernel that defines distances \emph{between different graphs}.
}
For an in-depth analysis of common kernels on graphs, including the now-famous PageRank, Heat, and Commute-time kernels, see \textcite{SimilaritiesgraphsKernels_Avrachenkov2019}.
In \textcite{GeneralizedEuclideanmeasure_Coscia2020} the ``commute time'' kernel is said to generate a ``generalized euclidean distance'', and using an adjacency matrix kernel before normalization by marginals gives what is known as ``soft cosine'' measures, which incorporate prior knowledge \emph{e.g.}, for semantic similarities between words.

Importantly, each of these methods assume that a network structure is already available.
What can often happen in practice, however, is the use of a filtered/thresholded version of the hypergraphic Gram matrix \emph{as the network}, because no a priori network is available.
This runs the risk of over-estimating the proximity of nodes in similar regions of node-space, reinforcing local correlations at the expense of long-distance path estimates.
A more formalized analysis of this problem, which we term ``clique bias,'' can be found in Section~\ref{sec-clique-bias}.

As covered in the introduction, finding the underlying dependency network from the hypergraphic data is the core concern of network recovery.
Reliably recovering this structure would then allow for less biased use of these graph kernels in real-life application.

\chapter{Roads to Network Recovery}\label{sec-lit-review}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``Network data cleaning is thus the lovechild of network backboning and link prediction, but that's a rather barren marriage -- as far as I know.''}

\hfill -- Michele Coscia\\
\doublespacing
\end{quote}

\end{minipage}

\end{flushright}

Here we give a brief overview of the key approaches to backboning and dependency recovery for networks through binary activations.
We organize the literature into categories based on the kinds of constraints that have been applied to make the network reconstruction ``inverse problem'' tractable: local structure, information-flow/resource constraints, or global structure.
Finally, we assess patterns in the assumptions made by the presented algorithms, and motivate the need for a new approach to fill a perceived gap in the network recovery space.

\section{Organizing Recovery Methods}\label{organizing-recovery-methods}

All recovery methods will require assumptions in addition to data to accomplish their task.
As discussed in \textcite{WhyHowWhen_Torres2021}, there are fundamental difference in dependency capability between the network and hypergraphic/bipartite representations of complex systems.
Necessarily, some information will be lost in translation between the two forms.

As a (hopefully) helpful way to organize the various \emph{kinds} of assumptions that are taking bipartite observations to simple graphs, we present an organization of common modeling assupmtions into three loosely-defined groups:

\begin{itemize}
\tightlist
\item
  Local Structure \& Additivity
\item
  Information Flow \& Resource Constraints
\item
  Global Structure Models
\end{itemize}

In truth, this classification should be viewed as more of a sliding scale, with approaches falling somewhere within.
Some approaches make very few assumptions about the shape a resulting network ``should'' take, but do so by making strong assumptions about how individual observations relate to a desired quantity, and especially how those observations are able to be combined to result in an ``answer'', however that looks.
Others instead provide clear normative constraints on the overall network topology, or emission mechanism, but this allows for flexibility in how data is individually handled.

This distinction could be thought of as a scale, which serves a role similar to pooling in bayesian inference.
Do individuals (observations) all have fundamentally separate distributions, so that global behavior (and by extension, uncertainty) is an aggregate phenomena?
Or do individuals (observations) inherit parameters from a global distribution shared by all, and anything outside that structural assumption must be ``noise''?
In between the extremes, some other assumption as to how the global and local scales mitigate information between them is required, \emph{i.e.}, \emph{partial pooling}.
In this domain, what we often see are attempts to perform noise corrections through the way information is thought to travel between nodes, generally.

For each of the above three groups we provide examples to illustrate modeling patterns and highlight common practice.\footnote{
  For a deeper assessment of the broad space of backboning and edge prediction in general a reader may be interested to see the overview in \textcite{atlasaspiringnetwork_Coscia2021}.}

\subsection{Local Structure \& Additivity Assumptions}\label{local-structure-additivity-assumptions}

These are, together, typically called ``association'' measures.
While they are sometimes presented as functions of the entire dataset, they nearly always find a basis in the inner-product operation, and have definitions in terms of \emph{contractions} along the data/observation dimension.
By relying on the (Euclidean) inner product, even with various re-weighting or normalization schemes, an analyst is making strong assumptions about their ability to reliably take measurements from linear combinations of observed activation vectors.

Essentially, if a measure relies on marginal counts or summation over the data axis (\(\mathbf{s}\)), then the main assumptions are at the \emph{local} level, about whether what we are adding together estimates our target correctly.
The most basic would be to count co-occurrences, and consequently the co-occurrence probability \(p_{11}=P(A,B)\).
However, for very rare co-occurrences, we need to correct for rate-imbalance of the nodes in much the same way correlation normalizes covariance.
This idea leads to ``cosine similarity''

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-cs} 

\vspace{-3mm}\textbf{Note \ref*{nte-cs}: Ochiai Coefficient (Cosine)}\vspace{3mm}

Effectively an uncentered correlation, but for binary observations the ``cosine similarity'' is also called the \emph{Ochiai Coefficient} between two sets \(A,B\), where binary ``1'' stands for an element belonging to the set.\autocite{Measuresecologicalassociation_Janson1981}
In our use case, it is measured as
\[
\frac{|A \cap B |}{\sqrt{|A||B|}}=\sqrt{p_{1\bullet}p_{\bullet 1}} \rightarrow  \frac{X^TX}{\sqrt{\mathbf{s}_i\mathbf{s}_i^T}}\quad \mathbf{s}_i = \sum_i \mathbf{x}_i
\]

\end{tcolorbox}

This interpretation of cosine similarity as the geometric mean of conditional probabilities is particularly useful when trying to approximate interaction rates.
The geometric mean as a pooling operator is conserved through Bayesian updates \autocite{ProbabilityAggregationMethods_Allard2012}, so the use of a prior with co-occurrences as base counts is possible for additive smoothing.
To do this, the goemetric mean of marginal counts acts as a ``psedovariable'' for exposure somewhere between A and B.
Empirically, this is a powerful approximation with good performance characteristics, for relatively little effort.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-or} 

\vspace{-3mm}\textbf{Note \ref*{nte-or}: Odds Ratios}\vspace{3mm}

Along with others derived from it, the Odds ratio is based on the ratio of conditional probabilities, and takes the form \[\text{OR}=\frac{p_{11}p_{00}}{p_{01}p_{10}}\]
Yule's Q and Y \autocite{MethodsMeasuringAssociation_Yule1912} are mobius transforms of the (inverse) \(\text{OR}\) and \(\sqrt{\text{OR}}\), respectively, that map association values to \([-1,1]\).
\[Q = \frac{\text{OR}-1}{\text{OR}+1}\quad Y=\frac{\sqrt{\text{OR}}-1}{\sqrt{\text{OR}}+1}\]

\end{tcolorbox}

Odds ratio is important to logistic regression, where the coefficients are usually the log-odds ratios of occurrence vs.~not (\(\log{\text{OR}}\)).\\
Yule's Y, also called the ``coefficient of colligation'', tends to scale with association strength in an intuitive way, so that proximity to 1 or -1 paints a more useful picture than the odds-ratio alone.

Another association measure, based in information theory, asks ``how much can I learn about one variable by observing another?''

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-mi} 

\vspace{-3mm}\textbf{Note \ref*{nte-mi}: Mutual information}\vspace{3mm}

An estimate for the mutual information (\emph{i.e.}, between the sample distributions) can be derived from the marginals, as above, though it is more compactly represented as a pairwise sum over the domains of each distribution being compared:
\[
\text{MI}(A,B)\approx \sum_{i,j\in[0,1]} p_{ij} \log \left( \frac{p_{ij}}{p_{i\bullet}p_{\bullet j}} \right) 
\]

\end{tcolorbox}

It is non-negative, with 0 occurring when A and B are independent.
There are many other information-theoretic measures related to MI, but we specifically bring this up as it will be the basis for the Chow Liu method, later on.

Sometimes, especially in social networks, we might want to avoid overcounting relationships with very well-connected nodes.
This was brought up with respect to the normalized Laplacian before, but we could also perform a normalization on the underlying bipartite adjacencies.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-hyp} 

\vspace{-3mm}\textbf{Note \ref*{nte-hyp}: Hyperbolic Projection}\vspace{3mm}

Attempts to account for the overcounting of co-occurrences on frequently occurring nodes, vs.~rarer ones.\autocite{Scientificcollaborationnetworks._Newman2001}
\[ X^T\text{diag}(1+\mathbf{s}_j)^{-1}X \quad \mathbf{s}_j = \sum_j{\mathbf{x}_j'}\]
This re-weights each observation by its degree in the original bipartite graph.

\end{tcolorbox}

So far this is the first measure that re-weights observations \emph{before} contraction, so that it depends on having the individual observations available (rather than only the gram matrix).
In this case, each observation's entries are all equally re-weighted by the number of activations in it (each nodes ``activation fraction'' in that observation).

\subsection{Resource and Information Flow}\label{resource-and-information-flow}

These methods are somewhere between local and global constraint scales.
This is accomplished by imagining nodes as having some amount of a resource (like information or energy) and correcting for observed noise in edge activation by reinforcing the \emph{geodesics} that most likely transmitted that resource.

First, closely related to hyperbolic re-weighting, we can imagine the bipartite connections as evenly dividing each nodes' resources, before reallocating them to the nodes they touch, in turn.
For instance, we might say each author splits their time among all of the papers they are on, and in turn every co-author ``receives'' an evenly divided proportion of everyone's time they are co-authoring with.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-rp} 

\vspace{-3mm}\textbf{Note \ref*{nte-rp}: Resource Allocation}\vspace{3mm}

Goes one step further than hyperbolic projection, by viewing each node as having some ``amount'' of a resource to spend, which gets re-allocated by observational unit. \autocite{Bipartitenetworkprojection_Zhou2007}
\[ \text{diag}(\mathbf{s}_i)^{-1}X^T\text{diag}(\mathbf{s}_j)^{-1}X \]

\end{tcolorbox}

Interestingly, we could see this as a two-step random-walk normalization of the bipartite adjacency matrix.
First \(X\) is row-normalized, then column-normalized.
The final matrix is asymmetric, so a symmetric edge strength estimate is often retrieved by mean, max, or min reduction operations.

Rather than stop after two iterations, continuing to enforce unit marginals to convergence is known as the Sinkhorn-Knopp algorithm, which converges to a doubly-stochastic matrix (both marginal directions sum to 1).

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-ot} 

\vspace{-3mm}\textbf{Note \ref*{nte-ot}: Doubly Stochastic}\vspace{3mm}

If \(A\in \mathbb{S}^{n\times n}\) is positive, then there exists \(d_1,d_2\) such that \[W=\text{diag}(d_1)A\text{diag}(d_2)\] is doubly-stochastic, and \(W(u,v)\) is the \emph{optimal transport plan} between \(u\) and \(v\) with regularized Euclidean distance between them on a graph.\autocite{RobustInferenceManifold_Landa2023,Sinkhorndistanceslightspeed_Cuturi2013}

The doubly-stochastic filter \autocite{twostagealgorithm_Slater2009} removes edges from \(W\) until just before the graph would become disconnected.

\end{tcolorbox}

As the name implies, the optimal transport plan reflects the minimum cost to move some amount of resource from one node to another.
By focusing on best-case cost, we enforce a kind of ``principle of least action'' to bias recovery toward edges along these geodesics.

A more direct way to do this, perhaps, is to find the shortest paths from every node to each other node, and aggregate them.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-hss} 

\vspace{-3mm}\textbf{Note \ref*{nte-hss}: High-Salience Skeleton}\vspace{3mm}

Count the number of shortest-path trees an edge participates in, out of all the shortest-path-trees (one for every node).
\[ \frac{1}{n}\sum_{i=1}^n \mathcal{T}_{\text{sp}}(i) \]
where \(\mathcal{T}_{\text{sp}}(n)\) is the shortest-path tree rooted at \(n\)
\autocite{Robustclassificationsalient_Grady2012}

\end{tcolorbox}

Unfortunately, HSS is forced to scale with the number of nodes, and must calculate the entire spanning tree for each one.

\subsection{Global Structural Assumptions}\label{global-structural-assumptions}

Often times these constraints are as simple as ``the underlying dependency graph must belong to a family \(\mathcal{C}\)'' of graphs.
Observations are thought of as emissions from a set of node distributions, where edges are representations of dependency relationship between them.
To provide a foundation to formalize this notion, one framework is that of Markov Random Fields, which are undirected generalizations of bayes nets \autocite{Markovrandomfields_Kindermann1980} that use edges to encode \emph{conditional dependence} between node distributions.

One of the original structures for MRFs that we could recover from observed data was a \emph{tree}.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-chowliu} 

\vspace{-3mm}\textbf{Note \ref*{nte-chowliu}: Chow-Liu Spanning Tree}\vspace{3mm}

Enforces tree structure globally.
Approximates a joint probability
\[
P\left(\bigcap_{i=1}^n v_i\right) \approx P' = \prod_{e\in T} P(u_n(e)|v_n(e)) \quad T\in \mathcal{T}
\]
where \(\mathcal{T}\) is the set of spanning trees for the nodes.
The Chow-Liu tree minimizes the Kullback-Leibler (KL) Divergence \(\text{KL}(P \| P')\) by finding the minimum spanning tree over pairwise mutual information weights.\autocite{Approximatingdiscreteprobability_Chow1968}

\end{tcolorbox}

Recent work has made it possible to enforce spanning tree structure while efficiently performing monte-carlo-style bayesian inference, which estimates a distribution over spanning trees that explain observed behavior, and by extension the likelihood each edge is in one of these trees. \autocite{BayesianSpanningTree_Duan2021}

If instead we imagine our MRF as being made up of individual Gaussian emissions, then the overall network will be a multivariate gaussian with pairwise dependencies along the edges.
In fact, as a consequence of the Hammersley--Clifford theorem, the conditionally independent variables are \emph{precisely} the set of zero entries in the the precision (inverse-covariance) matrix \(\Theta\) of the multivariate model.
Exploiting this fact leads to a semidifinite program to minimize the frobenius-norm of \(\Theta\) with the sample covariance \(\| \hat{\Sigma}\Theta \|_F\)\footnote{
  Since the sample covariance will not give an unbiased estimate for precision, these problems often require significant regularization.
  This class of problems is called ``covariance shrinkage'', though we more specifically care about \emph{precision shrinkage} as illustrated in Note~\ref{nte-glasso}.}

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-glasso} 

\vspace{-3mm}\textbf{Note \ref*{nte-glasso}: GLASSO}\vspace{3mm}

Semidefinite program to find (regularized) maximum likelihood precision of graph-structured multivariate Normal distribution using \(\ell_1\) (``LASSO'') penalty \autocite{Sparseinversecovariance_Friedman2008}

\begin{equation}\phantomsection\label{eq-glasso}{
\operatorname*{argmax} \mathcal{L}(\Theta|\hat{\Sigma})
  = \operatorname*{argmin}_{\Theta \prec 0}\ \text{tr}(\hat{\Sigma} \Theta) - \log |\Theta|+ \rho\|\Theta\|_1
}\end{equation}

In the binary case Equation~\ref{eq-glasso} is still guaranteed to find the structure of the generating MRF, but \emph{only for graphs with singleton separators}, as shown in \textcite{Structureestimationdiscrete_Loh2012}.

\end{tcolorbox}

The ``singleton separator'' condition means that only MRFs structured like trees or block graphs will have graph-structured precision matrices returned by the GLASSO program.
In effect (for the purpose of recovery from binary node activations) using GLASSO assumes either multivariate gaussian structure, or at the very least that all clique-separator sets are single-node.
In practice, GLASSO is used for more than just this, but with theoretical misspecification we must rely more on empirical validation.\footnote{
  something that this work attempts to begin addressing through standard reference datasets like MENDR, more on which is discussed in Section~\ref{sec-FP-experiments}}

There are many other models in this class, which provide strong global assumptions to make inference tractable.
Not all look like structural assumptions on the graph structure itself, like \emph{Degree Sequence Models}. \autocite{backbonebipartiteprojections_Neal2014,Comparingalternativesfixed_Neal2021}
They assume that the fundamental property of these datasets is their bipartite node degree distributions, leading to a generative model that can sample bipartite adjacency matrices with similar observation/node degree distributrions\footnote{In that literature the ``observation'' partition's nodes are typically called ``artifacts'' instead.}

Still others assume that the graphical structure can be described as generated by \emph{Stochastic Block Models}, a meta-network of communities and their inter-community connection probabilities.
They don't constrain the graph class itself, but instead prescribe a generative process for graph creation (through community blocks).
These have very nice properties for bayesian inference of structure, and can be modularly combined or nested for varying levels of specificity and hierarchical structure, sometimes with incredible computational efficiency. \autocite{ReconstructingNetworksUnknown_Peixoto2018,NetworkReconstructionCommunity_Peixoto2019}

\section{A Path Forward}\label{a-path-forward}

In addition to the categories above, there is a second ``axis'' that practitioners should keep in mind when selecting their recovery algorithm of choice.
Each of the above listed techniques can be mostly separated into two categories, based on whether hypergraphic/bipartite observations are assumed to be in \emph{data space}, or in \emph{model space}.

\subsection{Data assumptions}\label{data-assumptions}

Recall from Section~\ref{sec-lin-ops}: an operator takes our model parameters and maps them to data space.
The implication for inverse problems is a need to \emph{remove the effect} of the operator, because we cannot directly observe phenomena in a way compatible with our model (\emph{e.g.}, which might model underlying causal effects).

This is a core point of view in \textcite{Statisticalinferencelinks_Peel2022}, where those authors describe nearly all of network analysis as \emph{inferring hidden structure}:

\singlespacing

\begin{quote}
``Here we argue that the most appropriate stance to take is to frame network analysis as a problem of inference, where the actual network abstraction is hidden from view, and needs to be reconstructed given indirect data.''

\hfill -- Peel et al., \autocite{Statisticalinferencelinks_Peel2022}
\end{quote}

\doublespacing

We note that this isn't strictly true, \emph{assuming} that the ``network'' is intended to represent something measured by the direct observation.
For instance, if a network is intended to represent a discretization of distances (such as a k-nearest neighbors approximation) for computational efficiency.
The co-occurrence measures can be thought of as estimators of node-node distances, especially with appropriate smoothing to remove zero-valued distances from undersampling.\footnote{
  See Section~\ref{sec-steiner} for an elaboration of this connection via the ``forest kernel''.}
In otherwords, if an analyst wishes to discretize distances as incidences in a complex network, they are effectively using ``high-pass'' filter to remove low-similarity entries, which is an effective way to assess community structure---exactly like clustering for continuous data.\footnote{
  Or, at a slight risk of reductionism, drawing a world atlas with two colors for ``above and below sea-level'': useful simplification for rapid assessment of shapes.}
In fact, for an example of this exact network-as-discretization idea being used for state-of-the-art clustering performance, see HDBSCAN in \textcite{HybridApproachHierarchical_Malzer2020}.

Because it is difficult to know a priori what a domain will require of network analysts, our main recommendation is for algorithm creators to transparently describe their technique's data-space assumption:

\begin{itemize}
\tightlist
\item
  are observations already in \emph{model space}, perhaps with with alleatoric noise to be removed?, or,
\item
  are they in \emph{data space} and require solving some form of inverse problem to recover a model specification?
\end{itemize}

Once again from the \textcite{Statisticalinferencelinks_Peel2022} review:

\singlespacing

\begin{quote}
Surprisingly, the development of theory and domain-specific applications often occur in isolation, risking an effective disconnect between theoretical and methodological advances and the way network science is employed in practice.

\hfill -- Peel et al. \autocite{Statisticalinferencelinks_Peel2022}
\end{quote}

\doublespacing

In a similar vein, we believe that a large amount of metrological inconsistency and struggle has at its heart a communication and technology transfer problem, which standardization and community toolkit support can hopefully work toward fixing.

With this in mind, we show in Table~\ref{tbl-roads} an overview of the covered approaches, and whether the method presumes operation on observations in the same space as the \emph{model}, or if some inverse problem is needed.

\begin{table}

\caption{\label{tbl-roads}Organizing recovery methods by representation space and level}

\centering{

\subcaption{\label{tbl-roads-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part1/1-03-recovery-road_files/figure-latex/-content-codefigs-graphs-tbl-roads-output-1.pdf}}

}

}

\end{table}%

To add to the point, the last column in Table~\ref{tbl-roads} shows whether the full bipartite representation is even needed to perform the technique, or if it is possible with the gram matrix or marginal values alone.\footnote{note that any of these could make use of the bipartite ``design matrix'', \emph{e.g.}, to estimate the edge support with \emph{stability selection}\autocite{StabilitySelection_Meinshausen2010} by subsampling it multiple times and repeating the algorithm accordingly.}

\subsection{Model vs Estimation Approach}\label{model-vs-estimation-approach}

Because network reconstruction can be such a computationally intense problem, special consideration must be paid to not only the model, but to the \emph{approach} taken to estimate the model.
Many of the methods described conflate the two, since models are often constructed \emph{because} of the estimation approaches they enable.
It is worth distinguishing the two ideas before moving to the next chapter: a \emph{model} is a conceptual paradigm for what a recovered network \emph{represents}, while an \emph{approach} is a mechanism to estimate that model's parameters.

For instance, GLASSO is often thought of as a ``model'', and this is somewhat true, but the model is a combination of using the Markov Random Field assumption with a definition of ``goodness'' based on minimizing the Frobenius norm between an estimate and our observations.
The \emph{approach} is to add a regularization term (L1), and pick one of many techniques to minimize the total loss function, \emph{e.g.} coordinate-descent (CD).
The model makes assumptions about global properties (MRF) of the graph, while the approach determines the computational complexity and can allow for certain types of information to be utilized.
Coordinate descent operates node-wise, meaning our complexity will scale with network size, but can also allow data to be split along the observation dimension to allow bootstrapping techniques like Stability Selection \autocite{StabilitySelection_Meinshausen2010}.

Recent advances have focused on generalizing \emph{approaches} to improve performance of techniques like CD.
Greedy coordinate-descent (GCD) has been proposed as a way to achieve sub-quadratic convergence time (in network-size), by utilizing NN-descent to approximate a set of edge candidates at each iteration \autocite{Scalablenetworkreconstruction_Peixoto2024}.
In a follow-up work, a generic approach to applying GCD to arbitraty network probability model used the \emph{minimum description length} principle to overcome overfitting issues in \(L_1\)-based regularization schemes like GLASSO \autocite{Networkreconstructionvia_Peixoto2024}.
While still limited by super-linear scaling in network size, the performance and accuracy gains using this approach are \emph{considerable}, once an appropriate modeling framework is selected to apply it to (\emph{e.g.} SBM or Ising).

\subsection{Filling the ``gap''}\label{filling-the-gap}

In the next sections, we focus on filling a gap for models that only make local assumptions and preserve additivity, but \emph{assume that data is not represented directly in the network model-space} (\emph{i.e.} data-space assumption).
First, we need a modeling framework for network recovery that retains additivity in a principled way.
Much like the role that nonparametric estimators like KDE/Nadarya-Watson play in regression, or Kaplan-Meier estimators in survival analysis \autocite{Topicsadvancedeconometrics_Bierens1996,NonparametricEstimationIncomplete_Kaplan1958}, additive models only make assumptions about local structure.
But from these assumptions, they can provide critical insight into data and its structure, and push analysts to make regular ``sanity checks'' when results or assumptions conflict.
Chapter~\ref{sec-desirepath} will cover \emph{Desire Path Densities}, our modeling framework for additive, data-space model specifications.

Next, an \emph{approach} to estimating model parameters is needed.
By exploiting a common class of domain-informed constraints---namely, node activation \_via spreading process---we can build an algorithm for network reconstruction that scales linearly in observation count, while remaining approximately \emph{constant} in network size.
This will be covered in Chapter~\ref{sec-fp}.

\part{Nonparametric Network Recovery With Random Spanning Forests}

\chapter{Latent Graphs with Desire Paths}\label{sec-desirepath}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``A desire path is no more than the trace of a decision---less than that, an impulse---to find a new way to join what we know with what we have yet to discover.''}

\hfill -- David Farrier\\
\doublespacing
\end{quote}

\end{minipage}

\end{flushright}

Addressing gaps discussed in Chapter~\ref{sec-lit-review} to reach a generative model for network recovery requires careful attention to the generation mechanism for node activations.
While there are many ways we might imagine bipartite data to be be generated, presuming the existence of a dependency graph that \emph{causes} activation patterns will give us useful ways to narrow down the generative specification.

First, we will investigate the common assumption that pairwise co-occurrences can serve as proxies for measuring relatedness, and how this ``gambit of the group'' is, in fact, a strong bias toward dense, clique-filled recovered networks.
Using the relationship between matrix products and sums of vector outer-products, we then motivate a generalization of co-occurrence estimation that can be flexibly adapted to domain knowledge, as appropriate, and avoid undue clique bias altogether.
Finally, we propose a simple mechanism to frame network reconstruction as combinations of multiple overlaid subgraphs, by treating edges as i.i.d. Beta-Bernoulli random variables.\\
Inspired by the behavior of so-called ``desire paths'', we constrain the beta prior to ensure desired behavior, which we call the \emph{Desire Path Density} estimate of the global graph structure.

\section{The Gambit of the Inner Product}\label{sec-clique-bias}

As we saw repeatedly in Chapter~\ref{sec-lit-review}, networks are regularly assumed to arise from co-occurrences, whether directly as counts or weighted in some way.
This assumption can be a significant a source of bias in the measurement of edges.
In this section we provide an intuitive understanding for \emph{why} a flat count of co-occurrence leads to ``hairballs'' (specifically, a bias for dense clusters and cliques), related to the use of matrix products on node activation design matrices.

\subsection{Gambit of the Group}\label{gambit-of-the-group}

It seems reasonable, when interactions are unobserved, to assume some evidence for all possible interactions is implied by co-occurrence.
However, similar to the use of uniform priors in other types of inference, if we don't have a good reason to employ a fully-connected co-occurrence prior on interaction dependencies, we are adding systematic bias to our inference.
Whether co-occurrence observations can be used to infer interaction networks directly was discussed at length in \textcite{Techniquesanalyzingvertebrate_Whitehead1999}, where they call this the \emph{gambit of the group}.

\begin{quote}
``So, consiously or unconsciously, many ethnologists studying social organization makr what might be called the `gambit of the group': they assume that animals which are clustered {[}\ldots{]} are interacting with one another and then use membership of the same cluster {[}\ldots{]} to define association.''

\textcite{Techniquesanalyzingvertebrate_Whitehead1999}
\end{quote}

This work was rediscovered in the context of measuring assortativity for social networks,\footnote{Assortativity is, roughly, the correlation between node degree and the degrees of its neighbors.} where the author of \textcite{PerceivedAssortativitySocial_Fisher2017} advises that ``group-based methods'' can introduce sampling bias to the calculation of assortativity, namely, systematic overestimation when the sample count is low.

A reader can analyze general problems with failing to specify a model of what ``edges'' actually \emph{are} more in-depth in \textcite{Statisticalinferencelinks_Peel2022}.
They also include a warning not to naively use correlational measures with a threshold, since even simple 3-node systems will easily yield false positives edges.
Still, it would be helpful for practitioners to have a more explicit mental model of \emph{why} co-occurence-based models yield systematic bias, and use that to build an alternative having some of the same benefits (speed, interpretability, uncertainty quantification, etc.)

\subsection{Inner-Product projections and ``clique bias''}\label{inner-product-projections-and-clique-bias}

Underlying correlation and co-occurrence models for edge strength is a reliance on matrices of inner products between node occurrence vectors.
They all use gram matrices (or centered/scaled versions of them), which were brought up in Section~\ref{sec-products}.
The matrix multiplication performed represents inner products between all pairs of feature vectors.
For \(X(i,j)\in\mathbb{B}\), these inner products sum together the times in each observation that two nodes were activated together.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-04-rand-sf_files/figure-latex/-content-codefigs-graphs-fig-stack-outerprod-output-1.pdf}}

}

\caption{\label{fig-stack-outerprod}Gram matrix as sum of observation outer products}

\end{figure}%

However, another (equivalent) way to view matrix multiplication is as a sum of outer products
\[
G(j,j') = X^T X = \sum_{i=1}^m X(i,j)X(i,j')= \sum_{i=1}^m \mathbf{x}_i\mathbf{x}_i^T
\]
Those outer products of binary vectors create \(m\times m\) matrices that have a 1 in every \(j,j'\) entry where nodes \(j,j'\) both occurred, shown in Figure~\ref{fig-stack-outerprod}.
Each outer product is effectively operating as a \(D_i+A_i\) with degrees normalized to 1.
If the off-diagonals can be seen as adjacency matrices, they would strictly represent a clique on nodes activated in the \(i\)th observation
In this sense, any method that involves transforming or re-weighting a gram matrix, is implicitly believing that the \(i\)th observation is a \emph{complete graph} for all \(i\).
This is illustrated in Figure~\ref{fig-stacked-graphs}.

\begin{figure}

\begin{minipage}{0.33\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-04-rand-sf_files/figure-latex/-content-codefigs-graphs-fig-obs-set-output-1.pdf}}

}

\subcaption{\label{fig-obs-set}Observations as activation sets}

\end{minipage}%
%
\begin{minipage}{0.67\linewidth}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-04-rand-sf_files/figure-latex/-content-codefigs-graphs-fig-stack-bow-output-1.pdf}}

}

\subcaption{\label{fig-stack-bow}Edge Measurements with Group Gambit (BoW) assumption}

\end{minipage}%

\caption{\label{fig-stacked-graphs}Inner-product projections as sums of cliques illustrating ``clique bias''.}

\end{figure}%

If every observation of node activations leads to an implied clique, we can reframe much of the ``hairball'' effect as a systematic bias (i.e.~measurement error in the sense of\_trueness\_).
We call this \emph{clique bias}: the inferred graph will inherently include more and more cliques of node subsets as data arrives (assumed to themselves be cliques).

For many modeling scenarios, this paradigm allows practitioners to make a more straight-forward intuition-check: do clique observations \emph{make sense} here?
When a list of authors for a paper is finished, does that imply that all authors mutually interacted with all others directly to equally arrive at the decision to publish?
This would be similar to assuming the authors might simultaneously enter the same room, look at a number of others (who all look exclusively at each other, as well), and \emph{all at once} decide to publish together.

Or, from the standpoint of scaling: does each extra node activation impart an amount of information that depends on the number of activated nodes?
Put another way, if we knew our observations were on a planar graph, each node might require around 3 new edges.\footnote{Triangulations are worst-case, so \(|E|\leq 3|V|-6\) due to Euler's formula}
A tree or path adds one new edge for each new node.
But a clique assumption means that each extra node activation adds edges quadratically in the number of already-activated nodes.
Does this make sense?
In our introduction, we described a more likely scenario we could expect from an observer on the ground: a researcher asks a colleague or two to collaborate, who might know a couple more with relevant expertise, and so on.
From purely a logistical standpoint, it quickly becomes unfeasible for authors to mutually collaborate with all other co-authors equally: 10 coauthors already implies the 10th had to equally split interaction among 9 others to satisfy our model.

\section{Networks as Desire Path Density Estimates}\label{networks-as-desire-path-density-estimates}

Unfortunately, methods based on inner-product thresholding are still incredibly common, in no small part due to how \emph{easy} it is to create them from occurrence data, regardless of this ``clique-bias''.
The ability to map an operation onto every observation, \emph{e.g.}, in parallel, and then reduce all the observations into an aggregate edge estimate is a highly desirable algorithmic trait.
This may be a reason so many projection and backboning techniques attempt to re-weight (but retain) the same basic structure, time and again.

What we need is a way to maintain the ease-of-use of inner-product network creation:

\begin{itemize}
\tightlist
\item
  Map an operation onto each observation
\item
  Reduce to an aggregate edge guess over all observations
\end{itemize}

but with a more domain-appropriate operator at the observation level.

Let's start with replacements for the clique assumption.
There are many non-clique classes of graphs we might believe local interactions occur on: path-graphs, trees, or any number of graphs that reflect the topolgy or mechanism of local interactions in our domain of interest.
Authors have proposed classes of graphs that mirror human perception of set shapes \autocite{Relativeneighborhoodgraphs_Jaromczyk1992}\footnote{
  \emph{e.g.}, for dependencies based on perception, such as human decision making tendencies, or causes based on color names.}, or graphs whose modeled dependencies are strictly planar \autocite{toolfilteringinformation_Tumminello2005}\footnote{
  \emph{e.g.}, when interactions are limited to planar dependencies, like inferring ancient geographic borders.}.
Alternatively, the interactions might be scale free, small-world, trees, or samples from stochastic block models.\autocite{StochasticblockmodelsFirst_Holland1983}
In any case, these assumptions provide an explicit way to describe the set of \emph{possible} interaction graphs we believe our individual observations are sampled from.

\subsection{Subgraph Distributions}\label{sec-subgraph-dists}

Let's use the notation from Equation~\ref{eq-edge-vectors}, such that each observation of nodes \(\mathbf{x}_i\) is implicitly derived from a set of activated edges \(\mathbf{r_i}\).
To start, our current belief about what overall structure the whole network can take is \(G^*=(V,E,B^*)\), where \(B^*\) might always return \(1\) to start out (the complete graph).
To further constrain the problem, let us assume that node activation is noiseless: any activated nodes were truly activated, and unactivated nodes were truly inactive (no false negative or false positive activations).\footnote{
  Hidden nodes (unobserved nodes beyond the \(n\)) are outside the scope of this work, though could potentially be implied for certain structures \emph{e.g.}, when the metric is known to be tree-like.
  \textcite{TreeIam_Sonthalia2020} use a greedy embedding that minimizes distortion to estimate the need for added \emph{Steiner} nodes.}
So, each \(\mathbf{x}_i\) will induce a subgraph \(g_i = G^*[V_i]\), where \(V_i = \{v\in \mathcal{V} | X(i,\mathcal{V})=1\}\).
Then, our domain knowledge takes the form of a constraint on edges within that subgraph, which will induce a family of subgraphs on \(g_i\).
This family \(\mathcal{C}_i\) belongs to the relevant class of graphs \(\mathcal{C}\), limited to nodes \(V_i\), \emph{i.e.},

\begin{equation}\phantomsection\label{eq-subgraph-family}{
\begin{gathered}
\mathcal{C}_i = \{(V,E,B_i) \in\mathcal{C}|B_i(e,v)=B^*(e,v)\mathbf{1}_{V_i}(v)\mathbf{1}_{E_i}(e)\}\\
E_i\in\{\mathcal{E}\in\mathcal{P}(E)| g_i[\mathcal{E}]\in\mathcal{C}\}
V_i = \{v\in \mathcal{V} | X(i,\mathcal{V})=1\}
\end{gathered}
}\end{equation}\footnote{\(\mathcal{P}(A)\) is the \emph{powerset} of \(A\), \emph{i.e.}, the set of all subsets of \(A\).}

In other words, the edges that ``caused'' to the node activations in a given observation must \emph{together} belong to a graph that, in turn, belongs to our desired class, which must occur on the nodes that were activated.

Assuming we can define an associated measure \(\mu_i(c)\) (one for each \(c\in\mathcal{C}_i\)) we are able to define a probability distribution over subgraphs in the class.\footnote{
  This is certainly not a trivial assumption, and might either be ill-defined or require techniques like the Gumbel trick\autocite{GradientEstimationStochastic_Paulus2020} to approximate, unless the partition function \(Z\) has a closed form, or \(\mu\) is already a probability measure on some \(\sigma\)-algebra over \(\mathcal{C}\).
  Closed-form \(\mathcal{Z}\) values on \(\mathcal{C}\) are known for a handful of graph classes, such as the space of spanning trees, \(\mathcal{C}=\mathcal{T}\).
  However, another way this might be accomplished is through random geometric graphs\autocite{RandomPlaneNetworks_Gilbert1961}, or geometric spanners like the Relative Neighborhood \autocite{Relativeneighborhoodgraphs_Jaromczyk1992} graphs on a ``sprinkling'' of points that preserves their observed pairwise distances.\\
  This is further discussed in Section~\ref{sec-future-hyperbolic}.}
Using notation similar to distributions over spanning trees in \textcite{EfficientComputationExpectations_Zmigrod2021}:

\begin{equation}\phantomsection\label{eq-subgraph-prob}{
\begin{gathered}
p_i(c) = \frac{\mu_i(c)}{Z_i}\\
Z_i = \sum_{c\in\mathcal{C}_i} \mu_i(c)
\end{gathered}
}\end{equation}

This can be represented using the vectorization in Equation~\ref{eq-edge-vectors}, due to the one-to-one correspondence established, so that, with a slight abuse of notation treating \(\mathcal{C}_i\) as the parameter of distribution \(p_i\):
\begin{equation}\phantomsection\label{eq-edgevec-prob}{
\mathbf{r}_{i}(e)|\mathbf{x_i} \sim p_i(\mathcal{C},E,V)
}\end{equation}

So we may not have an exact vector, but we have established a way to specify a family of edge vectors that could be responsible.
With \(p_i(c)\), we also have a mechanism to sample them when a partition function is available (or able to be approximated).

With these mechanics in place, we see that choosing ``cliques'' (implied by the inner product) is a trivial application of Equation~\ref{eq-edgevec-prob}, since that is equivalent to selecting the class of cliques on \(V_i\) nodes.
This has only one element (\(\|\mathcal{C}_{\text{clique}}\|=1\)), there is only 1 possible selection, with probability \(p_i(K^{V_i})=1\).

\subsection{Graph Unions as Desire Paths}\label{graph-unions-as-desire-paths}

With a distribution over subgraphs each observation, we are potentially able to sample from them for bootstrap or Monte Carlo estimation purposes, or simply find a maximum likelihood estimate for each distribution.
Assuming this is true, we may now sample or approximate a matrix \(R(i,e):I\times E \rightarrow \mathbb{B}\).\\
Methods for doing this efficiently in certain cases are the focus of Section~\ref{sec-FP} and Section~\ref{sec-lfa-gibbs}.
However, once \(R(i,e)\) is estimated, a reasonable mechanism for estimating the support of the set of edges is to use \(\frac{\text{count}}{\text{exposure}}\), but with a few needed modifications.

First, while the nodes counts in \(\sum_i B(i,\cdot)\) are by assumption \emph{not} independent, or even pairwise independent, the \emph{edge traversal} counts \(\sum_i R(i,\cdot)\) could more reasonably be considered so.
A model certainly could be constructed where edge existence depends on other edges existing (or not).
But nothing is inherently self-inconsistent with a model that assumes the traversability of individual edges will be independent of one another.

Let P(e) be the probability that an edge is traversed (in any observation), and P(u,v) the probability that nodes \(u,v\) co-occur.
To approximate the overall traversability of an edge, we can calculate an empirical estimate for the conditional probability \(P(e|(u,v))=\frac{P(e)\cap P(u,v)}{P(u,v)}\) that an edge is traversed, given that the two incident nodes are activated.
This estimate can use the same Beta-Bernoulli distribution from Equation~\ref{eq-beta-binomial}.

Still, how do we ensure our estimate is approximating traversability, so that the probability that an edge probability converges toward 1 as long as it \emph{has to be possible} for \(e\) to be traversed?
Recall from the introduction that, from a measurement perspective, the underlying networks rarely ``exist'' in the sense that we never truly find the underlying network, but only observations sampled from it.
Imagine that the ``real'' network is a set of paved sidewalks: our procedure is similar to watching people walk along paths between locations, and wanting to estimate which of the paths would be tread ``enough'' to be paved.
This is where we build on an intuition based on the popular idea of \emph{desire paths} which is a colloquial name for paths that form when individuals walk over grass or dirt enough to carve a trail.
In network analysis and recovery from activations, then, we are only allowed to see lists of visited locations.
If we can add a domain-informed assumption on what the \emph{trajectories} of individuals could have been, based on those locations, then we can estimate the desire paths that might have formed from them.
Importantly, we can use this framing to ``reset'' the trueness of our uncertainty: given a trajectory assumption, the desire path uncertainty becomes one of precision due to lack of knowledge about which path was taken.
As \textcite{Statisticalinferencelinks_Peel2022} recommend, desire paths are \emph{inferred}, and we never rely on needing to guess the actual ``pavement''---only beliefs about ``future paving''.

If presented with two equal-length desire paths, an individual is likely to choose the one that has been tread more often before \emph{i.e.}, the ``more beaten'' path.
So, we don't want a probability that an edge has been traversed, but a probability over fractions of the time we expect an edge to have been traversed \emph{more often than not}: how likely it is to ``be beaten''.
This is accomplished by forcing \(\alpha, \beta < 1\).
For the case where \(\alpha=\beta=0.5\), we call this special case an \emph{arcsine} distribution.

In a coin tossing game where each ``heads'' gives a point to player A and ``tails'' to player B, then the point differential is modeled as a random walk.
The arcsine distribution \(\text{Beta}(\tfrac{1}{2},\tfrac{1}{2})\) is exactly the probability distribution for the fraction of time we expect one player to be ``ahead'' of the other. \autocite{WhatisArcsine_Ackelsberg2018}

\begin{quote}
``\emph{Contrary to popular opinion, it is quite likely that in a long coin-tossing game one of the players remains practically the whole time on the winning side, the other on the losing side.''}

William Feller\autocite[Chapter III]{IntroductionProbabilityTheory_Feller1968}
\end{quote}

This is desirable behavior for a distribution over edge traversability!
We expect the vast majority of edges to have a 0 or 1 as the most likely values, with the expected fraction of observations that an edge being traversed was ``ahead'' of it being \emph{not} traversed matching our empirical count.
We generalize this with \(\alpha = 1-\beta\), with \(\alpha + \beta = 1\), such that the new beta posterior from Equation~\ref{eq-beta-binomial} with \(s\) successes over \(k\) trials is:

\begin{equation}\phantomsection\label{eq-desirepath-binom}{
\begin{gathered}
\pi \sim \text{Beta}(\alpha + c, 1-\alpha-c) \\
c = \frac{s-ak}{k+1}\\
\end{gathered}
}\end{equation}

Important to note is that \(k\) is measured over the \emph{co-occurrences} \((u,v)\), and successes are the traversals \(e\) derived from our samples in \(R\).
This lets us formulate a likelihood model for each edge's traversibility in the global network \(G\) (\emph{i.e.}, whether \(B(e,v)>0\) for any \(v\)), which is i.i.d. Bernoulli.
\begin{equation}\phantomsection\label{eq-empirical-model}{
\begin{gathered}
\pi_e \sim \text{Beta}(\alpha, 1-\alpha), e\in E\\ 
E \sim \text{Bernoulli}(\pi_e), e \in E
\end{gathered}
}\end{equation}

This does not yet specify a likelihood for \(\mathcal{C}_i\), because we have not included a mechanism for the down-selection to each \(\mathbf{x}_i\).
This will be addressed more completely for the special case of \(\mathcal{C}=\mathcal{F}\), the set of spanning forests on a graph, in Section~\ref{sec-lfa-like}.
In general, however, failing to specify the prior distribution on \(\mathcal{C}_i\) does not necessarily make Equation~\ref{eq-edgevec-prob} unusable, but necessitates an ``empirical bayes'' approach.
With the marginals and co-occurrence counts for nonzero values in \(X\), we can make a point estimate for each edge \emph{given} a node subset, without needing to consider a prior distribution for each \(\mathbf{x_i}\).

The nonparametric approach, in cases that merit the use of spanning trees, will be central to accurate and scalable estimation of \(B\) through our proposed method covered in the next chapter, \emph{Forest Pursuit}.

\chapter{\texorpdfstring{Approximate Recovery in Near-linear Time by \emph{Forest Pursuit}}{Approximate Recovery in Near-linear Time by Forest Pursuit}}\label{sec-fp}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``Does the pursuit of truth give you as much pleasure as before? Surely it is not the knowing but the learning, not the possessing but the acquiring, not the being-there but the getting there that afford the greatest satisfaction.''}

\hfill -- Carl Friedrich Gauss\\
\doublespacing
\end{quote}

\end{minipage}

\end{flushright}

In this chapter, we build on the notion of ``Desire Path'' estimation of a dependency network from node activations --- sampling from a class of subgraphs constrained to active nodes, then merging them.
We present \emph{Forest Pursuit} (FP), a method that is scalable, trivially parallelizes, and offline-capable, while also outperforming commonly used algorithms across a battery of standardized tests and metrics.
The key application for using FP is in domains where node activation can be reasonably modeled as arising due to \emph{random walks}---or similar spreading process---on an underlying dependency graph.

Because we wish to model our node activations as being \emph{caused} by other nodes (that they depend on), we draw a connection to a class of models for \emph{spreading}, or, \emph{diffusive processes}.
We outline how random-walks are related to these diffusive models of graph traversal, enabled by an investigation of the graph's ``regularized laplacian'' from \textcite{Semisupervisedlearning_Avrachenkov2017}.

First, we build an intuition for the use of trees as an unbiased estimator for desire path estimation when spreading processes are at work on the latent network.
Then, the groundwork for FP is laid by combining sparse approximation through \emph{matching pursuit} with a loss function modeled after the Chow Liu representation for joint probability distributions.
The approximate complexity for FP is linear in the spreading rate of the modeled random walks, and linear in dataset size, while running in \(O(1)\) time with respect to the network size.
This departs dramatically from other methods in the space, all of scale in the number of nodes in the entire network.
We then test FP against an array of alternative methods (including GLASSO) with MENDR, our proposed standard reference dataset and testbench for network recovery.
FP outperforms other tested algorithms in nearly every case, and empirically confirms its complexity scaling for sub-thousand network sizes.

\section{Random Walks as Spanning Forests}\label{random-walks-as-spanning-forests}

The class of diffusive processes we focus on ``spread'' from one node to another.
If a node is activated, it is able to activate other nodes it is connected to, directly encoding our need for the graph edges to represent that nodes ``depend'' on others to be activated.
In this case, a node activates when another node it depends on spreads their state to it.
These single-cause activations are often modeled as a random-walk on the dependency graph: visiting a node leads to its activation.

\subsection{Random Walk Activations}\label{random-walk-activations}

Random walks are regularly employed to model spreading and diffusive processes on networks.
If a network consists of locations, states, agents, etc. as ``nodes'', and relationships between nodes as ``edges'', then random walks consist of a stochastic process that ``visits'' nodes by randomly ``walking'' between them along connecting edges.
Epidemiological models, cognitive search in semantic networks, website traffic routing, social and information cascades, and many other domains also involving complex systems, have used the statistical framework of random walks to describe, alter, and predict their behaviors. \autocite{effectivenessrandomwalks_Kim2023,RobustCascadeReconstruction_Xiao2018,Humanmemorysearch_Jun2015,RandomWalksElectric_Doyle2000,MarkovPerspectiveDevelopment_Mane2011}

When network structure is known, the dynamics of random-walks are used to capture the network structure via sampling \autocite{LittleBallFur_Rozemberczki2020}, estimate node importance's{[}Mathematicsnetworks\_Newman2018{]}, or predict phase-changes in node states (\emph{e.g.}, infected vs.~uninfected)\autocite{StructureFunctionComplex_Newman2003}.
In our case, since we have been encoding the activations as binary activation vectors, the ``jump'' information is lost---activations are ``emitted'' for observation only upon the random walker's initial visit.\autocite{Humanmemorysearch_Jun2015}
Further, the ordering of emissions has been removed in our binary vector representation, leaving only co-occurrence groups in each \(\mathbf{x}_i\).\footnote{For a brief treatment of the case that INVITE emission order is preserved, see Chapter~\ref{sec-ordered}}
In many cases, however, the existence of relationships is not known already, and analysts might \emph{assume} their data was generated by random-walk-like processes, and want to use that knowledge to estimate the underlying structure of the relationships between nodes.

\subsection{Activations in a Forest}\label{activations-in-a-forest}

As a general setting, the number of node activations (\emph{e.g.}, for datasets like co-authorship) is much smaller than the set of nodes (\(\|\mathbf{x}_i\in\mathbb{S}^n\|_0 \ll n\))\footnote{
  \(\|\cdot\|_0\) is the \(\ell_0\) ``pseudonorm'', counting non-zero elements (the support) of its argument.}\\
\textcite{Semisupervisedlearning_Avrachenkov2017} go to some length describing discrete- and continuous-time random walk models that can give rise to binary activation vectors like our \(X(i,j):I\times J\rightarrow \mathbb{B}\).
The \emph{regularized laplacian} (or \emph{forest}) kernel of a graph\autocite{SimilaritiesgraphsKernels_Avrachenkov2019} plays a central role in their analysis, as it will in our discussion going forward.

\begin{equation}\phantomsection\label{eq-regulap}{
Q_{\beta} = (I+\beta L)^{-1}
}\end{equation}

In that work, it is discussed as the optimal solution to the semi-supervised ``node labeling'' problem, having a regularization parameter \(\beta\), though its uses go far beyond this.\autocite{GraphLaplacianRegularization_Pang2017,Countingrootedforests_Knill2013,MatrixForestTheorem_Chebotarev2006}
\(Q\) generalizes the so-called ``heat kernel'' \(\exp{(-t\tilde{L})}\), in the sense that it solves a lagrangian relaxation of a loss function based on the heat equation.
This can be related to the PageRank (\(\exp{(-tL_{\text{rw}})}\)) kernel as well, which is explicitly based on random walk transition probabilities.

In fact, \(Q\) can be viewed as a transition matrix for a random walk having a geometrically distributed number of steps, giving us a small expected support for \(\mathbf{x}_i\), as needed.\footnote{\(Q\) can also be interpreted as a continuous-time random walk location probability, after exponentially distributed time, if spending exponentially-distributed time in each node.}
However we interpret \(Q\), a remarkable fact emerges due to a theorem by Chebotarev\autocite{MatrixForestTheorem_Chebotarev2006,Countingrootedforests_Knill2013}: each entry \(q=Q(u,v)\) is equal to the probability\footnote{edge weights scaled by \beta} that nodes \(u,v\) are connected in a randomly sampled \emph{spanning rooted forest}

In other words, co-occurring node activations due to a random walk or heat kernel are deeply tied to the chance that those nodes find themselves \emph{on the same tree in a forest}.

\subsection{Spreading Dependencies as Trees}\label{spreading-dependencies-as-trees}

With the overt link from spreading processes to counts of trees made, there's room for a more intuitive bridge.

For single-cause, single source spreading process activations---on a graph---the activation dependency graph for each observation/spread/random walk \emph{must be a tree}.
With a single cause (the ``root''), which is the starting position of a random walker, a node can only be reached (activated) by another currently activated node.
If the random walk jumps from one visited node to another, previously visited one, that transition did not result in an activation, so the \emph{dependency} count for that edge should not increase.
This description of a random walk, where subsequent visits do not ``store'' the incoming transition, is roughly equivalent to one more commonly described as a \emph{Loop-Erased} random walk.
It is precisely used to uniformly sample the space of spanning trees on a graph.\autocite{Generatingrandomspanning_Wilson1996}

Much like a reluctant co-author ``worn down'' by multiple collaboration requests, we can even include random walks that ``receive'' activation potential from more than one source.
Say a node is activated when some fraction of its neighbors have all originated a random walk transition to it, or a node activates on its second visit, or similar.
We simply count (as dependency evidence) the ultimate transition that precipitated activation.
This could be justified from an empirical perspective as well: say we observe an author turn down requests for one paper from two individuals, but accept a third.
We could actually infer a \emph{lowered} dependency on the first two, \emph{despite} the eventual coauthorship.
Only the interaction that was observed as successful necessarily counts toward success-dependency, barring any contradicting information.

It's important to add here that \emph{mutual convincing} by multiple collaborators simultaneously (or over time) is expressly left out.
In other words, only pairwise interactions are permitted.
This is not an additional assumption, but a key limitation of our use of graphs in the first place!
As Torres et al.~go to great lengths elaborating in \autocite{WhyHowWhen_Torres2021}, it is critical to correctly model dependencies when selecting a structural representation of our problem to avoid data loss.
The possibility for multi-way interactions would necessitate the use of either a simplicial complex or a hypergraph as the carrier structure, \emph{not a graph}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-graphs-fig-stack-tree-output-1.pdf}}

}

\caption{\label{fig-stack-tree}Edge Measurements with true (tree) dependencies known}

\end{figure}%

Figure~\ref{fig-stack-tree} demonstrates the use of trees as the distribution for subgraphs, instead of outer-products/cliques.

\section{Sparse Approximation}\label{sparse-approximation}

As indicated previously, we desire a representation of each observation that takes the ``node space'' vectors (\(\mathbf{x}_i\)) to ``edge space'' ones (\(\mathbf{r}_i\)).
We have separated each observation with the intention of finding a point-estimate for the ``best'' edges, such that the edge vector induces a subgraph belonging to a desired class.
If we assume that each edge vector is in \(\mathbb{B}^{\omega}\), so that the interactions are unweighted, undirected, simple graphs, then for any family of subgraphs we will be selecting from at most \(\omega\leq {n\choose 2}\) edges.

Representing a vector as a sparse combination of a known set of vectors (also known as ``atoms'') is called \emph{sparse approximation}.

\subsection{Problem Specification}\label{sec-fp-problem}

Sparse approximation of a vector \(\mathbf{x}\) as a representation \(\mathbf{r}\) using a dictionary of atoms (columns of \(D\)) is specified more concretely as \autocite{EfficientimplementationK_Rubinstein2008}:
\begin{equation}\phantomsection\label{eq-sparse-approx}{\mathbf{\hat{r}} = \operatorname*{argmin}_{\mathbf{r}}{\|\mathbf{x}-D\mathbf{r} \|_2^2} \quad \text{s.t.} \|\mathbf{r}\|_0\leq N }\end{equation}
where \(N\) serves as a sparsity constraint (at most \(N\) non-zero entries).
This is known to be NP-hard, though a number of efficient methods to approximate a solution are well-studies and widely used.
Solving the Lagrangian form of Equation~\ref{eq-sparse-approx}, with an \(\ell_1\)-norm in place of \(\ell_0\), is known as \emph{Basis Pursuit}\autocite{SparseApproximateSolutions_Natarajan1995}, while greedily solving for the non-zeros of \(\mathbf{r}\) one-at-a-time is called \emph{matching pursuit}\autocite{Matchingpursuitstime_Mallat1993}.
In that work, each iteration selects the atom with the largest inner product \(\langle \mathbf{d}_{i'},\mathbf{x}\rangle\).

We take an approach similar to this, but with the insight that the inner product will not result in desired sparsity (namely, a tree).
Our dictionary in this case will be the set of edges given by \(B\) (see Section~\ref{sec-subgraph-dists}), while our sparsity is given by the relationship of the numbers of nodes and edges in a tree:
\begin{equation}\phantomsection\label{eq-sparse-approx-tree}{
\mathbf{\hat{r}} = \operatorname*{argmin}_{\mathbf{r}}{\|\mathbf{x}-B^T\mathbf{r} \|_2^2} \quad \text{s.t.}\quad \|\mathbf{r}\|_0 = \|\mathbf{x}\|_0 - 1
}\end{equation}

There are some oddities to take into account here.
As a linear operator (see Section~\ref{sec-lin-ops}), \(B^T\) takes a vector of edges to node-space, counting the number of edges each node was incident to.
This means that, even with a ground-truth set of interactions, \(B^T\) would take them to a new matrix \(X_{\text{deg}}(i,j):I\times J \rightarrow \mathbb{N}\), which has entries of the number of interactions each individual in observation \(i\) was involved in.
While very useful for downstream analysis (see Section~\ref{sec-fp-preprocess}), the MSE loss in Equation~\ref{eq-sparse-approx-tree} will never be zero, since \(X_{\text{deg}}\) entries are not boolean.
Large-degree ``hub'' nodes in the true graph would give a large residual, and the adjoint would subsequently fail to remove the effect of \(B^T\) on the edge vectors.

It might be possible to utilize a specific semiring, such as \((\min,+)\), to enforce inner products (see Section~\ref{sec-products}) that take us back to a binary vector.
This would be more than a simple hack, and belies a great depth of possible connection to the problem at hand.
It is known that ``lines'' (arising from equations of the inner product) in tropical projective space \emph{are trees}.\autocite{tropicalGrassmannian_2004}
In addition, the tropical equivalent to Kirchoff's polynomial (which counts over all possible spanning trees), is the direct computation of the minimum spanning tree.\autocite{TropicalKirchhoffsformula_Jukna2021}
For treatment of sparse approximation using tropical matrix factorization, see \textcite{Sparsedataembedding_Omanovic2021}

Instead, we will take an empirical bayes approach to the estimation of sparse vectors.\autocite{EmpiricalBayesianStrategy_Wipf2007}
As a probabilistic graphical model, we assume each observation is emitted from a (tree-structured) Markov Random Field \emph{only defined on the activated nodes}.
This is underdetermined (any spanning tree could equally emit the observed activations), so we use an empirical prior as a form of shrinkage: the co-occurrences of nodes across all observed activation patterns.
This let's us optimize a likelihood from Equation~\ref{eq-edgevec-prob}, for the distribution of spanning trees on the subgraph of \(G^*\) inducted by \(\mathbf{x}\).
\begin{equation}\phantomsection\label{eq-sparse-approx-tree}{
\mathbf{\hat{r}} = \operatorname*{argmax}_{\mathbf{r}}{\mathcal{L}(\mathbf{r}|\mathbf{x})} \quad \text{s.t.}\quad \mathbf{r}\sim \mathcal{T}(G^*[\mathbf{x}])
}\end{equation}

\subsection{Maximum Spanning (Steiner) Trees}\label{sec-steiner}

The point estimate \(\hat{\mathbf{r}}\) is therefore the mode of a distribution over trees, which is precisely the maximum spanning tree.\autocite{EfficientComputationExpectations_Zmigrod2021}
If we allow the use of all observations \(X\) to find an empirical prior for \(\mathbf{r}\), then we can calculate a value for the mutual information for the activated nodes, and use this to directly calculate the Chow-Liu estimate.
One algorithm for finding a maximum spanning tree is Prim's\autocite{Datastructuresnetwork_Tarjan1983}, which effectively performs the matching pursuit technique of greedily adding an edge (\emph{i.e.}, non-zero entry in our vector) one-by-one.
In this way, we effectively \emph{do} perform matching pursuit, but minimizing the KL-divergence between observed node activations and a tree-structured MRF limited to those nodes, alone (rather than the mean-square-error).

However, the mode of the tree distribution is not strictly the one that uses mutual information as edge weights.
There is reason to believe that edge weights based on pairwise joint probabilities might also be appropriate.
Namely, the Hunter-Worsley bound for unions of (dependent) variables says that the sum of marginal probabilities over-counts the true union of activations (including by dependence relations).
This alone would be known as Boole's inequality, but the amount it overcounts is \emph{at most} the weight of the maximum spanning tree over pairwise joint probabilities.\autocite{upperboundprobability_Hunter1976}
Adding the tree of joint co-occurrence probabilities is the most conservative way to arrive at the observed marginals from the probability of at least one node occurring (which could then be the ``root'').

Finally, we realize that the problem statement (``find the maximum weight tree on the subgraph'') is not the same as an MST, per-se, but rather the so-called ``Steiner Tree'' problem.
In other words, we would like our tree of interactions to be of minimum weight on a node-induced subgraph \emph{of the true graph}.
The distribution of trees that our interactions are sampled from should be over the available edges in the recovered graph, \emph{which we do not yet have}.
Thankfully, a well-known algorithm for approximating the (graph) Steiner tree problem instead finds the minimum spanning tree over the \emph{metric closure} of the graph.\autocite{fastalgorithmSteiner_Kou1981}

This metric closure is a complete graph having weights given by the shortest-path distances between nodes.
While we don't know those exact values either, we do have the fact that the distance metric implied by the forest kernel (in Equation~\ref{eq-regulap}) is something of a relaxation of shortest paths.
In the limit \(\beta\rightarrow 0\), \(Q\) is proportional to shortest path distances, while \(\beta\rightarrow\infty\) instead gives commute/resistance distances.\autocite{Semisupervisedlearning_Avrachenkov2017}
And that kernel is counting the probability of co-occurrence on trees in any random spanning forest!

All this is to say that node co-occurrence measures are more similar to node-node distances in the underlying graph, \emph{not estimators of edge existence}.
But we can use this as an empirical prior to approximate Steiner trees that \emph{are on the true graph}.
For another recent application of sampling steiner trees to reconstruct node dependencies (though not for global network reconstruction), see \textcite{RobustCascadeReconstruction_Xiao2018}.

\section{Forest Pursuit}\label{sec-FP}

Instantiating the above, we propose \emph{Forest Pursuit}, an relatively simple algorithm for correction of clique-bias under a spreading process assumption.

\subsection{Algorithm Summary}\label{algorithm-summary}

Once again, we assume \(m\) observations of activations over \(n\) nodes, represented as the design matrix \(X:I\times J \rightarrow \mathbb{B}\).
Like GLASSO, we assume that a Gram matrix (or re-scaling of it) is precomputed, for the non-streaming case.

Based on the discussion in Note~\ref{nte-cs} we will use the cosine similarity as a degree-corrected co-occurrence measure, with node-node distances estimated as \(d_K=-\log{\text{Ochiai}(j,j')}\).\footnote{
  Note that any kernel could be used, given other justification, though anecdotal evidence has the negative-log-Ochiai distance performing marginally better than MI distance or Yule's \(Q\).}

For each observation, the provided distances serve to approximate the metric closure of the underlying subgraph induced by \(\mathbf{x}\).
This is passed to an algorithm for finding the minimum spanning tree.
Given a metric closure, the MST in turn would be an approximation of the desired Steiner tree that has a total weight that will be a factor of at most \(2-\tfrac{2}{\|\mathbf{x}\|_0}\) worse than the optimal tree.\autocite{fastalgorithmSteiner_Kou1981}
For \(\|\mathbf{x}\|_0 \ll n\) (\emph{i.e.}, many fewer authors-per paper than total authors), this error bound will be close to 1 (perfect reconstruction).
The error factor bound approaches double the weight (in the worst-case) of the optimal tree as the expected number of authors-per-paper grows to infinity.

After the point estimates for \(\mathbf{r}\) have been calculated as trees, we can use the desire-path beta-binomial model (Equation~\ref{eq-desirepath-binom}) to calculate the overall empirical Bayes estimate for \(\hat{G}\).
As a prior for \(\alpha\), instead of a Jeffrey's or Laplace prior, we bias the network toward maximal sparsity, while still retaining connectivity.
In other words, we assume that \(n\) nodes only need about \(n-1\) edges to be fully connected, which implies a prior expected sparsity of
\begin{equation}\phantomsection\label{eq-min-connect}{
\alpha^*=\frac{n-1}{\tfrac{1}{2}n(n-1)} = \frac{2}{n}
}\end{equation}
which we can use as a sparsity-promoting initial value for \(\text{Beta}(\alpha^*,1-\alpha^*)\).

\begin{algorithm}[htb!]
\caption{Forest Pursuit}
\label{alg-fp}
\begin{algorithmic}[1]
\Require $X\in \mathbb{B}^{m\times n}, d_K\in \mathbb{R}_{\geq 0}^{n\times n}, 0<\alpha<1$
\Ensure $R \in \mathbb{B}^{m \times {n\choose 2}}$
\Procedure{ForestPursuitEdgeProb}{$X, d_K, \alpha$}
  \State $R \gets $\Call{ForestPursuit}{$X, d_K$}
  \State $\hat{\alpha}_m\gets$\Call{DesirePathBeta}{$X,R, \alpha$}
  \State \textbf{return} $\hat{\alpha}_m$
\EndProcedure
\Procedure{ForestPursuit}{$X, d_K$}
  \State $R(\cdot,\cdot) \gets 0$
  \For{$i\gets 1, m$}\Comment{\textit{each observation}}
    \State $\mathbf{x}_i \gets X(i,\cdot)$
    \State $R(i,\cdot) \gets $\Call{PursueTree}{$\mathbf{x}_i, d_K$} 
  \EndFor
  \State \textbf{return} $R$
\EndProcedure
\Procedure{PursueTree}{$\mathbf{x}, d$}
\Comment{\textit{Approximate Steiner Tree}}
  \State $V \gets \{v\in \mathcal{V} | \mathbf{x}(\mathcal{V})=1\}$
  \Comment{\textit{activated nodes}}
  \State $T \gets$\Call{MST}{$d[V,V]$}
  \Comment{\textit{e.g., Prim's Algorithm}}
  \State $\mathbf{u,v}\gets \{(j,j')\in J\times J | T(j,j')\neq 0\}$
  \State $\mathbf{r} \gets e_n(\mathbf{u,v})$
  \Comment{\textit{unroll tree adjacency}}
  \State \textbf{return} $\mathbf{r}$ 
\EndProcedure
\Procedure{DesirePathBeta}{$X,R, \alpha$}
  \State $\mathbf{s} \gets \sum_{i=1}^m R(i,e)$
  \State $\sigma \gets \sum_{i=1}^m X(i,j)$
  \State $\mathbf{k} \gets e_n(\sigma\sigma^T)$
  \Comment{\textit{co-occurrence counts}}
  \State $\hat{\alpha}_m \gets \alpha + \frac{\mathbf{s}-\alpha \mathbf{k}}{\mathbf{k}+1}$
  \State \textbf{return} $\hat{\alpha}_m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg-fp} outlines the algorithm in pseudocode for reproducibility.

\subsection{Approximate Complexity}\label{sec-fp-complexity}

The Forest Pursuit calculation presented in Algorithm~\ref{alg-fp} assumes an initial value for the distance matrix, which is similar to the covariance estimate that is pre-computed (as an initial guess) for GLASSO.
Therefore we do not include the matrix multiplication for the gram matrix in our analysis, at least in the non-streaming case.
Because every observation is dealt with completely independently, the FP estimation of \(R\) is linear in observation count.
It is also trivially parallelizeable,\footnote{Although, no common python implementation of MST algorithms are as-yet vectorized or parallelized for simultaneous application over many observations.
  We see development of non-blocking MSTs in common analysis frameworks as important future work.}
and the bayesian update for \(\hat{\alpha}_m\) can be performed in a streaming manner, as well.

Each observation requires a call to ``PursueTree'', which involves an MST call for the pre-computed subset of distances on nodes activated for that observation.
Note the use of ``MST'' here requires any minimum spanning tree algorithm, \emph{e.g.}, Prim's, Kruskal's, or similar.
It is recommended to utilize Prim's in this case, however, since Prim's on a sufficiently dense graph can be made to run in \(O(n)\) time for \(n\) activated nodes by using a \emph{d}-tree for its heap queue.\autocite{Datastructuresnetwork_Tarjan1983}
Since we are always using the metric closure, Prim's will always run on a complete graph.

Importantly, this means that FP's complexity does not scale with the size of the network, but only the worst-case activation count of a given observation, \(O(s_{\text{max}})\), where \(s_{\text{max}}=\max_i{(\|X(i,\cdot)\|_0)}\)
We say this is \emph{approximately} constant in node size:

\begin{itemize}
\tightlist
\item
  The total number of nodes is typically a \emph{given} for a single problem setting
\item
  In many domains, the basic \emph{spreading} rate of diffusion model (\emph{e.g.}, \(R_0\), or heat conductivity), does not scale with the total size of an observation
\end{itemize}

That last point means that constant scaling with network size is generally down to the domain in question.
For instance, a heat equation simulated over a small area, having a given conductivity, will not have a different conductivity over a larger area; conductivity is a material property.
Similarly, a virus might have a particular basic reproduction rate, or a set of authors might have a static distribution over how many collaborators they wish to work with.
The former is down to viral load generation, and the latter a sociological limit: a bigger department usually does not imply more authors-per-paper by itself.

Similar to Equation~\ref{eq-min-connect}, we might reasonably assume that the expected degree of nodes is roughly constant with network size \emph{i.e.}, an inherent property of the domain.
So, the density of activation vectors (as a fraction of all possible edges) is going go scale with the inverse of \(n\).
This makes our process, which is linear in activation count, out to be \emph{constant} \(O(1)\) in network size.
Then, if \(\bar{s}\) is the expected non-zero count of each row of \(X\), the final approximate complexity of FP is \(O(m\bar{s})\).\footnote{
  In our reference implementation, which uses Kruskal's algorithm, the theoretical complexity is likewise \(O(m\bar{s}^2\log{\bar{s}})\), though in our experience the values of \(\bar{s}\) are small enough to not impact the runtime significantly.}

\section{Simulation Study}\label{sec-FP-experiments}

To test the performance of FP against other backboning and recovery methods, we have developed a public repository \href{https://github.com/rtbs-dev/affinis}{\texttt{affinis}} containing reference implementations for FP, along with many co-occurrence and backboning techniques.
The library contains source code and examples for many of the presented methods, and more.

In addition, to support the community and provide for a standard set of benchmarks for network recovery from activations, the \href{https://github.com/rtbs-dev/mendr}{\texttt{MENDR}} reference dataset and testbench was developed.
To make reproducible comparison of recovery algorithms easier, \texttt{MENDR} includes hundreds of randomly generated networks in several classes, along with random walks sampled \emph{on those networks}.
It can also be extended through community contribution, using data versioning to allow consistent comparison between different reports and publications over time.

\subsection{Experimental Method}\label{experimental-method}

For each algorithm shown in Table~\ref{tbl-methods}, every combination of the parameters in Table~\ref{tbl-mendr} was tested. 30 random graphs for each of nodes were tested, which was repeated again for each of three separate kinds of global graph structure.
Every algorithm that could be supplied a prior via additive smoothing is shown in Table~\ref{tbl-methods} as ``\(\alpha\)?: Yes'', and a minimum-connected (tree) sparsity prior was supplied \(\alpha=\tfrac{2}{n}\).
The others, esp.~GLASSO, do not have a \(\tfrac{\text{count}}{\text{exposure}}\) form, and could not be easily interpreted in a way that allowed for additive smoothing.
However, since the regularizaiton parameter for GLASSO is often critical for finding good solutions, a 5-fold cross validation was performed for each experiment to select a ``best'' value, with the final result run using that value.
While this does have a constant-time penalty for each experiment, the reconstruction accuracy is significantly improved with this technique, and would reflect common practice in using GLASSO for this reason.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3729}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1525}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1695}}
  >{\centering\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1695}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1356}}@{}}
\caption{Summary of algorithms compared}\label{tbl-methods}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
abbrev.
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\alpha\)?
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
class
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
source
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Algorithm
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
abbrev.
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(\alpha\)?
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
class
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
source
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Forest Pursuit & FP & Yes & local & - \\
GLASSO & GL & No & global & \autocite{Sparseinversecovariance_Friedman2008,Structureestimationdiscrete_Loh2012} \\
Ochiai Coef. & CS & Yes & local & \autocite{Measuresecologicalassociation_Janson1981} \\
Hyperbolic Projection & HYP & No & local & \autocite{Scientificcollaborationnetworks._Newman2001} \\
Doubly-Stochastic & eOT & Yes & resource & \autocite{twostagealgorithm_Slater2009,Sinkhorndistanceslightspeed_Cuturi2013} \\
High-Salience Skeleton & HSS & Yes & resource & \autocite{Robustclassificationsalient_Grady2012} \\
Resource Allocation & RP & Yes & resource & \autocite{Bipartitenetworkprojection_Zhou2007} \\
\end{longtable}

The three classes of random graphs represent common use cases in sparse graph recovery.
In addition, the block and tree graphs are types we expect GLASSO to correctly recover in this binary setting.\autocite{Structureestimationdiscrete_Loh2012}
The block graphs of size \(n\) were formed by taking the line-graph of randomly generated trees of size \(n+1\).\\
Trees were randomly generated using Prüfer sequences as impelmented in NetworkX \autocite{ExploringNetworkStructure_Hagberg2008}.
To simulate possible social networks and other complex systems that show evidence of preferential attachment, scale-free graphs were sampled through the Barabási--Albert (BA) model, which was randomly seeded with a re-attachment parameter \(m\in\{1,2\}\)\autocite{EmergenceScalingRandom_Barabasi1999}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.4000}}
  >{\centering\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.6000}}@{}}
\caption{Experiment Settings (\texttt{MENDR} Dataset)}\label{tbl-mendr}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
values
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
parameters
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
values
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
random graph \textbf{kind} & Tree, Block, BA\((m\in\{1,2\}\)) \\
network \textbf{\(n\)-nodes} & 10,30,100,300 \\
random \textbf{walks} & 1 sample \(m\sim\text{NegBinomial}(2,\tfrac{1}{n})+10\) \\
random walk \textbf{jumps} & 1 sample \(j\sim\text{Geometric}(\tfrac{1}{n})+5\) \\
random walk \textbf{root} & 1 sample \(n_0 \sim \text{Multinomial}(\textbf{n},1)\) \\
random \textbf{seed} & 1, 2, \ldots{} , 30 \\
\end{longtable}

Every graph has a static ID provided by MENDR, along with generation and retrieval code for public review.
New graphs kinds and sizes are simple to add for future benchmarking capability.

\subsection{Metrics}\label{metrics}

To compare each algorithm consistently, several performance measures have been included in the MENDR testbench.
They are all functions of the True Positive/Negative (TP/TN) and False Positive/Negative (FP/FN) values.

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-precision} 

\vspace{-3mm}\textbf{Note \ref*{nte-precision}: Precision (P)}\vspace{3mm}

Fraction of positive predictions that are true, also called ``positive predictive value'' (PPV) \[P= \frac{TP}{TP+FP}\]

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-recall} 

\vspace{-3mm}\textbf{Note \ref*{nte-recall}: Recall (R)}\vspace{3mm}

Fraction of true values that were returned as positive.
Also called the TP-rate (TPR), and has an inherent trade-off with precision. \[R=\frac{TP}{TP+FN} \]

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-mcc} 

\vspace{-3mm}\textbf{Note \ref*{nte-mcc}: Matthews Correlation Coefficient (MCC)}\vspace{3mm}

Balances all of TP,TN,FP,FN.
Preferred for class-imbalanced problems (like sparse recovery) \autocite{statisticalcomparisonMatthews_Chicco2023}
\[\frac{TP\cdot TN - FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\]

\end{tcolorbox}

\begin{tcolorbox}[enhanced jigsaw, toprule=.15mm, left=2mm, leftrule=.75mm, rightrule=.15mm, colframe=quarto-callout-note-color-frame, arc=.35mm, colback=white, opacityback=0, bottomrule=.15mm, breakable]

\quartocalloutnte{nte-fm} 

\vspace{-3mm}\textbf{Note \ref*{nte-fm}: Fowlkes-Mallows (F-M)}\vspace{3mm}

Geometric mean of Precision and Recall, as opposed to the F-Measure that returns the harmonic mean.
Also known to be the limit of the MCC as TN approaches infinity\autocite{MCCapproachesgeometric_Crall2023}, which is useful as TN grows with \(n^2\) but TP only with \(n\).
\[\sqrt{P\cdot R}\]

\end{tcolorbox}

Because this work is focused on unsupervised performance, specifically for the use of these algorithms by analysts investigating dependencies, we opt to calculate TP,TN,FP,FN at every unique edge probability/strength value returned by each algorithm.
Then, because we do not know a priori which threshold level will be selected by an analyst in the unsupervised setting we need a mechanism to select from or aggregate these values to come up with an overall score.
In the supervised case we would have access to the ground truth, so the ``optimal'' threshold can be reported.
Though this is not a reliable unsupervised scenario, it can give an idea of the upper-limit for an algorithm's performance.
For this purpose, we report the maximum MCC value as ``MCC-max''.

Another common approach was discussed in Note~\ref{nte-ot}, where we find the maximum allowable edge sparsity before the graph would otherwise become disconnected.
This balances the desire to achieve sparsity while also enforcing topological constraints on the graph, so that we cannot improve our precision artificially by isolating components of the overall network.
This ``minumum-connected'' threshold was calculated for each network estimate, and reported for MCC as ``MCC-min''.

Finally, a more interesting approach might be to define a distribution over likely thresholds, and find the expected value over, say, MCC, which incorporates our uncertainty in thresholding.
In the future this could be informed by domain-specific thresholding tendencies, but for now we will take a conservative approach and report the expected values E{[}MCC{]} and E{[}F-M{]} over all unique threshold values (\emph{i.e.} a flat prior over thresholds).
To consistently compare the expected values, we transform the thresholds for every experiment to the range \([\epsilon, 1-\epsilon]\), to avoid division-by-0 at the extremes.

Another common approach to score aggregation is the Average Precision Score (APS).
This is not the average precision over the thresholds however, but instead the expected precision over the possible recall values achievable by the algorithm.
It is approximating the integral under the parametric P-R curve, instead of the thresholds themselves.
\[\text{APS} = \sum_{e=1}^{\omega} P(e)(R(e)-R(e-1))\]
where \(P(e)\) and \(R(e)\) are the precision and recall at the threshold set by the edge \(e\), in rank-order.
This is more commonly done for supervised settings, however, and will report a high value as long as \emph{any} threshold is able to return a both a high precision and a high recall, simultaneously.

\subsection{Results - Scoring}\label{results---scoring}

The median results, along with the inter-quartile-range (IQR), are summarized accross all experiments in Table~\ref{tbl-fp}.

\begin{table}

\caption{\label{tbl-fp}Comparing median(IQR) scores for various metrics}

\centering{

\subcaption{\label{tbl-fp-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-tbl-fp-output-1.pdf}}

}

}

\end{table}%

A visualization of these results are shown in Figure~\ref{fig-fp-overall}, with a specific callout to compare E{[}MCC{]}, MCC-min, and MCC-max in Figure~\ref{fig-thres-mcc}.
Only FP is able to report MCC and F-M values with medians over about 0.5, regularly reaching over 0.8.
GLASSO is clearly the second-best at recovery in these experiments, though for scale-free networks the improvement over simply thresholding the Ochiai coefficient is negligible.
For APS, both GLASSO and Ochiai are equally able to return high scores, indicating at least one threshold for each that performed well.
However, the best-case MCC-max for FP is still marginally better than GLASSO, along with the MCC-min having a better median score \emph{as well as} a much tighter uncertainty range.\\
To address the APS discrepancy, a simple mechanism for FP to perform equally well is discussed in Section~\ref{sec-fpi}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-fp-overall-output-1.pdf}}

}

\caption{\label{fig-fp-overall}Comparison of MENDR recovery scores}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-thres-mcc-output-1.pdf}}

}

\caption{\label{fig-thres-mcc}Comparison of scores for expected, optimal, and min-connected MCC}

\end{figure}%

Breaking down the results by graph kind in Figure~\ref{fig-fp-compare}, we see the remarkable ability of FP to dramatically outperform every other algorithm in MCC and F-M, showing remarkable accuracy \emph{together with stability} over the set of threshold values.
This is indicative of FP's ability to more directly estimate the support of each edge, with lower values occurring only when co-occurrences aren't being consistently explained with the same set of incidences.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-fp-compare-output-1.pdf}}

}

\caption{\label{fig-fp-compare}Comparison of MENDR Recovery Scores by Graph Type}

\end{figure}%

Another important capability of any recovery algorithm is to improve its estimate when provided with more data.
Of course, this also will depend on other factors, such as the dimensionality of the problem (network size), and specifically for us, whether \emph{longer} random walks makes network inference better or worse.

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-mendr-trends-output-1.pdf}}

}

\subcaption{\label{fig-mendr-trends-1}Trend: MCC vs network size}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-mendr-trends-output-2.pdf}}

}

\subcaption{\label{fig-mendr-trends-2}Trend: MCC vs observation count}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-mendr-trends-output-3.pdf}}

}

\subcaption{\label{fig-mendr-trends-3}Trend: MCC vs random-walk length}

}

\caption{\label{fig-mendr-trends}Score trends vs problem scaling}

\end{figure}%

As Figure~\ref{fig-mendr-trends} shows, FP is positively correlated with \emph{all three}.
Most importantly, the trend for FP is strongest as the number of observations increases, which is not a phenomenon seen in the other methods.
In fact, it appears that count-based methods' scores are negatively correlated with added random walk length and added observations.
Only HYP and CS scores are shown in Figure~\ref{fig-mendr-trends}, but all other tested methods (other than FP and GLASSO) show the same trend.

However, because the graph sampling protocol includes \(n\) in the distributions for the observation count and random-walk length, we additionally performed a linear regression on the (log) parameters. The \emph{partial residual} plots are shown in Figure~\ref{fig-partials-mcc}, which shows the trends of each variable after controlling for the others.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-partials-mcc-output-1.pdf}}

}

\caption{\label{fig-partials-mcc}Partial Residuals (regression on E{[}MCC{]})}

\end{figure}%

This analysis indicates that \emph{all} methods should likely increase in their performance when extra observations are added, though FP does this more efficiently than either CS or GLASSO.
Interestingly, CS is largely unaffected by network size, compared to FP and GL, though GL performs the worst in this regard.
However, it is in the random-walk length that we see the benefit of dependency-based algorithms.
The Ochiai coefficient suffers dramatically as more nodes are activated by the spreading process, since this means the implied clique size grows by the square of the number of activations.
FP remains unaffected by walk-length, while (impressively) GLASSO appears to have a marginal boost in performance when walk lengths are high.

\subsection{Results - Runtime Performance}\label{results---runtime-performance}

For both Forest Pursuit and GLASSO, runtime efficiency is critical if these algorithms are going to be adopted by analysts for backboning and recovery.
Figure~\ref{fig-runtime} shows the (log-)seconds against the same parameters from before.
For similar sized networks, FP is consistently taking 10-100x less time to reach a result than GLASSO does.
Additionally, many of the experiments led to ill-conditioned matrices that failed to converge for GLASSO under any of the regularization parameters tests (the ``x'' markers in Figure~\ref{fig-runtime}).
As expected, the number of observations plot shows a clear limit in terms of controlling the lower-bound of FPs runtime, since in this serial version every observation runs one more call to MST.
On the other hand, GLASSO appears to have significant banding for walk length and observation counts, likely indicating dominance of network size for its runtime.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-runtime-output-1.png}}

}

\caption{\label{fig-runtime}Runtime Scaling (Forest-Pursuit vs GLASSO)}

\end{figure}%

To control for each of the variables, and to empirically validate the theoretical analysis in Section~\ref{sec-fp-complexity}\}, a regression of the same three (log-)parametwers was performed against (log-)seconds.
The slopes in Figure~\ref{fig-partials-runtime}, which are plotted on a log-log scale, correspond roughly to polynomial powers in linear scale.
In regression terms, we are fitting the log of \[y_{\text{sec}} = ax_{\text{param}}^\gamma\] so that the slope in a log-log plot is \(\gamma\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-05-forest-pursuit_files/figure-latex/-content-codefigs-results-fig-partials-runtime-output-1.pdf}}

}

\caption{\label{fig-partials-runtime}Partial Residuals (regression on computation time)}

\end{figure}%

In a very close match to our previous analysis, the scaling of FP is almost entirely explained by the observation count and random-walk length, alone: the coefficient on network size shows constant-time scaling.
Similarly, the scaling with observation count is very nearly linear time, as predicted.
The residuals show non-linear behavior for the random-walk length parameter, which would make sense, due to the theoretical \(\|E\|\log\|V\|\) scaling of Kruskal's algorithm.
At this scale, \(n\log n\) and \(n^2\log n\) complexity might appear smaller than linear time, due to the log factor.
GLASSO hardly scales with random walk length, and only marginally with observation count.
In typical GLASSO, the observation count has already been collapsed to calculate the empirical covariance matrix, so its effects here might be due instead to the cross-validation and the need to calculate empirical covariance for observation subsets.
The big difference, however, is GLASSO scaling in significantly superlinear time---almost \(O(n^2)\).
This is usually the limiting factor for analyst use of such an algorithm in network analysis more generally.

\chapter{Modifications \& Extensions}\label{sec-extend}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``Sure, we could include all the of variables in the analysis, but do-calculus (or similar) will usually tell us that we don't have to.''}

\hfill -- Richard McElreath\\
\doublespacing
\end{quote}

\end{minipage}

\end{flushright}

\emph{Forest Pursuit} is a flexible base for adding or modifying behavior as needed.
In this chapter we demonstrate two ways to extend it, along with a probabilistic model to serve as a foundation for future extensions.
First, we address the perceived shortcoming of FP compared to GLASSO in terms of APS score, showing that a simple modification to the way FP weights edge probability recovers the same performance as GLASSO (at the cost of other scores).
Then we use the implicit causal dependency tree structure of each observation, together with the Matrix Forest Theorem \autocite{MatrixForestTheorem_Chebotarev2006,Countingrootedforests_Knill2013} to more generally define our generative node activation model.
This leads to a generative model for binary activation data as rooted random spanning forests on the underlying dependency graph.
Finally, we use this model to continue where FP left off by alternating between estimation of \(R\) an \(B\), in a method we call \emph{Expected Forest Maximization} (EFM).
EFM shows small but consistent improvement in performance over FP, at the cost of computation (and unknown convergence) time.

\section{Forest Pursuit Interaction Probability}\label{sec-fpi}

Without using stability selection\autocite{StabilitySelection_Meinshausen2010}, GLASSO is not directly estimating the ``support'' of the edge matrix, but the strength of each edge.
to do similar with FP, we could directly estimate the frequency of edge occurrence using \(R(i,e)\) marginal averages, rather than conditioning on co-occurrence.
Simply multiplying each FP edge probability by the co-occurrence probability of each node-node pair gives this as well, which we call FPi: the direct ``interaction probability'' for each pair of nodes.

\subsection{Simulation Study Revisited}\label{simulation-study-revisited}

By doing this simple re-weighting, FPi actually beats GLASSO's median APS for the dataset, but at the cost of MCC and F-M scores (which both drop to between FP and GLASSO), as Figure~\ref{fig-fpi} demonstrates.
Similarly, the individual breakdown by graph kind in Table~\ref{tbl-fpi} shows a similar pattern, with FPi coming close to GLASSO for scale-free networks, but exceeding it for trees and matching for block graphs.
Still, the difference is small enough, and at such a significant penaly to MCC and F-M scores over a variety of thresholds, that it is hard to recommend the FPi re-weighting unless rate-based edge analysis is desired, e.g.~if Poisson or Exponential occurrence models are desired.

\begin{figure}

\begin{minipage}{\linewidth}

\begin{table}[H]

\caption{\label{tbl-fpi}Comparing median(IQR) scores against FPi}

\centering{

\subcaption{\label{tbl-fpi-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-results-tbl-fpi-output-1.pdf}}

}

}

\end{table}%

\end{minipage}%
\newline
\begin{minipage}{\linewidth}

\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-results-fig-fpi-output-1.pdf}}

}

\caption{\label{fig-fpi}FPi shows improved APS, optimal MCC, and MCC (min-connect)}

\end{figure}%

\end{minipage}%

\end{figure}%

Consistent with the increased APS, however, we also see an improvement in MCC-min and MCC-max.
If there is a domain-driven reason to select \emph{specific} thresholds, or if GLASSO is known to provide reasonable results for a given problem, FPi stands a good chance of improving on the optimal or min-connected MCC score.

\subsection{Simulation Case Study}\label{simulation-case-study}

To illustrate what is going on, we have selected two specific experiments as a case study, in Figure~\ref{fig-pr-curves}.
In the first, \texttt{BL-N030S01}, a 30-node block graph with 53 random walk samples, has FP performing worse than GLASSO and Ochiai, in terms of APS (which is reported in the legends).
We see that FP shows high precision, which drops off significantly to increase recall at all.
Only a few edges had high probability (which is usually desirable for sparse approximation), and some of the true edges were missed this way.
However, FPi rescaling makes rarer edges fall off earlier in the thresholding, letting the recall rise by dropping rare edges, rather than simply the low-confidence ones.

In the second, \texttt{SC-N300S01} is a 300-node scale-free network with 281 walks.
Both FP and FPi show significantly better recovery capability, since enough walks have visited a variety of nodes to give FP better edge coverage.
In this graph, no algorithm comes within 0.25 of FP's impressive 0.88 APS, especially with 300 nodes and fewer than that many walks.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/images/PR.pdf}}

}

\caption{\label{fig-pr-curves}P-R curves for two experiments}

\end{figure}%

\section{Generative Model for Correlated Binary Data}\label{sec-lfa-like}

Generating multivariate, \emph{correlated} binary data is of interest across many of the fields discussed in this work, since such models can be used as foundations structural inference (especially if they have a defined likelihood function).
One of the foundational generative models for correlated binary data makes direct use of the multivariate-normal's precision matrix buy sampling directly from it and thresholding values to be binary \autocite{generationcorrelatedartificial_Leisch1998}.
In a related sense, the multivariate probit (MVP) is used to analyse such data, treating the likelihood of observed binary outcomes as probabilities from the cumulative distribution function of the underlying multivariate normal.

Another generative model is the \emph{Ising Model}, which has long been used as a tool for modeling particle spins in lattice structures, but in the inverse setting is seeing increased application in data science for recovering network structure \autocite{Inversestatisticalproblems_Nguyen2017}
While techniques like Gibbs sampling might be used to sample from the Ising distribution, a common technique to estimate the structure of the lattice is to perform logistic regression on each node's activation values using the activation of all other nodes.
The equivalence of this Multivariate Logit (MVL) to the Ising model is discussed in \textcite{Assortmentoptimizationgiven_Vasilyev2025}

As was briefly mentioned in Chapter~\ref{sec-lit-review}, another class of models makes use of the bipartite structures implied by the binary activation data.
These degree sequence methods can synthesize other bipartite structures (and therefore, generate binary observations) that match a desired degree distribution \autocite{fastballfastalgorithm_Godard2022,Randomlysamplingbipartite_Neal2023}.
If the case that the true number of nodes is unknown (or should be approximately inferred), a class of models known as the \emph{Indian Buffet Process} \autocite{IndianBuffetProcess_Griffiths2011} is s distribution over all binary matrices with a finite number of rows.
It can therefore sample entire datasets, like the degree sequence models, but with more flexibility in inferring the number of necessary nodes.

For all of these generative models, there is a lack of model that takes into account the underlying information available to us when we \emph{know} that the activation structures arise from a \emph{spreading process} (like random walk visits).
Here we propose a compound distribution with a reasonable likelihood that can not only model and generate correlated binary outcomes, but also be inferred using the techniques first discussed for \emph{Forest Pursuit}.
We accomplish this by exploiting the isometry between distributions over random spanning forests and spanning tree distributions over an augmented graph.
We are able to derive a simple likelihood and a rapid inference scheme using the Matrix Forest Theorem of Chebotarev and Shamis \autocite{MatrixForestTheorem_Chebotarev2006}

\subsection{Marked Random Spanning Forest (RSFm) distribution}\label{marked-random-spanning-forest-rsfm-distribution}

Every spanning forest on a graph described by \(Q\) can be thought of as an equivalent spanning tree over a graph augmented with an extra ``source'' node, which is connected to every other node with a weight \(\tfrac{1}{\beta}\).
Sampling random spanning trees on the augmented graph is equivalent to random spanning forests on the original.\autocite{Semisupervisedlearning_Avrachenkov2017}
We can use this fact to create a distribution for node activation sets based on \emph{co-occurrence on a rooted tree}.

A ``rooted tree'' is a tree with a marked node.
In Figure~\ref{fig-inject-plan} we see this illustrated, where a randomly sampled tree on the graph augmented with R leads to many subtrees in the original graph.
Marking one node (d) at random selects the tree that contains (d,h,e), which corresponds to record \(x_1\) back in Figure~\ref{fig-obs-set}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-graphs-fig-inject-plan-output-1.pdf}}

}

\caption{\label{fig-inject-plan}Dissemination plan as rooted RST on augmented graph}

\end{figure}%

Note that the marked node does not necessarily need to be the one that the source ``injected'' to, since the observed activations set is equivalent for any of (d,h,e) being marked.
This is an important symmetry when we will not know which node ``actually'' started each cascade, during inference.\\
It means that the node activation set and the graph structure are conditionally independent, given a sampled spanning tree on the augmented graph.
Sampling efficiently from a spanning tree distribution is a well studied problem, and we can use that efficiency in combination with a node-marking (categorical) distribution to formulate an overall distribution for node activation.

Therefore, RSFm distribution models the probability of emitting node \(j\) in the \(i\)-th observation as the probability of occurring in the same tree as a marked ``root'' node \(\phi_{ij}\), given a graph of \(z_E\) edges and a source ``distance'' parameter \(\beta\).
Since the graph is assumed to be independent of the trees sampled on it, we can use the law of total (conditional) probability:\\
\[
P(x_{ij}|z_E, \phi_{ij}) = \sum_{T\in\mathcal{T}_{+R}}P(x_{ij}| T,\phi_{ij}) P(T|z_E)
\]

Incredibly, the probabilities for \(P(T|z_E)\) and \(P(x|T,\phi)\) all have closed form representations.
The spanning tree distribution discussed elsewhere\autocite{BayesianSpanningTree_Duan2021,EfficientComputationExpectations_Zmigrod2021} can be used to motivate a spanning forest distribution, which is based on the Laplacian of the augmented graph \(L_+\).\footnote{Here, \(L_+(G)\) and \(L_+(G/e)\) represent the Laplacian of \(G_{+R}\) and \(G_{+R}\) without edge \(e\), respectively.}
This means that the probability of a spanning tree in the augmented graph is proportional to the probability of a forest in the original, or, \(P(T\in\mathcal{T}_{+R}|z_E) \propto P(F\in\mathcal{F}|z_E)\).

\[
P(x_{ij}|z_E, \phi_{ij}) = \sum_{F\in\mathcal{F}}P(x_{ij}| F,\phi_{ij}) P(F|z_E)
\]

Using this equivalence on Kirchoff's matrix tree theorem gives the Chebotarev-Shamis Forest Theorem\autocite{MatrixForestTheorem_Chebotarev2006} for the partition function over spanning forests \(Z_\mathcal{F}\), which gives a closed form for Equation~\ref{eq-subgraph-prob} over all spanning forests (not just spanning trees on a subgraph).
If \(\mu(F)\) is a measure on forests, e.g.~the product of weights of edges, then:

\[
\begin{gathered}
P(F|z_E) = \frac{1}{Z_\mathcal{F}} \mu(F)\\
Z_\mathcal{F} = \det{(I+\beta L)}
\end{gathered}
\]

There are a variety of techniques already cited for sampling from this distribution, because of the connection with spanning trees on an augmented graph.
This means we can sample from RSFm with only one modification from normal spanning tree distributions:

\begin{itemize}
\tightlist
\item
  Generate random spanning trees on \(G_{+R}\), according to \(P(F|z_E)\).
\item
  Uniformly sample a marked node \(\phi\).
\item
  Activate all nodes connected to \(\phi\) in the induced spanning forest.
\end{itemize}

To do inference, however, we may again use the forest kernel \(Q\), which gives us probabilities for node co-occurrence on trees in a spanning forest.
We only need to combine entries of Q for the observed nodes to determine the likelihood of an observation.
\(\sum_\mathcal{F} P(x_{ij}|F,\phi_{ij})\) would be the probability of a node occurring (or not) on the same subtree as the marked node, and as previously discussed, this is just \(Q(u\in V_i,\phi)\) for each node that is activated, and \(1-Q(v\notin V_i,\phi)\), for each that is not.
Note that this assumes we marginalize over a uniformly distributed \(\phi\).

\subsection{Model Specification}\label{sec-lfa-gibbs}

The overall model that fills out the gaps left in Equation~\ref{eq-empirical-model} has the form:

\[
\begin{aligned}
\pi_{e\in E} &\sim \operatorname{Beta}_E(\alpha, 1-\alpha)     \\
z_{e\in E} &\sim \operatorname{Bernoulli}_E(\pi_e)             \\
\phi_{i\in I, j\in J} &\sim \operatorname{Categorical}_I(\theta) \\
x_{i\in I,j\in J} &\sim \operatorname{RSFm}_K(\beta, z_e, \phi_n) \\
\end{aligned}
\]

The above still has the problem of needing an estimate for \(Q=(I+\beta L_z)\) based on the graph described by the active edges in \(z\).
It could be done with a monte-carlo sampler, though the nested likelihood may prove difficult.

The conditional independence mentioned previously could lend itself to a collapsed Gibbs-sampling scheme.
Each observation is a marked node and a sampled random spanning forest, which can equivalently be described as a sampled spanning tree over the activated nodes, like originally discussed in Equation~\ref{eq-subgraph-prob}.
This works because we can marginalize out the equal marked-node probability \(\phi\) if we assume a uniform probability of selection for each node, and because we can jointly use the desire-path model to derive an estimate for z\_E from the \emph{internal} edge samples of RSFm.

Beginning with the FP point estimate, each edge in every spanning subtree can be efficiently resampled according to the \emph{Bayesian Spanning Tree} distribution from \textcite{BayesianSpanningTree_Duan2021}.
Once every edge in a tree has been resampled, the overall estimate for the desire path network can be updated, and sampling can continue.
This would be very similar to the way collapsed Gibbs sampling works for Latent Dirichlet Allocation\autocite{Latentdirichletallocation_Blei2003}, but with edges selected from a spanning forest distribution instead of ``topics'' from a multinomial.
Derivations and implementation of such a scheme is left for future work.

\section{Expected Forest Maximization}\label{expected-forest-maximization}

Another possibility is to approach the problem as a kind of matrix factorization, jointly estimating \(B\) and \(R\) in an alternating manner.
Where \emph{Forest Pursuit} was an empirical Bayes estimate for \(R\), alternating from there between \(B\) and \(R\) leads to a simple Expectation Maximization scheme.

\subsection{Factorization \& Dictionary Learning}\label{factorization-dictionary-learning}

Jointly finding a sparse representation of data using a linear combination of basis vectors (called a ``dictionary''), \emph{and} finding an optimal dictionary with which to do the embedding, is called ``sparse dictionary learning''.
One of the original proposed methods to solve this was the Method of Optimal Directions (MOD)\autocite{Methodoptimaldirections_Engan1999}, which uses a sparse ``pursuit''-like method to find the representation vectors, and then uses that representation to find \(\hat{B}\gets XR^+\).\footnote{
  \(R^+\) is the pseudo-inverse of \(R\).\\
}
However, our desire path estimation for independent (arcsine) Bernoulli edges gives an efficient workaround to needing the pseudo-inverse.
Based on our interpretation of the forest kernel, counting co-occurrences on trees is equivalent to estimating the inverse of the regularized graph laplacian, anyway.
Each Forest Pursuit estimate can yield a new distance \(d_Q(u,v)\), which can be used as a regularized shortest-path distance for small enough \(\beta\).
Then, new steiner tree estimates will approximate the modes of each spanning tree distribution, which can be used for a new desire-path estimate of the edges, and so-on.

Probabilistically, we are finding the expectation over spanning forests (collapsing over uniform \(\phi\)) in the form of calculating \(Q\), and then maximizing the likelihood of each observation through \emph{Forest Pursuit}.
The proposed algorithm \emph{Expected Forest Maximization} is outlined in Algorithm~\ref{alg-efm}.

\begin{algorithm}[htb!]
\caption{Expected Forest Maximization (EFM)}
\label{alg-efm}
\begin{algorithmic}[1]
\Require $X\in \mathbb{B}^{m\times n}, d_K\in \mathbb{R}_{\geq 0}^{n\times n}, 0<\alpha<1$
\Ensure $R \in \mathbb{B}^{m \times {n\choose 2}}$
\Procedure{EFM}{$X, d_K, \alpha, \beta, \epsilon$}
  \State $R \gets $\Call{ForestPursuit}{$X, d_K$}
    \State $\hat{\alpha}_m\gets$\Call{DesirePathBeta}{$X,R, \alpha$}
    \While {$\|\hat{\alpha}-\alpha\|_{\infty}>\epsilon$}
        \State $\alpha_m \gets \hat{\alpha}_m$
        \State $Q \gets (I+\beta L_{\text{sym}}(\alpha_m))^{-1}$
        \State $d_K \gets d_Q$
        \State $R \gets $\Call{ForestPursuit}{$X, d_K$}
        \State $\hat{\alpha}_m\gets$\Call{DesirePathBeta}{$X,R, \alpha_m$}
    \EndWhile
  \State \textbf{return} $\hat{\alpha}_m$
\EndProcedure
\end{algorithmic}
\end{algorithm}

In practice, we limit iterations to less than 100, though that limit is not reached in our testing.
We use a small \(\beta=10^{-3}\) to approximate shortest path distances, and we can do so \emph{within} the forest pursuit loop to avoid inverting large matrices.
Instead of \(d_K\), the graph structure itself (once an initial estimate is made) can be passed to \emph{Forest Pursuit}, where each subgraph can estimate \(d_{Ki}\) independently.
After all, shortest paths in a subgraph when we assume \emph{no hidden nodes} will be the same as in the global graph.
Note as well the use of the symmetric normalized Laplacian \(L_{\text{sym}}\).
We use this to enable the splitting of inversions like this, because the global Laplacian has higher possible degrees than the subgraph, and we do not wish to bias Steiner tree estimation by node degree.
Not doing so leads to rapid divergence of the loss function, as all MSTs will tend toward star-graphs around the highest-degree node.

\subsection{EFM Simulation Study}\label{efm-simulation-study}

One-shot Forest Pursuit appears to perform quite well, so it's useful to quantify the expected gain in performance by repeating it an unknown number of times.
There are no generic guarantees for EM convergence, though anecdotally the number of iterations was limited in our experiments to under a thousand, and that limit was never hit while using a covergence parameter of \(\epsilon=1\mathrm{e}-5\).

The distribution of E{[}MCC{]} score change vs.~FP is shown in Figure~\ref{fig-efm-mcc}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-results-fig-efm-mcc-output-1.pdf}}

}

\caption{\label{fig-efm-mcc}Change in Expected MCC (EFM vs FP)}

\end{figure}%

While useful, it's not clear whether individual edges are more likely to be ``true'' edges, \emph{given} a bigger change in EFM score.
To test this, a logistic regression was performed for every experiment in MENDR against the true edge values, using the change in scores on those edges between FP and EFM as training data.
To avoid overfitting, a significant amount of regularization was applied, chosen using 5-fold crossvalidation.
The coefficients for all experiments are shown as a histogram in Figure~\ref{fig-efm-logits}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-results-fig-efm-logits-output-1.pdf}}

}

\caption{\label{fig-efm-logits}Logistic Regression Coef. (EFM - FP) vs.~(Ground Truth)}

\end{figure}%

The graph kind did not make a significant difference to EFM improvement, but overall log-odds improvement is very low.Still, the value is positive accross the entire dataset, so EFM does have a very small-but-nonzero impact on improving edge prediction.

The runtime graphs can also be updated, with EFM shown in Figure~\ref{fig-efm-runtime} and Figure~\ref{fig-efm-partials-runtime} against FP and Glasso.
EFM still ran significantly faster than GLASSO in this region.
However, the scaling with network size is no longer constant-time, especially since convergence used above is the max-abs error, which requires that \emph{every node} reach a minimum level of convergence and might take much longer, overall.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-results-fig-efm-runtime-output-1.png}}

}

\caption{\label{fig-efm-runtime}Runtime Scaling (Forest-Pursuit vs GLASSO)}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part2/2-06-latent-forest-alloc_files/figure-latex/-content-codefigs-results-fig-efm-partials-runtime-output-1.pdf}}

}

\caption{\label{fig-efm-partials-runtime}Partial Residuals (regression on computation time)}

\end{figure}%

\part{Applications \& Case Studies}

\chapter{Qualitative Application of Relationship Recovery}\label{sec-qual}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``Please tell me what this is all about, papa. Please, please tell me what this is. Please, what is it?''}

\hfill -- Cosette, Les Misérables (1998)\\
\doublespacing
\end{quote}

\end{minipage}

\end{flushright}

A more realistic test of \emph{Forest Pursuit's} behavior is in unsupervised, qualitative analysis of complex systems and networks.
In this chapter we will recreate two well-known bipartite datasets from network science, and compare several network recreation methods on them.
As we have discussed previously\autocite{PerceivedAssortativitySocial_Fisher2017}, there is some interest in quantifying network assortativity for social networks, and whether the positive assortativity effect is a methodological artifact.
Assuming an analyst is interested in social network \emph{dependencies}---like which nodes are directly \emph{influencing} which others---we provide a glimpse at how correcting for clique bias with \emph{Forest Pursuit} (FP) affects assortativity.

In the second case study, we also investigate how centrality measures can be adversely affected when clique-bias is not accounted for, especially when appearance in cliques is known to be rare for certain node types.

\section{Network Science Collaboration Network}\label{network-science-collaboration-network}

Following in the steps of \textcite{Findingevaluatingcommunity_Newman2004}, we recreate a network from co-authorships in works cited by two literature review papers, \textcite{StructureFunctionComplex_Newman2003} and \textcite{ComplexnetworksStructure_BOCCALETTI2006}.
The list of all cited papers was reconstructed via web-of-science queries, and these were used to construct author ``activation'' observations.\footnote{
  An additional paper (\textcite{Coherentnoisescale_Sneppen1997}) was added to ensure the the largest connected component contained necessary communities that were originally missing compared to \textcite{Findingevaluatingcommunity_Newman2004}.}
Nodes and papers were retained only if they had two or more papers and authors listed, respectively.
A baseline co-authorship network was initially constructed using the largest connected component, and a modularity-maximizing community detection algorithm {[}\textcite{Findingcommunitystructure_Clauset2004};Findingevaluatingcommunity\_Newman2004{]} was used to highlight community structure.
As recommended in \textcite{Findingevaluatingcommunity_Newman2004}, we also show a reduced course-grained ``quotient'' graph of the communities and their interconnection to help visualize and understand the large-scale structure each network presents.\\
This baseline network can be seen in Figure~\ref{fig-netsci-cooc}, where (to keep with the treatment in \textcite{Findingevaluatingcommunity_Newman2004}) we have added a quotient network of the communities themselves.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-netsci-cooc-output-1.pdf}}

}

\caption{\label{fig-netsci-cooc}134 Network scientists connected by co-authorship}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-netsci-tree-output-1.pdf}}

}

\caption{\label{fig-netsci-tree}Chow-Liu tree of NetSci collaborator dependency relationships}

\end{figure}%

The detected assortativity \(r=0.059\) is small, but positive.
As expected, each community tends to have a dominant ``clique'', since many members of these groups tend to mutually author papers together (simultaneously).

However, if we are after a ``social network'' that describes social influence of individuals with respect to others, then we are hoping to recover collaborative \emph{dependency relationships}.

\begin{itemize}
\tightlist
\item
  Who is causing who to join papers?
\item
  Who are the central ``collaborators'' that influence the paper writing of many others?
\end{itemize}

With some background knowledge of how university labs tend to work (with students, post-docs, advisors, colleagues, etc.), we might not believe that every member of a community has significant influence on the writing of every other member.
So, to estimate a dependency network of collaboration relationships, we could turn to a Chow-Liu tree, which is shown in Figure~\ref{fig-netsci-tree}.

This dependency is much more sparse, and indeed the assortativity coefficient has dropped to \(r=-0.251\).
In academic writing, we might even expect a lowered assortativity, since students that may not go on to be prolific in their original field would still seek out prolific advisors, initially.
Unfortunately, enforcing a tree structure has some negative side effects.
Overlaying the original community structure from Figure~\ref{fig-netsci-cooc} onto the tree and calculating a quotient graph shows that the community structure is impacted by the change.
Many of the communities are ``unrolled'' into long chains of authors, since trees cannot allow small loops or cycles.
This makes for a shortest-path distance between community hubs (in the same field) to be upwards of 9-10 jumps, which goes against the scale-free/small-world nature we expect from social systems.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-netsci-fp-output-1.pdf}}

}

\caption{\label{fig-netsci-fp}Forest Pursuit estimate of NetSci collaborator dependency relationships}

\end{figure}%

With \emph{Forest Pursuit}, we can attempt to correct for ``clique bias'' in the measurement of dependency relationships, while allowing for flexibility in the global network structure.
The FP recovered collaborator graph is shown in Figure~\ref{fig-netsci-fp}.

The communities from the co-occurrence network are entirely preserved, with a nearly identical community structure in the graph quotient compared to the co-occurrence graph.
But now we see an assortativity that is close to zero (slightly negative at \(r=-0.069\)).
This brings the assortativity more in line with the World Wide Web networks, or even close to results for random preferential attachment networks that are zero in the limit\autocite{AssortativeMixingNetworks_Newman2002}.

Relative to the tree network, there are not so many long chains, as many authors have been allowed to ``loop-back'' and have relationships with nearby colleagues, reducing the path distance between communities in the process.
Still, like the trees, FP tends to reduce the degree of each node, which is summarized in Figure~\ref{fig-netsci-degree}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-netsci-degree-output-1.pdf}}

}

\caption{\label{fig-netsci-degree}Degree distributions of FP vs co-occurrence social networks}

\end{figure}%

From a domain modeling perspective, it might be reasonable to assume that any given author has 2-3 influential collaboration relationships shown, especially considering students and post-docs are likely to constitute a good number of nodes.
Even from a logistics perspective (during a given time window working with an advisor) the number of times a student asks/is asked to participate in a paper has to be limited, given average publishing rates.\\
Only rare individuals would have upwards of 10 influential relationships, with a mean closer to the 2-3 of the FP network, rather than a full quarter having 10 and the median being 4-5 (as in the co-occurrence network).
In addition, recall that this is a social network derived from literature review paper citations, not exhaustive inventories of each author's work, so every influential relationship should not be shown in this sample.

So, which network is preferable?
It depends.

The premise of FP is to find a max. likelihood network \emph{such that} the observed activations (authors on a paper) arise from a random walk along dependencies (author relationships of \emph{influence} causing others to join).
If influential (dependency) relationships are being measured, then the logistics of maintaining 2-3 of them seems more realistic than 5-10 (especially only from the sampled papers).
Additionally, we expect students having few influential relationships would attach preferentially to advisors with more, so a positive assortativity might be problematic.
FP agrees with these conclusions, making it \emph{consistent} with that set of a priori domain beliefs vs.~the co-occurrence network alone.
These kinds of qualitative assessments are not rigorous, but the modeling questions required to choose FP (or not) go a long way to reducing error in \emph{trueness}.
They help modelers validate whether the recovered network was made in a consistent way with their domain knowledge.

\section{Les Misérables Character Network}\label{les-misuxe9rables-character-network}

Another famous network derives from \textcite{StanfordGraphBaseplatform_Knuth1993} via mining character co-occurrences in chapters of \emph{Les Misérables} (1862), Victor Hugo's sprawling saga of {[}in{]}equality and {[}in{]}justice in 19th-century France.
This dataset is often reported as being inherently a \emph{graph}, but we have reconstructed the underlying bipartite observations for the purpose of removing clique bias from the network reconstruction.
In the original network, an extra count is added to the weight of an edge every time a character occurs in a chapter with another.
Once again, we have only retained characters/chapters that appeared with two or more chapters/characters, respectively.
In Figure~\ref{fig-lesmis-cooc}, we run the same community detection scheme to improve the visual understandability of the network, along with inset quotient graphs.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-lesmis-cooc-output-1.pdf}}

}

\caption{\label{fig-lesmis-cooc}Les Miserables character co-occurrence network}

\end{figure}%

Note that the communities demonstrate clear clique-like behavior.
On one hand, this does make assessment of clusters easier, since characters in roughly the same scenes are densely grouped together.
However, this network makes it difficult to parse relationships, such that backboning would become necessary.

We should also ask \emph{what we want} out of this ``social'' network.
Another way we might think about a social network of characters is ``which characters \emph{influence} the appearance (or not) of which other characters?''
By extension, ``which characters are \emph{significant}, in the sense that they dictate the appearance of more characters than others?''
From a domain modeling perspective, this is like asking \emph{how} an author is deciding which characters to include in a chapter.
By not correcting for clique-bias, we are implicitly assuming that Victor Hugo would be setting out to write a chapter and immediately writes down a list of every character, independently, should appear.
Instead, it's likely that the desired appearance of certain characters in a chapter \emph{leads to} the inclusion of others, as the plot requires.

Because the inclusion of certain characters \emph{leads to} the inclusion of others (by this model), we can reasonably model the authorial ``character inclusion process'' as \emph{spreading} from character to character.
With this in mind, we apply Forest Pursuit to correct for clique-bias, and show the resulting dependency network (with the original community partition) in Figure~\ref{fig-lesmis-fp}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-lesmis-fp-output-1.pdf}}

}

\caption{\label{fig-lesmis-fp}Les Miserables character dependency network (Forest Pursuit)}

\end{figure}%

The network edge probabilities have been thresholded in the same manner as the DS (minimum connectivity) filter, only including necessary edges to still retain overall connectivity.
As before, our community structure shown in the quotient graph has been preserved, even with the significant edge density reduction.

Applying these networks, we might wish to distinguish through them which characters are ``main'' and which are ``supporting'', though more on a gradient than in a binary/classification sense.
One way to do this is through centrality measures \autocite{Mathematicsnetworks_Newman2018,atlasaspiringnetwork_Coscia2021}, which estimate the ``importance'' of nodes in various ways.
Eigenvector centrality, specifically, finds the importance of nodes relative to the importance of each node's neighbors\footnote{
  Regularized eigenvector centrality is equivalent to PageRank centrality, \emph{i.e.} the seminal work from \autocite{PageRankCitationRanking_Page1999}, of Google fame.}
It does this by estimating the stationary distribution of a random walk on the network (what is the probability after many jumps of ending up in each node?)

The topology of a network greatly impacts the centrality measure recovered.
In fact, a well-known result shows that much of the variance in eigenvector/PageRank centrality values can be explained with node in-degree alone.\autocite{ApproximatingPageRankDegree_Fortunato}
Of course, we \emph{do} tend to see nodes with high-degree as \emph{hubs}, and more important for it---that is, we \emph{would} if we weren't so concerned with having to deal with clique bias in the network.
The tendency of eigenvector centrality to rely on degree (and therefore reward densely-connected cliques with high importance) prevents us from using it over more expensive methods like \emph{betweenness centrality}.
Instead, we might be able to differentiate between high-degree nodes in cliques and ``actual'' hubs by controlling for clique bias ahead of time with \emph{Forest Pursuit}.

In Figure~\ref{fig-lesmis-centrality} we show a bump-plot of the ranks of the top 15 most central nodes for both the original co-occurrence network and the FP network estimate.
In the original network, centrality rewards characters appearing in mutual cliques as being central, especially the ``revolutionary'' group (shown in orange in Figure~\ref{fig-lesmis-fp} and Figure~\ref{fig-lesmis-cooc}).
Are these really the most important characters?
Important characters like Cosette and Fantine are nowhere near the top.
We wish to find important characters in the sense that they have influence over the appearance of many other characters.
In that case, even Javert (though being an important recurring character) largely plays the role of generic law enforcement throughout, not necessarily pulling other characters in or out of scenes with him as much as others like Marius.
More than Javert, Marius plays central motivating roles across several distinct social groups throughout the novel.
All of these problems are corrected to some degree in the FP estimate.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-07-qualitative_files/figure-latex/-content-codefigs-qualitative-fig-lesmis-centrality-output-1.pdf}}

}

\caption{\label{fig-lesmis-centrality}Changes in character centrality ranking for FP vs co-occurrence}

\end{figure}%

Why are the centrality levels for Fantine, Eponine, and Cosette so much higher in FP? Why were they so low in the first place?
This brings up another benefit of correcting clique-bias: estimating character importance \emph{despite} systematic interaction-reduction bias from authors.
As made famous by the so-called ``Bechdel test'' \autocite{MediaMarginsPopular_Savigny2015}, authors tend to systematically reduce the agency and interaction rates of female characters, an effect that has been studied from many angles covering multiple centuries \autocite{genderagencygap_Stuhler2024}.
We highlight every female character in Figure~\ref{fig-lesmis-centrality} (bold/thicker paths), which shows that the co-occurrence network had one single female character in the top 15 most central (Eponine, ranked 14th).
This is because their degrees are overshadowed by large cliques!
By correcting for clique-bias, \emph{every} female character significantly increases in centrality ranking
This way, the occurrence of many characters can be explained \emph{through} these female characters, rather than assuming each character separately connects with every other, biasing importances toward large groups of men.

In context, titles of entire books and volumes of \emph{Les Misérables} are named after women we would consider crucial to the plot (\emph{e.g.} Fantine, Cosette, and Eponine).\\
As \textcite{genderagencygap_Stuhler2024} indicates, however, authors are likely to under-represent their agency and interactions with other characters throughout the story, so networks based primarily on co-occurrence will likewise systematically underestimate their importance.

\chapter{Recovery from Working Memory \& Partial Orders}\label{sec-ordered}

\begin{flushright}

\begin{minipage}{.7\linewidth}

\singlespacing

\begin{quote}
\emph{``So an analytically astute observer would find that a salmon is more closely related to a camel than it is to a hagfish. On what basis, then, do we justify grouping salmon under the label fish, but not camels?''}

\hfill -- Samuel Beswick (sarcastically)\\
\end{quote}

\doublespacing

\end{minipage}

\end{flushright}

This whole time we have assumed that the ordering of nodes was unknown, or at the very least unreliable.
However, there are frequently cases, especially in text processing applications, where we have some sense of an ordering on activations.
By \emph{partial order} on a set (a ``poset''), we mean that all elements are either comparable as greater (before) or less (after), or incomparable.
The set of posets is therefore precisely isomorphic to the set of \emph{directed acyclic graphs}, based on reachability.

Our original example with authors might be thought of as a poset: (i) precedes/asks (f) and (j), (j) precedes/asks (b), but (b) and (f) are incomparable.
We don't know if (i) asked (f) before or after (j) asked (b).\footnote{
  Interestingly, in the case where the node activations \emph{are} given a total order (in the form of \emph{timestamps}), \textcite{Inferringnetworksdiffusion_GomezRodriguez2012} derive an algorithm called \emph{NetInf}.
  It utilizes sums over minimum spanning trees that satisfy time constraints, similar to a special case of Forest Pursuit where all activations have a total ordering.}
We have updated the original example with explicit partial orders in Figure~\ref{fig-obs-tree}. Note that nodes in some lists could be re-arranged while keeping the partial order the same (keeping all arrows pointing to the right).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-graphs-fig-obs-tree-output-1.pdf}}

}

\caption{\label{fig-obs-tree}Observations as partially-ordered sets}

\end{figure}%

Sadly our co-authorship example does not often include the (partial) order of author additions,\footnote{though some domains do imbue special meaning to the listed author order, a fact that might be interesting to investigate in future network recovery efforts!} but other common network recovery problems do have an inherent order to them.
A very common need is to recover concept association networks, whether from lists of tags or directly from a corpus of written language.
What's needed is an assumption on how the observed partial order of concepts is generated.
\textcite{ForagingSemanticFields_Hills2015} proposes a ``foraging'' mechanism, so that concepts get sequentially recalled from ``semantic patches'' of nearby concepts in memory.
The partial order comes from our ability to maintain more than one concept in working memory, so that the next concept can be ``foraged'' from any of the other recently recalled ones\autocite{magicalnumberseven_Miller1956,Dynamicsearchworking_Hills2012}.

In this section, we briefly cover a method for network inference by \textcite{Humanmemorysearch_Jun2015} that utilizes partial order information from ordered lists of concepts, called INVITE.
We use it to demonstrate improvement over bag-of-words and markov assumptions for downstream technical language processing \autocite{Technicallanguageprocessing_Brundage2021} tasks, as originally demonstrated in \autocites{UsingSemanticFluency_Sexton2019}[and][]{OrganizingTaggedKnowledge_Sexton2020}

Finally, we show that using \emph{Forest Pursuit} for partially ordered data can still be quite useful for network backboning, and for a fraction of the computational cost.
We investigate a network recovery task from verbal/semantic fluency data \autocite{Estimatingsemanticnetworks_Zemla2018}, which involves recovery of a network of animal relationships from memory and recall experiments.
Even without directly using partial order information, proper data preparation along with previously-discussed (un-ordered) recovery methods can lead to significantly improved network backboning and analysis capability

\section{Technical Language Processing with INVITE}\label{technical-language-processing-with-invite}

Maintenance work orders are often represented as categorized data, though increasingly quantitative use of a technician's descriptions is being pursued by maintenance management operations \autocite{BenchmarkingKeywordExtraction_Sexton2018,CategorizationErrorsData_Sexton2019}.
Tags are another way to flexibly structure this otherwise ``unstructured'' data, which Table~\ref{tbl-mwo} shows in comparison to more traditional categorization.

\begin{table}

\caption{\label{tbl-mwo}Maintenance Work Order as categorized vs.~tagged data}

\centering{

\subcaption{\label{tbl-mwo-1}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-graphs-tbl-mwo-output-1.pdf}}

}

}

\end{table}%

Whether entered directly or generated from text by keyword extraction, the tags will tend to have ordering information readily available.
A traditional way to model this kind of text is through either bag-of-words (the co-occurrence node activation data already discussed) or as a sequence of order-n markov model emissions.
An order-n markov model \(\text{MC}n\) estimates the probability of observing the \(i\)th item \(t_i\) in a sequence \(T\) as
\[
P(t_i|T) \approx P(t_i | t_{i-1}, \cdots,t_{i-n})
\]

Unlike the clique bias from before, assuming markov jumps for each observation leads to a different kind of bias, with higher precision but reduced recall as shown in Figure~\ref{fig-stack-markov}.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-graphs-fig-stack-markov-output-1.pdf}}

}

\caption{\label{fig-stack-markov}Partial-order edge measurements with Markov assumption}

\end{figure}%

Without knowing the underlying dependency relationships, it's difficult to estimate which edges were used by a random-walker, since subsequent visits in memory to a ``tag'' are not being reported once a technician first adds it.
\autocite{Humanmemorysearch_Jun2015} call this an ``Initial-visit-emitting'' random walk, or INVITE for short.
To more accurately recover the network, they suggest maximizing the absorption probability for each step of a partial order, individually, knowing which nodes have already been activated.

\subsection{Optimizing absorbing-state probabilities}\label{optimizing-absorbing-state-probabilities}

Say the set of components or concepts that have a corresponding tag in our system is denoted by the node-set \(N\).
A user-given set of \(T\)~\footnote{While some sources use ``tagging'' as a proxy for a set of strictly
  un-ordered labels (as in multi-label classification), we preserve
  the mechanism by which the tags were generated in the first place,
  i.e., in a \emph{specific} order.} for a specific record can be denoted as a Random Walk (RW) trajectory \(\mathbf{t}=\{t_1, t_2, t_3, \cdots t_{T}\}\), where \(T\leq N\).
This limit on the size of \(T\) assumes tags are a set of unique entries.
Any transitions between previously visited tags in \(\mathbf{t}\) will not be directly observed, making the transitions observed in \(\mathbf{t}\) strictly non-Markov, and allowing for a \emph{potentially infinite} number of possible paths to arrive at the next tag \emph{through previously visited ones}.
Instead of directly computing over this intractable model for generating \(\mathbf{t}\), the key insight from the original INVITE paper~\autocite{Humanmemorysearch_Jun2015} comes from partitioning \(\mathbf{t}\) into \(T-1\) Markov chains with absorbing states.
Previously visited tags are ``transient'' states, and unseen tags are ``absorbing''.
It is then possible to calculate the absorption probability into the \(k\) transition (\(t_k \rightarrow t_{k+1}\)) using the \emph{fundamental matrix} of each partition.
If the partitions at this jump consist of \(q\) transient states with transition matrix among themselves \(\mathbf{Q}^{(k)}_{q\times q}\), and \(r\) absorbing states with transitions into them from \(q\) as \(\mathbf{R}^{(k)}_{q\times r}\), the Markov transition matrix \(\mathbf{M}^{(k)}_{n\times n}\) has the form
\begin{equation}\phantomsection\label{eq-trans-matrix}{
\mathbf{M}^{(k)} =
    \begin{pmatrix}
        \mathbf{Q}^{(k)}  & \mathbf{R}^{(k)} \\
        \mathbf{0}        & \mathbf{I}
    \end{pmatrix}
}\end{equation}

Here \(\mathbf{0}\), \(\mathbf{I}\) represent lack of transition between/from absorbing states.
It follows from \autocite{RandomWalksElectric_Doyle2000} that the probability \(P\) of a chain starting at \(t_k\) being absorbed into state \(k+1\), is given as
\begin{equation}\phantomsection\label{eq-absorb}{
\begin{gathered}
    P\left(t_{k+1} \middle| t_{1:k},\mathbf{M}\right) =
        \left.\mathbf{N}^{(k)}R^{(k)}\right|_{q,1}\\
\mathbf{N} = \left( \mathbf{I}-\mathbf{Q} \right) ^{-1}
\end{gathered}
}\end{equation}

The probability of being absorbed at \(k+1\) conditioned on jumps \(1:k\) is thus equivalent to the probability of observing the \(k+1\) INVITE tag.
If we approximate an a priori distribution of tag probabilities to initialize our chain as \(t_1\sim\text{Cat}(n,\theta)\) (which could be empirically derived or simulated), then the likelihood of our observed tag chain \(\mathbf{t}\), given a transition matrix, is:
\[
\mathcal{L}\left(\mathbf{t}| \theta, \mathbf{M}\right) =
        \theta(t_1)\prod_{k=1}^{T-1} P\left(t_{k+1}\,\middle|\ t_{1:k},\mathbf{M}\right)
\]

Finally, if we observe a set of tag lists \(\mathbf{C} = \left\{ \mathbf{t}_1, \mathbf{t}_2, \cdots, \mathbf{t}_{c} \right\}\), and assume \(\theta\) can be estimated independently of \(\mathbf{M}\), then we can frame the problem of structure mining on observed INVITE data as a minimization of negative log-likelihood.
A point estimate for our association network given \(\mathbf{M}\) can found as:
\[
    \mathbf{M}^* \leftarrow \operatorname*{argmin}_{\mathbf{M}} \quad
    \sum_{i=1}^{C}
    \sum_{k=1}^{T_i-1}
        -\log \mathcal{L} \left(t^{(i)}_{k+1} \middle| t^{(i)}_{1:k},\mathbf{M}\right)
\]

This (deeply nested) likelihood can now be optimized using standard solvers, and our reference implementation uses stochastic gradient descent via PyTorch \autocite{AutomaticdifferentiationPyTorch_Paszke2017}.\footnote{
  The n-gram markov transition models (MC1,MC2) trained for comparison vs INVITE were trained using \texttt{pomegranate}\autocite{Pomegranatefastflexible_Schreiber2018}.}

\subsection{Application: Mining Excavator MWOs}\label{application-mining-excavator-mwos}

To assess the applicability of the INVITE-based similarity measure to real-world scenarios, we apply our model to tags annotated for a mining dataset pertaining to 8 similarly-sized excavators at various sites across Australia~\autocite{Whyautonomousassets_Hodkiewicz2017,Cleaninghistoricalmaintenance_Hodkiewicz2016}.

The tags were created by a subject-matter expert spending 1 hour of time in the annotation assistance tool \texttt{nestor}~\autocite{NestorToolNatural_Sexton2019}, using a methodology outlined in a previous benchmarking study for that annotation method~\autocite{BenchmarkingKeywordExtraction_Sexton2018}.

That work compared the ability of tags to estimate survival curves and mean time-to-failure, when compared with a custom-designed keyword extraction tool based on classifying the maintenance issues by subsystem.
While certain sets of tags were able to predict time-to-failure with high accuracy for certain subsystems, a key problem identified in that work is knowing a priori which tags best indicate when a subsystem is failing?

\begin{quote}
\emph{Which tags best represent a given subsystem?}
\end{quote}

Some tags are sufficient (albeit unnecessary) conditions to indicate a subsystem.
That the ``hydraulic'' tag indicates a \emph{Hydraulic System} MWO is obvious, but so might a ``valve'' tag---``hydraulic'' is implied but not present.
Consequently, we can treat the problem of assigning tags to represent a subsystem as a semi-supervised multi-class classification problem in the style of \textcite{Semisupervisedlearning_Avrachenkov2017}.
Like in that work, we need to know a selection tag\(\rightarrow\)subsystem assignments, as well as network of weighted tag-tag edges.

Then, if we can compare the semi-supervised tag classifications to a ground-truth classification by a human annotator (which are available for the excavator dataset thanks to \textcite{sharedreliabilitydatabase_Ho2015}), we can assess the ability of each network to capture the human annotator's internal/cognitive tag relationship structure.

\subsection{Which network assigns tags to subsystems most like a domain expert?}\label{which-network-assigns-tags-to-subsystems-most-like-a-domain-expert}

To test the ability of the similarity measures to accomplish this, the top three most common subsystems in the data were used as classes, namely, Hydraulic System, Engine, and Bucket.
The tags ``hydraulic'', ``engine'', and ``bucket'' were assigned to those subsystems as known labels, respectively.
Tags were filtered to only include ones of high-importance and sufficient information: only work orders containing at least 3 unique tags, and only tags that occurred at least 10 unique times within the those work orders, were included for this analysis (\(C=263\) MWOs, \(N=40\) tags).
Then the number of occurrences for every tag can be compared across subsystems, giving each tag a ground-truth multinomial (categorical) probability distribution for occurring within each subsystem.
We determine ground-truth classification labels as subsystems that account for \(\geq60\%\) of each tag's occurrences.
Tags more balanced than that are considered ``unknown subsystem''.

To perform semi-supervised classification on the recovered relationship graphs, we use a label-spreading algorithm described in~\autocite{Learninglocalglobal_Zhou2004}, which itself was inspired by spreading activation networks in experimental psychology~\autocite{architecturecognition_Anderson2013,Observationphasetransitions_Shrager1987}. The result of this algorithm is tags having a score for each class, with the classification being the maximally scored class for that tag. These class assignments can then be compared to the ground-truth labels, which we have done by weighted macro-averaging of the \(F_1\)-score (see Figure~\ref{fig-excavate-f1kl}).

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/images/F1_KL_ntags3_freq5_topn50_thres60_saveTrue.pdf}}

}

\subcaption{\label{fig-excavate-f1kl}}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/images/ternary_ntags3_freq5_topn50_thres60_saveTrue.pdf}}

}

\subcaption{\label{fig-excavate-ternary}}

}

\caption{\label{fig-excavate-results}Semisupervised MWO Tag Classification with Network Label Propagation}

\end{figure}%

The classification of the INVITE-based similarity measure far outperforms the other measures as a preprocessor for label-spreading, when measured by average \(F_1\)-score.
However, since these ``classifications'' are actually thresholded multinomial distributions (with some tags regularly occurring across multiple subsystems), how do we know if an underlying structure has actually been recovered, rather than simply a black-box classifier that happens to perform well at this setting?

To begin answering this question, we might ask whether the relative scores returned by label-spreading are similar to the original multinomial distributions themselves, rather than the overall classification.
To find out, we use softmax normalization\footnote{For visualization, a temperature parameter was added to softmax, and this was optimized for minimum KLD via Brent's method~\autocite{algorithmguaranteedconvergence_Brent1971} for each similarity measure independently to provide an equal footing for comparison.}
to transform each tag's scores into a ``predicted multinomial'', before finally calculating the Kullback-Leibler divergence (KLD) between the true and predicted multinomials for every tag.
The total KLD, summed over all tags, is also shown in Figure~\ref{fig-excavate-ternary}, along with positions of each tag's multinomial as projected onto the 2-simplex for the true and \(F_1\)-optimal predicted distributions.
Once again, the INVITE performs much better at this task, over a wide range of \(\sigma\) (lower is better).

A reason for the performance disparity can be seen in the simplex projections: recovered topology via INVITE-similarity does a much better job of separating the three classes, while not letting any single tag overcompensate by dominating a subsystem's area.
Even the ``unknown'' tags are correctly placed roughly between Bucket and Hydraulic System regions, reflecting the true topology of the system.

\section{Forest Pursuit Animal Network}\label{sec-animal-fluency}

For our last case study, we return to Forest Pursuit as a useful tool for analysis even when that might mean ignoring partial ordering information.
Note that Equation~\ref{eq-absorb} is effectively the same as a (\(\beta=1\)) forest matrix if the transition (normalized adjacency) matrix was replaced by a (sub-)graph laplacian.
Intuitively, the INVITE loss function is summing up over absorption (log-)probabilities at each new node activation: i.e.~each step of a tree's creation.
Because the probability of a sampled tree is precisely proportional to the product of its edge weights, then weighing a tree by its absorption probabilities and running INVITE should have a mathematically similar effect as the FP estimate.
The similarity would be exact whenever the tree that set of nodes traveled along \emph{was} the mode of the tree distribution: the MST.

In other words, whenever the random walks \emph{did} use the minimum distance to reach each node, the two methods should be equivalent.
While this isn't happening much individually, the effect of \emph{many} random walks will average out to this MST, precisely because \emph{it is the tree distribution mode} from which we assume node activations sample under \emph{marked Random Spanning Forests}.

To illustrate FP's efficacy in network estimation despite ignoring partial order information, we turn to a classic network recovery problem in this space: \emph{semantic networks from fluency data}\autocite{newdissimilaritymeasure_Prescott2006,Estimatingsemanticnetworks_Zemla2018,semanticorganizationanimal_Goni2011}.

\subsection{Domain-aware preprocessing}\label{domain-aware-preprocessing}

Verbal (semantic) fluency tests involve asking participants to list as many items belonging to a prompted category in their available time.
For instance, an ``animals'' prompt could lead to an answer like ``dog, cat, lion, tiger, bear, wolf\ldots{}'', etc.
Like before, the general idea is that recall of each item derives from a random walk in a cognitive ``memory space'', with emissions (usually) only when a new animal is encountered.
A participant might have backtracked internally to \emph{dog} from bear and jumped to wolf, for example.

Using a high-dimensional multilabel embedding space is possible, with one-column-per-animal, but the lists tend to be quite long and give networks with ``hairball'' tendencies.
However, if our intention is to find \emph{dependencies} between animals for a large number of participants, we might reasonably limit the co-occurrence to only a nearby subset of each list.
Effectively, we can limit the set of possible co-occurring animals based on our domain knowledge, namely, the limits of working memory.
Conservatively, with a well-cited work claiming 7 \(\pm\) 2 semantic units at a time in working memory\autocite{magicalnumberseven_Miller1956}, we can limit the set of possible dependencies for a given item to the 5 items on either side.
The 5 previous might have originated a jump to the current term, and the 5 after might be subsequent targets, for a \emph{rolling window} of 10 terms at-a-time.\footnote{
  This is precisely the logic that leads to the use of Skip-Gram and Continuous-Bag-of-Words transformations of text to weakly supervise word2vec and GloVe models\autocite{GloveGlobalvectors_Pennington2014,Efficientestimationword_Mikolov2013}.}

For our case study, we use lists of animals submitted by participants as described in \autocite{semanticorganizationanimal_Goni2011,Estimatingsemanticnetworks_Zemla2018}. We limit the animals under consideration to those that occurred in more than 30 lists for a good sample size, as well as lists with at least two animals.
This resulted in 100 animals over 293 fluency lists.
However, we ignore this filtering when creating the 10-animal rolling windows, to avoid inclusion of unrelated animals into prematurely filtered windows.
After re-applying the filter, 100 animals appear across 8020 working-memory windows.
Figure~\ref{fig-fluency-workingmemory} shows the effects of this preprocessing on marginal distributions.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-workingmemory-output-1.pdf}}

}

\caption{\label{fig-fluency-workingmemory}Effects of rolling-window activations on observation data}

\end{figure}%

Doing this preprocessing (for rolling-windows of 10) shifts relative animal frequencies downward (since there are many more ``observations'' from the rolling window), while also shifting the number of animals-per-observation to be strictly less than 10.
As desired, the pairwise cosine similarity of all vectors \(\mathbf{x}'_{j'},
\mathbf{x}'_{j}\) is significantly reduced.
While many participants might cover similar animals \emph{overall}, we want to investigate animal dependencies locally, and we don't expect individuals to always recall animals in the same memory ``area'' the whole time.

\subsection{Edge Connective Efficiency and Diversity}\label{sec-edge-diversity}

To compare the results of different backboning techniques, we introduce a new simple measure to quantify a network's sparsity, in terms of how many edges more than \(n-1\) (the minimum number needed to connect all \(n\) nodes) are being used.
\begin{equation}\phantomsection\label{eq-connect-efficient}{
\begin{gathered}
\psi_n(e) = \frac{e_{\text{min}}}{e}\frac{e_{\text{max}}-e}{e_{\text{max}}-e_{\text{min}}}\\
e_{\text{max}} = \frac{n(n-1)}{2} \quad e_{\text{min}} = n-1 
\end{gathered}
}\end{equation}

We call \(\psi\) the graph's ``connective efficiency'', and it will range from 0 when the graph is fully connected, to 1 when it is a tree, to \textgreater1 when it has insufficient edges to be connected.
This measure is intended to compare graphs that have had edges removed until it is about to be disconnected, such as with the \emph{Doubly Stochastic Filter} (DS) \autocite{twostagealgorithm_Slater2009}.
However, values greater than 1 also give insight to how much sparser than a tree some graph is.

The DS animal network is shown in Figure~\ref{fig-fluency-dsmin}.
With large, deeply connected clusters centered around contexts animals are found in Figure~\ref{fig-fluency-dsmin} looks very similar to the network recovered to make Fig. 4 in \autocite{semanticorganizationanimal_Goni2011}.
Clusters approximate animal types by their location relative to humans: farm/livestock, ocean/water creatures, ``African'' and jungle animals, small indoor mammals, small outdoor mammals, etc.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-dsmin-output-1.pdf}}

}

\caption{\label{fig-fluency-dsmin}Verbal Fluency (animals) Network Backbone (Doubly-Stochastic)}

\end{figure}%

This is largely how the literature on semantic fluency leaves their network recovery solution, with clusters based on human proximity or physical location.
However, there are other ways people might relate animals than simply by location.
Additionally, clique bias is quite strong in this network: why must every farm animal be mutually connected if its possible to recall any of them through one or two ``hub'' animals?
This is related to the incredible inefficiency of this backbone, with a \(\psi=0.35\) being rather closer to fully connected than sparse.
Additionally, the two animals that seem to appear \emph{regardless} of others are \emph{cat} and \emph{dog}, which ironically makes DS penalize their proximity to any of the clusters.
Both are ironically clustered with ocean animals due to their tendency to be listed near \emph{fish}.

The Chow-Liu tree network is shown in Figure~\ref{fig-fluency-tree}, and goes some way to alleviating these issues.
Clusters are largely intact, instead represented by large branches/subtrees off the main group.
However, some community relationships have been sacrificed to maintain strong individual edges, such as \emph{monkey}+\emph{giraffe} for location similarity at the expense of separating two halves of the pink cluster across a wide distance.
More alternate paths between creatures (i.e.~loops) are needed to better represent our perception of animal relationships.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-tree-output-1.pdf}}

}

\caption{\label{fig-fluency-tree}Verbal Fluency (animals) Dependency Network (Chow-Liu Tree)}

\end{figure}%

The other dependency network recovery method is GLASSO, which we have similarly thresholded at the minimum-connected point.
It only slightly improves on connective efficiency (\(\psi=0.44\)), though the cliques are replaced with much more dispersed connections throughout the graph.
We also see that reasonable inter-group connections are better represented, such as \emph{rabbit}+\emph{squirrel}, though \emph{cat} and \emph{dog} are still isolated due to overrepresentation throughout the dataset.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-glassomin-output-1.pdf}}

}

\caption{\label{fig-fluency-glassomin}Verbal Fluency (animals) Dependency Network (GLASSO)}

\end{figure}%

Subjectively, the GLASSO network is still difficult for an analyst to synthesize into useful knowledge, with so many edges, while the DS network only really managed to communicate one ``kind'' of knowledge (the context clusters).
We would ideally prefer a backbone that provides a wider diversity of important edge ``types'', for an analyst to better understand the kinds of animal relationships humans perceive.

To illustrate this, we show the \emph{Forest Pursuit}(FP) network in Figure~\ref{fig-fluency-fpmin}.
It has also been filtered to minimum-connected, like DS and GLASSO, though in this case the connective efficiency to reach that threshold is a staggering \(\psi=0.88\).

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-fpmin-output-1.pdf}}

}

\caption{\label{fig-fluency-fpmin}Verbal Fluency (animals) Dependency Network (Forest Pursuit)}

\end{figure}%

Unlike the other networks that push ``generalist'' nodes like \emph{cat} and \emph{dog} onto long, distant chains, those chains are used in the FP network to hold rare subgroups of clusters, treating them as ``gated'' by the prominent ``hubs'' of those groups.
For example, \emph{cat} is correctly linked to \emph{mouse} and \emph{lion} (in addition to \emph{bird}), while \emph{lemur} is pushed down a longer chain of primates, ``gated'' by \emph{gorilla}.
Similarly with \emph{eel} through \emph{lobster} and \emph{crab}.

A much broader edge-type diversity is also made apprarent with many non-context-based relationships made obvious with the improved sparsity.
An analyst has an easier job of creating ``edge-type inventories'', making the FP backbone an excellent exploratory assistant: animals can be related because they are:

\begin{itemize}
\tightlist
\item
  Co-located
\item
  Taxonomically similar (\emph{cheetah}+\emph{leopard})
\item
  Famous predator/prey pairs (\emph{cat}+\emph{mouse})
\item
  Pop-culture references (\emph{lion}\(\rightarrow\)\emph{tiger}\(\rightarrow\)\emph{bear})\footnote{Note that the dependency-based methods correctly interpred these three as \emph{not} being mutually connected in a triad, but specifically with this ordering (\emph{tiger} in the middle).}
\item
  Similar in ecological niche/role (\emph{koala}+\emph{sloth})
\item
  lexically similar/rhyming (\emph{moose}+\emph{goose})
\item
  Related through conservation or public awareness (\emph{panda}+\emph{gorilla})
\item
  etc.
\end{itemize}

This is further reflected in how FP alters the way \emph{centrality} measures behave.
Replicating Figure~\ref{fig-lesmis-centrality} for these graphs, Figure~\ref{fig-animal-centrality} shows the change in rank across the top 15 animals for the DS, GLASSO, and FP networks.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-animal-centrality-output-1.pdf}}

}

\caption{\label{fig-animal-centrality}Changes in animal centrality ranking for FP vs co-occurrence,GLASSO}

\end{figure}%

The DS centrality finds the most densely connected clique and gives all of its members incredibly high values.
Meanwhile, the none of the top 5 most common animals (\emph{dog,cat,lion,tiger,bear})\footnote{Interestingly, \emph{dog} never appears in centrality measures, and \emph{none} of the networks connect dog to any other animal than \emph{cat}.
  Meanwhile, \emph{wolf} is associated more with \emph{fox}, \emph{coyote}, \emph{dingo}, etc., which are notably all predators of farm animals.\\
  With the primary community structures being what they are (context/location-based), it seems that humans tend to put dogs in a category all their own.} have high centrality \emph{at all}.
Both GLASSO and DS have farm animals (\emph{chicken,goat,cow}) as the most important, despite the idea that goat likely could be reached from e.g.~\emph{cow} quite often.
FP adds more variety, giving hub-animals from different communities high centrality scores, each of which could lead to a variety of different paths.
While subjective, the ranks from FP appear to be a more holistic inventory of ``lynchpin'' animals, which provide nearby coverage for a large amount of others.

\subsection{Thresholded Structure Preservation}\label{thresholded-structure-preservation}

Another beneficial feature of \emph{Forest Pursuit} is how it fares under increased thresholding.
Because co-occurrence methods prioritize cliques, those cliques will remain connected as other edges are removed, effectively destroying the global structure of the animal network past a certain threshold.
As seen in Figure~\ref{fig-fluency-preservation}, in keeping only the top 2\% of edges, they are used to connect separated islands of animal communities.

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-preservation-output-1.pdf}}

}

\subcaption{\label{fig-fluency-preservation-1}co-occurrence retains local communities at the cost of global structure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-preservation-output-2.pdf}}

}

\subcaption{\label{fig-fluency-preservation-2}clique-bias correction preserves central structure by disconnecting rare nodes}

}

\caption{\label{fig-fluency-preservation}Differences in structural preservation with over-thresholding.}

\end{figure}%

Meanwhile, the FP network is quite robust to this excessive thresholding, with the global structure preserved at 2\%.
The removed edges have simply detached some of the rarer animals from the network entirely.
These isolates not only reflect a more metrologically-sound idea (that rarer nodes would be disconnected at higher certainty thresholds), but are also beneficial to analysts, since manually re-connecting isolates of rare animals is simpler than manually determining a global reconnection strategy for island groups.

\subsection{Forest Pursuit as Preprocessing}\label{sec-fp-preprocess}

Because \emph{Forest Pursuit} creates a representation of the observed data in \emph{edge space}, we can use Equation~\ref{eq-sparse-approx-tree} in the forward direction, creating a ``new''design matrix \(X\gets BR\).
As discussed in Section~\ref{sec-fp-problem}, Equation~\ref{eq-sparse-approx-tree} will create a design matrix of interaction counts for each node (its degree in the steiner tree approximation), rather than a binary ``on/off'' indicator.

By supplying \emph{other algorithms} with this new estimate for \(X\), this makes FP a kind of preprocessing on the data itself.
We can do this to bias the other methods toward greater sparsity in the backbone, without explicitly relying on point estimates for each observation (any tree with those node degrees would do the same).
As shown in Figure~\ref{fig-fluency-preprocess}, GLASSO and DS both increase their connective efficiency under FP preprocessing.

\begin{figure}

\centering{

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-preprocess-output-1.pdf}}

}

\subcaption{\label{fig-fluency-preprocess-1}Doubly Stochastic filter with FP (node degree) preprocessing}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/mediabag/content/part3/3-08-ordered_files/figure-latex/-content-codefigs-qualitative-fig-fluency-preprocess-output-2.pdf}}

}

\subcaption{\label{fig-fluency-preprocess-2}GLASSO Precision estimate with FP (node-degree) preprocessing}

}

\caption{\label{fig-fluency-preprocess}Forest Pursuit preprocessing for Doubly-Stochastic and GLASSO recovered networks}

\end{figure}%

This is likely due to the increase in ``signal-to-noise'' ratio for each datapoint, since observations are only similar when they have a node interacting the same ``amount'', not merely when similar nodes are activated.

Because the entries are now integer counts, FP preprocessing might also give a better path to using distribution-based embedding and clustering techniques, such as Hellinger distances between multinomial sample counts.
This goes for many other techniques from text processing that rely on multinomial assumptions (i.e.~techniques otherwise inapplicable to binary data).\\
FP Preprocessing, with further empirical and theoretical validation, might prove to be a powerful tool for practitioners to flexibly backbone and analyse their networks with a variety of new techniques.

\bookmarksetup{startatroot}

\chapter{Conclusion \& Future Work}\label{sec-conclusion}

Practitioners have long struggled with a lack of techniques for metrological quantification and measurement error handling in network science.
There is an ongoing need to specify valid network recovery models---ones that are not only assessed for precision, but designed for \emph{trueness}.
\textcite{Measurementerrornetwork_Wang2012} provides a ``reclassification'' of measurement error in networks, focusing on the true/false positive/negative dichotomies for both edge and node reporting.
This implicitly assumes that, like the karate-club graph \autocite{InformationFlowModel_Zachary1977}, our observations are of network components (edges/nodes).
When the object to be measured is not directly observable (as is the case for recovery from node activations), measurement error can also arise both from model noise sensitivity (lack of precision) and misspecification (lack of trueness).
Because of this:

\singlespacing

\begin{quote}
The practice of ignoring measurement error is still mainstream, and robust methods to take it into account are underdeveloped.

\hfill -- Tiago Peixoto \autocite{ReconstructingNetworksUnknown_Peixoto2018}
\end{quote}

\doublespacing

\section{Discussion and limitations}\label{discussion-and-limitations}

Much of our discussion on \emph{Forest Pursuit} and its extensions has revolved around assumptions about data availability and generation.
Practitioners often face situations where networks are recovered in essentially ``unsupervised'' settings, and their data could reasonably be modeled as arising from a spreading process on the graph they wish to recover.
However, these assumptions do not hold in general, and it's worth discussion how they impact the application of Forest Pursuit and where future research could fill in theoretical and practical gaps.

\subsection{Validation and Network Dynamics}\label{validation-and-network-dynamics}

We largely assumed that real-world network recovery is predominantly \emph{unsupervised}, so that results verification is very difficult in practice.
The MENDR dataset provides an initial foray into standardized reference problems, each having a ``ground-truth'', but this becomes much more complicated when real-world datasets do not.

One approach to verification would be to do forecasting on dependencies.
For collaboration networks, for instance, if two authors publish together, we are assuming they are conditionally dependent on each other (when there are no ``hidden'' node activations).\\
These links (the two-author papers) can be used as an incomplete ``ground-truth'', so we could theoretically test each algorithm's ability to predict dependencies when the two-author examples are either held out or predicted using all preceding papers.

One difficulty with this is the sheer number of true-negatives we expect in a sparse graph as the number of nodes increases.
In general a sparse connected graph's edge count only must grow linearly with node count, but the non-edge count grows quadratically.
This makes the chance that any two authors with a possible dependency \emph{do} coauthor exclusively together vanishingly small, in general.
Not all conditional dependencies within a department will lead to pair-papers, since some individuals will only publish in larger groups, for instance.
To add to the trouble, relationship networks over time have a good chance of being \emph{dynamic}: people move to new institutions, students graduate, etc.
Using predictions of \emph{future} two-author collaboration will risk running into errors from network dynamics like these.
It may be possible to include network dynamics into the relationship inference itself, such as with Dynamic Topic Models \autocite{Dynamictopicmodels_Blei2006}, but just as before we are left with the difficulty of verifying and validating a (now more complex) unsupervised model.

Within this train-of-thought, however, lies a possibility to create what are called \emph{metamorphic tests} suitable for verification of unsupervised models \autocite{METTLEMETamorphicTesting_Xie2020}.
Rather than look for a ground-truth the measure our network against, we can define properties of our network reconstruction that we know must hold given our modeling assumptions or domain knowledge.
For the co-author network case, instead of predicting any two-author paper, instead we might construct a list of all student-advisor pairs.
Because we know that their relationship is very likely to be dependent, these pairs should be recovered from an algorithm that is reconstructing author dependencies.
Algorithms could be tested against each other for performance on these kinds of metamorphic conditions, ensuring correctness in cases we specify.
Such metamorphic tests would be a valuable addition to the network reconstruction community, especially if individual domains could compile lists of relevant conditions that \emph{should} hold in any given reconstruction attempt.

\subsection{Spreading process assumption}\label{spreading-process-assumption}

As discussed in Section~\ref{sec-lfa-like}, the marked-Random Spanning Forest model was motivated by a need to incorporate prior knowledge about the generating mechanism of our data (namely, \emph{spreading processes}).
Consequently, our validation through synthetic datasets provided in MENDR used random walks to construct node-activations for inferring structure.
The results presented here verify Forest Pursuit's ability to recover structure in this setting, which also helps validate our design given the domain-based constraint (spreading-process generation).

However, other methods do exist for generating (correlated) binary activation data, and adding other types of datasets to the MENDR catalog would provide a mechanism to verify model recovery capability under other generation settings.
The addition of thresholded multivariate-normal samples \autocite{generationcorrelatedartificial_Leisch1998} would be a reasonable next-step to increasing the scope of possible verification for thesis algorithms.
Validation in these cases would need more theoretical work to provide a framework for understanding the expected behavior of Forest Pursuit under non-spreading assumptions.
However, we also hope to see further development of additive/local Desire Path Density models by the community that estimate relationships under other common generative schemes.
It is possible that planar graphs and path-graphs are a class that appropriately model MVL (Ising) and Markov (sequential) generation schemes, respectively.
More work is needed to show the behavior of Desire Path Densities with other graph classes.

\section{Modifications and extensions to Forest Pursuit}\label{sec-future-fp}

\emph{Desire Path Densities} and \emph{Forest Pursuit} are designed to be adaptable to several modalities of use\footnote{see for instance Section~\ref{sec-fp-preprocess}}.
However, there are key limitations of the model that could be addressed going forward, as well as future research directions inspired by our modeling paradigm.

\subsection{Multiple sources and hidden nodes}\label{multiple-sources-and-hidden-nodes}

One of the key assumptions to make the likelihood of the (marked) Random Spanning Forest tractable is to allow only one ``source'' node (the random walk starting node), and to sample it from a uniform categorical distribution.
However, by explicitly adding a ``root'' node that is implied by the random forest distribution (see Figure~\ref{fig-inject-plan}), we would immediately achieve the possibility for multiple sources.
A source would become any node incident to the root in a sampled spanning tree.
To prevent every node from being activated, the spanning tree could be ``pruned'' at some depth away from the root (which is a parameter that we could model with \emph{e.g.} a Geometric distribution).
How many sources get selected in a minimum spanning tree could be controlled through the weights given to edges incident to the root (which, if the reader recalls, is already represented by \(\beta\)).
Also possible could be the use of independent Bernoulli activations of nodes as sources (rather than a categorical selector) though heavy regularization would be required to avoid each node always becoming its own source.

Another assumption is that there were no \emph{hidden nodes}.
This is different from false negative nodes, in the sense that here we did not know a node \emph{existed} at all, but it is a source of dependency nonetheless.
This might be explained as seeing a need to \emph{add} a node so that observed distances between nodes are maximally tree-like.
\autocite{TreeIam_Sonthalia2020} uses a greedy algorithm (TreeRep) to learn a tree structure (hyperbolic metric) that has minimum distortion to a given distance metric.
If the distortion is too great, they add a Steiner node and continue.
Automatically building a tree and simultaneously adding Steiner nodes that reduce distortion would effectively result in recommendations for adding new nodes to the graph.
An analyst might be able to review these suggestions, interactively, to find ``hidden'' nodes.

\subsection{Generalizing inner products on incidences}\label{sec-future-hyperbolic}

Speaking of hyperbolic, many authors have recently investigated the usefulness of hyperbolic space for embedding graphs (as vectors) in a way that preserves hierarchies and sparsity \emph{naturally} \autocite{TreesContinuousEmbeddings_Chami2020,HyperbolicEntailmentCones_Ganea2018,LearningContinuousHierarchies_Nickel2018,RepresentationTradeoffsHyperbolic_Sala2018,Socialcentralizationsemantic_Linzhuo2020}.
If trees can be embedded losslessly \autocite{LowDistortionDelaunay_Sarkar2012} into \(\mathcal{H}^2\)\footnote{
  \(\mathcal{H}^2\) is the 2D hyperbolic manifold, embedded in a 3D Euclidean space.}, then traversing a tree as a random walker could be represented as a trajectory in an \(\mathcal{H}^{2+1}\) de-Sitter space.
Various techniques exist for finding embeddings of graphs as ``causal-sets'' in a Lorentzian spacetime \autocite{EmbeddinggraphsLorentzian_Clough2017}, and this approach could be combined with a way to smoothly sample a discretized space with (hyperbolic) Voronoi cells \autocite{HyperbolicVoronoiDiagrams_Nielsen2010,SemiDiscreteNormalizing_Chen2022}

\subsection{Application areas and case studies}\label{application-areas-and-case-studies}

Lastly, a critical test of \emph{Forest Pursuit} will be its application to a wider variety of domains under the scrutiny of each domain's experts.
From the network community itself, for instance, recent interest has been shown in assessing the methodological reasons for observed assortativity \autocite{PerceivedAssortativitySocial_Fisher2017}.
The conditions under which controlling for clique-bias \emph{also} reduces assortativity would be a useful tool when deciding to use different network cleaning techniques.

Further afield, semantic Verbal Fluency tests discussed in Section~\ref{sec-animal-fluency} are often administered for the purposes of assisting in diagnosis of Alzheimer's and Schizophrenia patients.
While experiments using inferred network structures \autocite{newdissimilaritymeasure_Prescott2006,Semanticverbalfluency_AriasTrejo2021} have been used to detecting early-onset neurological disease from topological differences, it could be useful to re-assess these outcomes when clique-bias has been better accounted for.

All of these applications would be further assisted by \emph{Forest Pursuit}'s ability to

\begin{itemize}
\tightlist
\item
  Infer network estimates \emph{quickly}, on streaming data, and
\item
  Incorporate prior (and incoming) knowledge from domain experts on edges.
\end{itemize}

Together, these properties should make for an ideal \emph{human-in-the-loop} analysis tool.
Indeed, for any qualitative study on nodes and discovering relationships between them, decreasing the annotation load (number of edges to assess)---\emph{while} increasing edge diversity---will be critical to correctly inventory the important categories of relationships or dependencies\footnote{see Section~\ref{sec-edge-diversity}}.
Building tools that enable practitioners and researchers to undertake complex grounded coding \autocite{codingmanualqualitative_Saldana2021} tasks like this is a rich area for possible Human-Systems Integration efforts going forward \autocite{Humanlooptechnical_Fung2024,AIInformedApproaches_Harper2022}.

\section{Summary of contributions}\label{summary-of-contributions}

To develop the practice of taking measurement error into account, we have proposed a combination of problem framing, measurement aggregation techniques, and methods for bias correction when recovering network structure from observed random walk visits.

This thesis provides:

\begin{itemize}
\tightlist
\item
  An interpretation of network recovery as an \emph{inverse problem} comparable to sparse approximation, with concise definitions of data, edge, and node vector spaces from an underlying incidence-structure formalism.
\item
  A taxonomy of structural assumptions used in literature to make network inference tractable:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    Local, global, or resource/information-flow structural constraints;
  \item
    Inverse-problem status (direct or indirect edge observation); and
  \item
    Whether activation observations are pre-aggregated before estimation.
  \end{enumerate}
\item
  A method, \emph{Forest Pursuit}, to address the need for a model with:

  \begin{enumerate}
  \def\labelenumi{\arabic{enumi})}
  \tightlist
  \item
    \emph{Local} observation constraints,
  \item
    An inverse-problem assumption for indirect edge observation, and
  \item
    Dependence on the bipartite nature of node observations.\\
  \end{enumerate}
\item
  A dataset and benchmarking toolkit (MENDR) to reproducibly compare algorithmic ability to recover network structure from random walk activations, which has been applied to demonstrate the scaling and accuracy of \emph{Forest Pursuit} over other methods.
\item
  Generalization of Forest Pursuit by developing a probabilistic model for it as a sparse dictionary learning technique, for which we provide an expectation maximization scheme to estimate.\\
\item
  Application of \emph{Forest Pursuit} as case studies in scientific collaboration networks, classic literature analysis, technical language processing, and semantic verbal fluency tests.
\end{itemize}

We lay this foundation in the hope of further improving the ability of practitioners to explore the structure of their data in a principled manner.


\singlespacing
\printbibliography

\doublespacing

\end{document}
